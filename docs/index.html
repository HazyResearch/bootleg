---
layout: default
---
<h1>About</h1>
<p>
  Bootleg is a self-supervised named entity disambiguation (NED) system that links mentions in text to entities in a knowledge base. Bootleg is built to improve disambiguation of entities which appear infrequently or not at all in training data, called <em>tail</em> entities. The tail is where disambiguation gets interesting as models cannot rely on learned textual cues for entities rarely seen in train. As shown in the figure below, this is why a BERT-based baseline, which memorizes co-occurrences between entities and text to disambiguate, struggles over the tail.
</p>
<div style="text-align: center;">
  <!-- <h1 class="site-name"><a href="{{ site.baseurl }}/">{{ site.name }}</a></h1> -->
  <!-- <p class="site-description">{{ site.description }}</p> -->
  <img src="images/bootleg-performance.png" height="270px" style="margin-top: 4px;" />
</div>
<p>
  The simple insight behind Bootleg is that resolving the tail requires subtle reasoning over <em>type</em> and <em>relation</em> signals. In Bootleg, we take an "embedding-centric" approach and represent types, relations, and entities as learned embeddings in a Transformer-based architecture. Critically, all entities of the same type (or relation) get the same type (relation) embedding, meaning all entities, including the tail entities, can leverage the signals learned for all entities of that type (relation). This is what allows for Bootleg to better disambiguate tail entities than models that rely only on textual patterns.
</p>
<p>
  Bootleg is still in active development, but is already in use at industry and research labs.
</p>
<h1>Getting Started</h1>
For help getting started with using Bootleg, check out our <a href="https://github.com/HazyResearch/bootleg">open-source code</a>, <a href="https://github.com/HazyResearch/bootleg/tree/master/tutorials">tutorials</a>, and <a href="https://arxiv.org/abs/2010.10363">paper</a>.

<h1>Results</h1>
Bootleg achieves state of the art on NED benchmarks and improves over a standard BERT NED baseline by over 40 F1 points on the tail of Wikipedia, which we define as entities occurring 10 or fewer times in our Wikipedia training data. As a fun experiment, we extracted Bootleg's learned representations and integrated them into a SotA model for TACRED<sup><a href="#note1">1</a></sup>. This Bootleg-enhanced TACRED model set a new SotA by 1 F1 point. See our <a href="https://arxiv.org/abs/2010.10363">paper</a> for details.
<h3>NED Benchmarks</h3>
We compare the newest version Bootleg as of October 2020 against the current reported SotA numbers on two standard sentence-level benchmarks (KORE50 and RSS500) and the standard document-level benchmark (AIDA CoNLL-YAGO).
<p>
  <table class="ui structured red table small-font">
    <thead>
        <tr>
          <th>Benchmark</th>
          <th>System</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>F1</th>
        </tr>
    </thead>
    <tbody>
      <tr>
        <td rowspan="2">KORE50<sup><a href="#note1">2</a></sup></td>
        <td>Hu et al., 2019<sup><a href="#note1">5</a></sup></td>
          <td>80.0</td>
          <td>79.8</td>
          <td>79.9</td>
      </tr>
    </tr>
    <td>Bootleg</td>
        <td><b>86.0</b></td>
        <td><b>85.4</b></td>
        <td><b>85.7</b></td>
      <tr>
        <td rowspan="2">RSS500<sup><a href="#note1">3</a></sup></td>
          <td>Phan et al., 2019<sup><a href="#note1">6</a></sup></td>
          <td>82.3 </td>
          <td>82.3</td>
          <td>82.3</td>
      </tr>
      <td>Bootleg</td>
      <td><b>82.5</b> </td> 
      <td><b>82.5</b></td>
      <td><b>82.5</b></td>
    </tr>
      <tr>
        <td rowspan="2">AIDA CoNLL YAGO<sup><a href="#note1">4</a></sup></td>
          <td>Fevry et al., 2020<sup><a href="#note1"7</a></sup></td>
          <td>-</td>
          <td><b>96.7</b></td>
          <td>-</td>
      </tr>
    </tr>
    <td>Bootleg</td>
    <td>96.9</td>
    <td><b>96.7</b></td>
    <td>96.8</td>
  </tr>
</tr>
    </tbody>
</table>
</p>
<h3>Tail Performance</h3>
We also evaluate the performance of Bootleg on the tail compared to a standard BERT NED baseline.
We create evaluation sets from Wikipedia by filtering by the frequency of the true entities in the training dataset and report micro F1 scores. The slight increase in performance over unseen entities compared to tail entities is due to the lower degree of ambiguity among the unseen entities compared to the tail entities.
<p>
  <table class="ui structured red table small-font">
    <thead>
        <tr>
          <th>Evaluation Set</th>
          <th>BERT NED Baseline</th>
            <th>Bootleg</th>
        </tr>
    </thead>
    <tbody>
      <tr>
        <td>All Entities</td>
          <td>85.9</td>
          <td><b>91.3</b></td>
      </tr>
    </tr>
    <td>Torso Entities</td>
        <td>79.3</td>
        <td><b>87.3</b></td>
    </tr>
      <td>Tail Entities</td>
          <td>27.8</td>
          <td><b>69.0</b></td>
      </tr>
      <tr>
          <td>Unseen Entities</td>
          <td>18.5</td>
          <td><b>68.5</b></td>
      </tr>
    </tbody>
</table>


<h2>References</h2>
<p>
<p style="font-size:14px" id="note1"><sup>1</sup>Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig
. <a href=https://www.aclweb.org/anthology/2020.acl-main.142.pdf>"TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task."</a>  In <i>ACL</i>, 2020.
<p style="font-size:14px" id="note1"><sup>2</sup>Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard Weikum. <a href=https://www.hoffart.ai/wp-content/papercite-data/pdf/hoffart-2012vx.pdf>"Kore: keyphrase overlap relatedness for entity disambiguation."</a>  In <i>CIKM</i>, 2012.
<p style="font-size:14px" id="note1"><sup>3</sup> Daniel Gerber, Sebastian Hellmann, Lorenz Bühmann, Tommaso Soru, Ricardo Usbeck, and Axel-Cyrille Ngonga Ngomo.<a href=https://link.springer.com/chapter/10.1007/978-3-642-41335-3_9>"Real-time rdf extraction from un- structured data streams."</a> In <i>ISWC</i>, 2013.
<p style="font-size:14px" id="note1"><sup>4</sup>Johannes Hoffart, Mohamed Amir Yosef,Ilaria Bordino,Hagen Fürstenau, Manfred Pinkal, Marc Spaniol, Bilyana Taneva, Stefan Thater, and Gerhard Weikum. <a href=https://www.aclweb.org/anthology/D11-1072.pdf>"Robust disambiguation of named entities in text."</a> In <i>EMNLP</i>, 2011.
<p style="font-size:14px" id="note1"><sup>5</sup>Shengze Hu, Zhen Tan, Weixin Zeng, Bin Ge, and Weidong Xiao. <a href=https://www.mdpi.com/2073-8994/11/4/453>"Entity linking via symmetrical attention-based neural network and entity structural features."</a> <i>Symmetry</i>, 2019.
<p style="font-size:14px" id="note1"><sup>6</sup> Minh C. Phan, Aixin Sun, Yi Tay, Jialong Han, and Chenliang Li. <a href=https://arxiv.org/pdf/1802.01074.pdf>"Pair-linking for
  collective entity disambiguation: Two could be better than all".</a> In <i>TKDE</i>, 2019.
<p style="font-size:14px" id="note1"><sup>7</sup> Thibault Févry, Nicholas FitzGerald, Livio Baldini Soares, and Tom Kwiatkowski. <a href=https://arxiv.org/pdf/2005.14253.pdf>"Empirical evaluation of pretraining strategies for supervised entity linking."</a> In <i>AKBC</i>, 2020.
</p>
<!--
<div class="posts">
  {% for post in site.posts %}
    <article class="post">

      <h1><a href="{{ site.baseurl }}{{ post.url }}">{{ post.title }}</a></h1>

      <div class="entry">
        {{ post.excerpt }}
      </div>

    </article>
  {% endfor %}
</div> -->
