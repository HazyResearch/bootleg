{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NED Tutorial\n",
    "\n",
    "In this tutorial, we walk through how to use Bootleg as an end-to-end pipeline to detect and label entities in a set of sentences. First, we show how to use Bootleg to detect and disambiguate mentions to entities. We then compare to an existing system named TAGME. Finally, we show how to use Bootleg to annotate individual sentences on the fly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how Bootleg performs on more natural language than we find in Wikipedia, we hand label the mentions and corresponding entities in 50 questions sampled from the [Natural Questions dataset (Google)](https://ai.google.com/research/NaturalQuestions). \n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- Pretrained Bootleg model and config [here](https://bootleg-emb.s3.amazonaws.com/models/2020_08_25/bootleg_wiki.tar.gz)*\n",
    "- Sample of Natural Questions with hand-labelled entities [here](https://bootleg-emb.s3.amazonaws.com/data/nq.tar.gz)\n",
    "- Entity data [here](https://bootleg-emb.s3.amazonaws.com/data/wiki_entity_data.tar.gz)*\n",
    "- Embedding data [here](https://bootleg-emb.s3.amazonaws.com/data/emb_data.tar.gz)*\n",
    "- Pretrained BERT model [here](https://bootleg-emb.s3.amazonaws.com/pretrained_bert_models.tar.gz)*\n",
    "\n",
    "*Same file as in benchmark tutorial and does not need to be re-downloaded.\n",
    "\n",
    "For convenience, you can run the commands below (from the root directory of the repo) to download all the above files and unpack them to `models`, `data`, and `pretrained_bert_models` directories. It will take several minutes to download all the files. \n",
    "\n",
    "    bash download_model.sh \n",
    "    bash download_data.sh \n",
    "    bash download_bert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import ujson\n",
    "from utils import load_mentions, tagme_annotate\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "root_dir = # FILL IN FULL PATH TO ROOT REPO DIRECTORY HERE \n",
    "\n",
    "cand_map = f'{root_dir}/data/wiki_entity_data/entity_mappings/alias2qids_wiki.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a GPU with at least 12GB of memory available, set the below to `True` to run inference on the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect Mentions\n",
    "Bootleg uses a simple mention extraction algorithm that extracts mentions using a given candidate map. We will use a Wikipedia candidate map that we mined using Wikipedia anchor links and Wikidata aliases for a total of ~8 million mentions (provided in the Requirements section of this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input dataset for the end-to-end pipeline, we assume a jsonlines file with a single dictionary with the key \"sentence\" and value as the text of the sentence, per line. For instance, you may have a file with the lines:\n",
    "\n",
    "    {\"sentence\": \"who did the voice of the magician in frosty the snowman\"}\n",
    "    {\"sentence\": \"what is considered the outer banks in north carolina\"}\n",
    "    \n",
    "Below, we have additional keys to keep track of the hand-labelled mentions, but this is purely for evaluating the quality of the end-to-end pipeline and is not needed in the common use cases of using Bootleg to detect and label mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_sample_orig = f'{root_dir}/data/nq/test_natural_questions_50.jsonl'\n",
    "nq_sample_bootleg = f'{root_dir}/data/nq/test_natural_questions_50_bootleg.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-15 14:33:11,740 Loading candidate mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7970529/7970529 [00:13<00:00, 585452.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-15 14:33:25,359 Loaded candidate mapping with 7970529 aliases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-15 14:33:38,486 Using 8 workers...\n",
      "2020-09-15 14:33:38,487 Reading in /dfs/scratch1/mleszczy/bootleg-internal//data/nq/test_natural_questions_50.jsonl\n",
      "2020-09-15 14:33:38,763 Wrote out data chunks in 0.28s\n",
      "2020-09-15 14:33:38,765 Calling subprocess...\n",
      "2020-09-15 14:33:39,545 Merging files...\n",
      "2020-09-15 14:33:39,584 Removing temporary files...\n",
      "2020-09-15 14:33:40,287 Finished in 1.8042943477630615 seconds. Wrote out to /dfs/scratch1/mleszczy/bootleg-internal//data/nq/test_natural_questions_50_bootleg.jsonl\n"
     ]
    }
   ],
   "source": [
    "from bootleg.extract_mentions import extract_mentions\n",
    "extract_mentions(in_filepath=nq_sample_orig, out_filepath=nq_sample_bootleg, cand_map_file=cand_map, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at a sample of the extracted mentions, we can compare the mention extraction phase to the hand-labelled mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>aliases_hand</th>\n",
       "      <th>spans_hand</th>\n",
       "      <th>aliases_bootleg</th>\n",
       "      <th>spans_bootleg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is it a bank holiday today in spain</td>\n",
       "      <td>[bank holiday, spain]</td>\n",
       "      <td>[[3, 5], [7, 8]]</td>\n",
       "      <td>[bank holiday, spain]</td>\n",
       "      <td>[[3, 5], [7, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>when was the first freeway built in los angeles</td>\n",
       "      <td>[los angeles]</td>\n",
       "      <td>[[7, 9]]</td>\n",
       "      <td>[freeway, los angeles]</td>\n",
       "      <td>[[4, 5], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>what was the japanese motivation for bombing pearl harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[[3, 4], [7, 9]]</td>\n",
       "      <td>[japanese, motivation, bombing, pearl harbor]</td>\n",
       "      <td>[[3, 4], [4, 5], [6, 7], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>who played smiley in tinker tailor soldier spy</td>\n",
       "      <td>[tinker tailor soldier spy]</td>\n",
       "      <td>[[4, 8]]</td>\n",
       "      <td>[smiley, tinker tailor soldier spy]</td>\n",
       "      <td>[[2, 3], [4, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>the representative of the british crown in nz</td>\n",
       "      <td>[british crown, nz]</td>\n",
       "      <td>[[4, 6], [7, 8]]</td>\n",
       "      <td>[british crown, nz]</td>\n",
       "      <td>[[4, 6], [7, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>who opened and closed the 1960 winter olympics</td>\n",
       "      <td>[1960 winter olympics]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "      <td>[1960 winter olympics]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>what teams are in the fa cup final</td>\n",
       "      <td>[fa cup final]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "      <td>[fa cup final]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>what was dennis hopper 's bike in easy rider</td>\n",
       "      <td>[dennis hopper, easy rider]</td>\n",
       "      <td>[[2, 4], [7, 9]]</td>\n",
       "      <td>[dennis hopper, easy rider]</td>\n",
       "      <td>[[2, 4], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>who does oregon state play in the college world series</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hitchhiker 's guide to the galaxy slartibartfast quotes</td>\n",
       "      <td>[hitchhiker 's guide to the galaxy, slartibartfast]</td>\n",
       "      <td>[[0, 6], [6, 7]]</td>\n",
       "      <td>[hitchhiker s guide to the galaxy, slartibartfast]</td>\n",
       "      <td>[[0, 6], [6, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>once upon a time season 6 episode list</td>\n",
       "      <td>[once upon a time season 6]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "      <td>[once upon a time season 6]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>where does the last name aponte come from</td>\n",
       "      <td>[aponte]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[aponte]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>who 's doing the halftime show in 2018</td>\n",
       "      <td>[halftime show]</td>\n",
       "      <td>[[4, 6]]</td>\n",
       "      <td>[halftime show]</td>\n",
       "      <td>[[4, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>landmark supreme court cases dealing with the first amendment</td>\n",
       "      <td>[supreme court, first amendment]</td>\n",
       "      <td>[[1, 3], [7, 9]]</td>\n",
       "      <td>[supreme court, first amendment]</td>\n",
       "      <td>[[1, 3], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what channel is the premier league on in france</td>\n",
       "      <td>[premier league, france]</td>\n",
       "      <td>[[4, 6], [8, 9]]</td>\n",
       "      <td>[premier league, france]</td>\n",
       "      <td>[[4, 6], [8, 9]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         sentence  \\\n",
       "17                            is it a bank holiday today in spain   \n",
       "36                when was the first freeway built in los angeles   \n",
       "48      what was the japanese motivation for bombing pearl harbor   \n",
       "24                 who played smiley in tinker tailor soldier spy   \n",
       "44                  the representative of the british crown in nz   \n",
       "26                 who opened and closed the 1960 winter olympics   \n",
       "15                             what teams are in the fa cup final   \n",
       "12                   what was dennis hopper 's bike in easy rider   \n",
       "31         who does oregon state play in the college world series   \n",
       "11        hitchhiker 's guide to the galaxy slartibartfast quotes   \n",
       "8                          once upon a time season 6 episode list   \n",
       "14                      where does the last name aponte come from   \n",
       "34                         who 's doing the halftime show in 2018   \n",
       "37  landmark supreme court cases dealing with the first amendment   \n",
       "3                 what channel is the premier league on in france   \n",
       "\n",
       "                                           aliases_hand         spans_hand  \\\n",
       "17                                [bank holiday, spain]   [[3, 5], [7, 8]]   \n",
       "36                                        [los angeles]           [[7, 9]]   \n",
       "48                             [japanese, pearl harbor]   [[3, 4], [7, 9]]   \n",
       "24                          [tinker tailor soldier spy]           [[4, 8]]   \n",
       "44                                  [british crown, nz]   [[4, 6], [7, 8]]   \n",
       "26                               [1960 winter olympics]           [[5, 8]]   \n",
       "15                                       [fa cup final]           [[5, 8]]   \n",
       "12                          [dennis hopper, easy rider]   [[2, 4], [7, 9]]   \n",
       "31                 [oregon state, college world series]  [[2, 4], [7, 10]]   \n",
       "11  [hitchhiker 's guide to the galaxy, slartibartfast]   [[0, 6], [6, 7]]   \n",
       "8                           [once upon a time season 6]           [[0, 6]]   \n",
       "14                                             [aponte]           [[5, 6]]   \n",
       "34                                      [halftime show]           [[4, 6]]   \n",
       "37                     [supreme court, first amendment]   [[1, 3], [7, 9]]   \n",
       "3                              [premier league, france]   [[4, 6], [8, 9]]   \n",
       "\n",
       "                                       aliases_bootleg  \\\n",
       "17                               [bank holiday, spain]   \n",
       "36                              [freeway, los angeles]   \n",
       "48       [japanese, motivation, bombing, pearl harbor]   \n",
       "24                 [smiley, tinker tailor soldier spy]   \n",
       "44                                 [british crown, nz]   \n",
       "26                              [1960 winter olympics]   \n",
       "15                                      [fa cup final]   \n",
       "12                         [dennis hopper, easy rider]   \n",
       "31                [oregon state, college world series]   \n",
       "11  [hitchhiker s guide to the galaxy, slartibartfast]   \n",
       "8                          [once upon a time season 6]   \n",
       "14                                            [aponte]   \n",
       "34                                     [halftime show]   \n",
       "37                    [supreme court, first amendment]   \n",
       "3                             [premier league, france]   \n",
       "\n",
       "                       spans_bootleg  \n",
       "17                  [[3, 5], [7, 8]]  \n",
       "36                  [[4, 5], [7, 9]]  \n",
       "48  [[3, 4], [4, 5], [6, 7], [7, 9]]  \n",
       "24                  [[2, 3], [4, 8]]  \n",
       "44                  [[4, 6], [7, 8]]  \n",
       "26                          [[5, 8]]  \n",
       "15                          [[5, 8]]  \n",
       "12                  [[2, 4], [7, 9]]  \n",
       "31                 [[2, 4], [7, 10]]  \n",
       "11                  [[0, 6], [6, 7]]  \n",
       "8                           [[0, 6]]  \n",
       "14                          [[5, 6]]  \n",
       "34                          [[4, 6]]  \n",
       "37                  [[1, 3], [7, 9]]  \n",
       "3                   [[4, 6], [8, 9]]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_mentions_df = load_mentions(nq_sample_orig)\n",
    "bootleg_mentions_df = load_mentions(nq_sample_bootleg)\n",
    "\n",
    "# join dataframes and sample\n",
    "pd.merge(orig_mentions_df, bootleg_mentions_df, on=['sentence'], suffixes=['_hand', '_bootleg']).sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample above, we see that generally Bootleg detects the same mentions as the hand-labelled mentions, however sometimes Bootleg extracts extra mentions (e.g \"worth\" in \"what is the worth of the catholic church\"). This is expected as we would rather the mention detection step filter out too few mentions than too many. It will be the job of the backbone model and postprocessing to filter out these extra mentions, by either thresholding the prediction probability or predicting a candidate that represents \"No Candidate\" (we refer to this as \"NC\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disambiguate Mentions to Entities\n",
    "\n",
    "We run inference using a pretrained Bootleg model to disambiguate the extracted mentions to Wikidata QIDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model config so we can set additional parameters and load the saved model during evaluation. We need to update the config parameters to point to the downloaded model checkpoint and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg import run\n",
    "from bootleg.utils.parser_utils import get_full_config\n",
    "\n",
    "config_path = f'{root_dir}/models/bootleg_wiki/bootleg_config.json'\n",
    "config_args = get_full_config(config_path)\n",
    "\n",
    "# set the model checkpoint path \n",
    "config_args.run_config.init_checkpoint = f'{root_dir}/models/bootleg_wiki/bootleg_model.pt'\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args.data_config.entity_dir = f'{root_dir}/data/wiki_entity_data'\n",
    "config_args.data_config.alias_cand_map = 'alias2qids_wiki.json'\n",
    "\n",
    "# set the data path and RSS500 test file \n",
    "config_args.data_config.data_dir = f'{root_dir}/data/nq'\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args.data_config.test_dataset.file = 'test_natural_questions_50_bootleg.jsonl'\n",
    "\n",
    "# set the embedding paths \n",
    "config_args.data_config.emb_dir =  f'{root_dir}/data/emb_data'\n",
    "config_args.data_config.word_embedding.cache_dir =  f'{root_dir}/pretrained_bert_models'\n",
    "\n",
    "# set the save directory \n",
    "config_args.run_config.save_dir = f'{root_dir}/results'\n",
    "\n",
    "# set whether to run inference on the CPU\n",
    "config_args.run_config.cpu = use_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation in `dump_embs` mode to dump predictions and contextualized entity embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-15 14:49:04,471 PyTorch version 1.5.0 available.\n",
      "2020-09-15 14:49:05,626 Loading entity_symbols...\n",
      "2020-09-15 14:49:54,576 Loaded entity_symbols with 5222808 entities.\n",
      "2020-09-15 14:49:54,585 Loading slices...\n",
      "2020-09-15 14:49:57,301 Finished loading slices.\n",
      "2020-09-15 14:49:57,306 Loading dataset...\n",
      "2020-09-15 14:49:57,309 Finished loading dataset.\n",
      "2020-09-15 14:50:12,811 Sampled 50 indices from dataset (dev/test) for evaluation.\n",
      "2020-09-15 14:50:13,051 Loading embeddings...\n",
      "2020-09-15 14:50:36,690 Finished loading embeddings.\n",
      "2020-09-15 14:50:36,845 Loading model from /dfs/scratch1/mleszczy/bootleg-internal//models/bootleg_wiki/bootleg_model.pt...\n",
      "2020-09-15 14:50:39,408 Successfully loaded model from /dfs/scratch1/mleszczy/bootleg-internal//models/bootleg_wiki/bootleg_model.pt starting from checkpoint epoch 2 and step 0.\n",
      "2020-09-15 14:50:39,436 ************************DUMPING PREDICTIONS FOR test_natural_questions_50_bootleg.jsonl************************\n",
      "2020-09-15 14:50:39,574 64 samples, 2 batches\n",
      "2020-09-15 14:50:44,342 Writing predictions...\n",
      "2020-09-15 14:50:44,345 Total number of mentions across all sentences: 100\n",
      "2020-09-15 14:50:44,393 Finished writing predictions to /dfs/scratch1/mleszczy/bootleg-internal//results/20200915_144904/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_labels.jsonl\n",
      "2020-09-15 14:50:44,449 Saving contextual entity embeddings to /dfs/scratch1/mleszczy/bootleg-internal//results/20200915_144904/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_embs.npy\n"
     ]
    }
   ],
   "source": [
    "bootleg_label_file, bootleg_emb_file = run.model_eval(args=config_args, mode=\"dump_embs\", logger=logger, is_writer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the overall quality of the end-to-end pipeline via precision / recall metrics, where the *recall* indicates what proportion of the hand-labelled mentions Bootleg correctly detects and disambiguates, and *precision* indicates what proportion of the mentions that Bootleg labels are correct. For instance, if Bootleg only labelled the few mentions it was very confident in, then it would have a low recall and high precision.\n",
    "\n",
    "To detect if mentions match the hand-labelled mention spans, we allow for +1/-1 word in the left span boundaries (e.g., 'the wizard of oz' and 'wizard of oz' are counted as the same mention). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.71 (55/78)\n",
      "Precision: 0.6 (55/92)\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_precision_and_recall\n",
    "\n",
    "bootleg_errors = compute_precision_and_recall(orig_label_file=nq_sample_orig, \n",
    "                                              new_label_file=bootleg_label_file, \n",
    "                                              threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze three classes of errors in the end-to-end pipeline below: \n",
    "1. *Missing mentions*: Fail to extract the mention \n",
    "2. *Wrong entity*: Correctly extract the mention but disambiguate to the wrong candidate  \n",
    "3. *Extra mentions*: Label a mention that is not hand-labelled as a mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>is there an active volcano in new zealand</td>\n",
       "      <td>[new zealand]</td>\n",
       "      <td>[Q664]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[active volcano, new zealand]</td>\n",
       "      <td>[[3, 5], [6, 8]]</td>\n",
       "      <td>[Q8072, NC]</td>\n",
       "      <td>[0.735, 0.283]</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>where is israel located on the world map</td>\n",
       "      <td>[israel, world map]</td>\n",
       "      <td>[Q801, Q653848]</td>\n",
       "      <td>[[2, 3], [6, 8]]</td>\n",
       "      <td>[israel, world map]</td>\n",
       "      <td>[[2, 3], [6, 8]]</td>\n",
       "      <td>[NC, Q653848]</td>\n",
       "      <td>[0.163, 0.908]</td>\n",
       "      <td>israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>who played in the last 3 nba finals</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[Q842375]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[NC]</td>\n",
       "      <td>[0.115]</td>\n",
       "      <td>nba finals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>when did rangers last win the scottish cup</td>\n",
       "      <td>[rangers, scottish cup]</td>\n",
       "      <td>[Q19597, Q308822]</td>\n",
       "      <td>[[2, 3], [6, 8]]</td>\n",
       "      <td>[rangers, scottish cup]</td>\n",
       "      <td>[[2, 3], [6, 8]]</td>\n",
       "      <td>[Q19597, NC]</td>\n",
       "      <td>[0.696, 0.116]</td>\n",
       "      <td>scottish cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_idx                                    sentence  \\\n",
       "0         7   is there an active volcano in new zealand   \n",
       "1        23    where is israel located on the world map   \n",
       "2        32         who played in the last 3 nba finals   \n",
       "3        45  when did rangers last win the scottish cup   \n",
       "\n",
       "              gold_aliases          gold_qids        gold_spans  \\\n",
       "0            [new zealand]             [Q664]          [[6, 8]]   \n",
       "1      [israel, world map]    [Q801, Q653848]  [[2, 3], [6, 8]]   \n",
       "2             [nba finals]          [Q842375]          [[6, 8]]   \n",
       "3  [rangers, scottish cup]  [Q19597, Q308822]  [[2, 3], [6, 8]]   \n",
       "\n",
       "                    pred_aliases        pred_spans      pred_qids  \\\n",
       "0  [active volcano, new zealand]  [[3, 5], [6, 8]]    [Q8072, NC]   \n",
       "1            [israel, world map]  [[2, 3], [6, 8]]  [NC, Q653848]   \n",
       "2                   [nba finals]          [[6, 8]]           [NC]   \n",
       "3        [rangers, scottish cup]  [[2, 3], [6, 8]]   [Q19597, NC]   \n",
       "\n",
       "       pred_probs         error  \n",
       "0  [0.735, 0.283]   new zealand  \n",
       "1  [0.163, 0.908]        israel  \n",
       "2         [0.115]    nba finals  \n",
       "3  [0.696, 0.116]  scottish cup  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['missing_mention'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mentions above get filtered because we set the probability threshold to 0.3 to help filter extra mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>35</td>\n",
       "      <td>reasons why south africa should include renewable energy in its energy mix</td>\n",
       "      <td>[south africa, renewable energy]</td>\n",
       "      <td>[Q258, Q12705]</td>\n",
       "      <td>[[2, 4], [6, 8]]</td>\n",
       "      <td>[south africa, renewable energy, energy mix]</td>\n",
       "      <td>[[2, 4], [6, 8], [10, 12]]</td>\n",
       "      <td>[Q11409, Q12705, Q1341346]</td>\n",
       "      <td>[0.696, 0.501, 1.0]</td>\n",
       "      <td>south africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40</td>\n",
       "      <td>where does the last name vigil come from</td>\n",
       "      <td>[vigil]</td>\n",
       "      <td>[Q16878937]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[vigil]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[Q16948716]</td>\n",
       "      <td>[0.488]</td>\n",
       "      <td>vigil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>34</td>\n",
       "      <td>who 's doing the halftime show in 2018</td>\n",
       "      <td>[halftime show]</td>\n",
       "      <td>[Q902899]</td>\n",
       "      <td>[[4, 6]]</td>\n",
       "      <td>[halftime show]</td>\n",
       "      <td>[[4, 6]]</td>\n",
       "      <td>[Q7642202]</td>\n",
       "      <td>[0.482]</td>\n",
       "      <td>halftime show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>48</td>\n",
       "      <td>what was the japanese motivation for bombing pearl harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[Q188712, Q127091]</td>\n",
       "      <td>[[3, 4], [7, 9]]</td>\n",
       "      <td>[japanese, motivation, bombing, pearl harbor]</td>\n",
       "      <td>[[3, 4], [4, 5], [6, 7], [7, 9]]</td>\n",
       "      <td>[Q184425, Q644302, Q52418, Q52418]</td>\n",
       "      <td>[0.744, 0.986, 0.985, 0.706]</td>\n",
       "      <td>pearl harbor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>48</td>\n",
       "      <td>what was the japanese motivation for bombing pearl harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[Q188712, Q127091]</td>\n",
       "      <td>[[3, 4], [7, 9]]</td>\n",
       "      <td>[japanese, motivation, bombing, pearl harbor]</td>\n",
       "      <td>[[3, 4], [4, 5], [6, 7], [7, 9]]</td>\n",
       "      <td>[Q184425, Q644302, Q52418, Q52418]</td>\n",
       "      <td>[0.744, 0.986, 0.985, 0.706]</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx  \\\n",
       "12        35   \n",
       "14        40   \n",
       "11        34   \n",
       "17        48   \n",
       "16        48   \n",
       "\n",
       "                                                                      sentence  \\\n",
       "12  reasons why south africa should include renewable energy in its energy mix   \n",
       "14                                    where does the last name vigil come from   \n",
       "11                                      who 's doing the halftime show in 2018   \n",
       "17                   what was the japanese motivation for bombing pearl harbor   \n",
       "16                   what was the japanese motivation for bombing pearl harbor   \n",
       "\n",
       "                        gold_aliases           gold_qids        gold_spans  \\\n",
       "12  [south africa, renewable energy]      [Q258, Q12705]  [[2, 4], [6, 8]]   \n",
       "14                           [vigil]         [Q16878937]          [[5, 6]]   \n",
       "11                   [halftime show]           [Q902899]          [[4, 6]]   \n",
       "17          [japanese, pearl harbor]  [Q188712, Q127091]  [[3, 4], [7, 9]]   \n",
       "16          [japanese, pearl harbor]  [Q188712, Q127091]  [[3, 4], [7, 9]]   \n",
       "\n",
       "                                     pred_aliases  \\\n",
       "12   [south africa, renewable energy, energy mix]   \n",
       "14                                        [vigil]   \n",
       "11                                [halftime show]   \n",
       "17  [japanese, motivation, bombing, pearl harbor]   \n",
       "16  [japanese, motivation, bombing, pearl harbor]   \n",
       "\n",
       "                          pred_spans                           pred_qids  \\\n",
       "12        [[2, 4], [6, 8], [10, 12]]          [Q11409, Q12705, Q1341346]   \n",
       "14                          [[5, 6]]                         [Q16948716]   \n",
       "11                          [[4, 6]]                          [Q7642202]   \n",
       "17  [[3, 4], [4, 5], [6, 7], [7, 9]]  [Q184425, Q644302, Q52418, Q52418]   \n",
       "16  [[3, 4], [4, 5], [6, 7], [7, 9]]  [Q184425, Q644302, Q52418, Q52418]   \n",
       "\n",
       "                      pred_probs          error  \n",
       "12           [0.696, 0.501, 1.0]   south africa  \n",
       "14                       [0.488]          vigil  \n",
       "11                       [0.482]  halftime show  \n",
       "17  [0.744, 0.986, 0.985, 0.706]   pearl harbor  \n",
       "16  [0.744, 0.986, 0.985, 0.706]       japanese  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['wrong_entity']).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the errors Bootleg makes is predicting too general of a candidate (e.g. Oregon State Beavers instead of Oregon State Beavers baseball). Other errors are due to ambiguous sentences (e.g. \"cast of characters in fiddler on the roof\" -> should this be the movie or the musical?). Finally another bucket of errors suggests that we need to boost certain training signals -- this is an area we're actively pursuing in Bootleg with an investigation of model guidability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27</td>\n",
       "      <td>i see the river tiber foaming with much blood</td>\n",
       "      <td>[river tiber]</td>\n",
       "      <td>[Q13712]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "      <td>[river tiber, foaming, blood]</td>\n",
       "      <td>[[3, 5], [5, 6], [8, 9]]</td>\n",
       "      <td>[Q13712, Q7243541, Q7873]</td>\n",
       "      <td>[1.0, 1.0, 0.9]</td>\n",
       "      <td>foaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>42</td>\n",
       "      <td>who proposed the coordinate system to describe the position of a point in a plane accurately</td>\n",
       "      <td>[coordinate system]</td>\n",
       "      <td>[Q62912]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "      <td>[coordinate system, plane]</td>\n",
       "      <td>[[3, 5], [14, 15]]</td>\n",
       "      <td>[Q62912, Q62912]</td>\n",
       "      <td>[0.964, 0.956]</td>\n",
       "      <td>plane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>game of thrones season 1 white hair girl</td>\n",
       "      <td>[game of thrones season 1]</td>\n",
       "      <td>[Q1658029]</td>\n",
       "      <td>[[0, 5]]</td>\n",
       "      <td>[game of thrones season 1, white hair]</td>\n",
       "      <td>[[0, 5], [5, 7]]</td>\n",
       "      <td>[Q1658029, Q1048314]</td>\n",
       "      <td>[0.985, 0.515]</td>\n",
       "      <td>white hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>24</td>\n",
       "      <td>who played smiley in tinker tailor soldier spy</td>\n",
       "      <td>[tinker tailor soldier spy]</td>\n",
       "      <td>[Q681962]</td>\n",
       "      <td>[[4, 8]]</td>\n",
       "      <td>[smiley, tinker tailor soldier spy]</td>\n",
       "      <td>[[2, 3], [4, 8]]</td>\n",
       "      <td>[Q11241, Q582811]</td>\n",
       "      <td>[0.885, 0.697]</td>\n",
       "      <td>smiley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>is there an active volcano in new zealand</td>\n",
       "      <td>[new zealand]</td>\n",
       "      <td>[Q664]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[active volcano, new zealand]</td>\n",
       "      <td>[[3, 5], [6, 8]]</td>\n",
       "      <td>[Q8072, NC]</td>\n",
       "      <td>[0.735, 0.283]</td>\n",
       "      <td>active volcano</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx  \\\n",
       "10        27   \n",
       "15        42   \n",
       "8         22   \n",
       "9         24   \n",
       "3          7   \n",
       "\n",
       "                                                                                        sentence  \\\n",
       "10                                                 i see the river tiber foaming with much blood   \n",
       "15  who proposed the coordinate system to describe the position of a point in a plane accurately   \n",
       "8                                                       game of thrones season 1 white hair girl   \n",
       "9                                                 who played smiley in tinker tailor soldier spy   \n",
       "3                                                      is there an active volcano in new zealand   \n",
       "\n",
       "                   gold_aliases   gold_qids gold_spans  \\\n",
       "10                [river tiber]    [Q13712]   [[3, 5]]   \n",
       "15          [coordinate system]    [Q62912]   [[3, 5]]   \n",
       "8    [game of thrones season 1]  [Q1658029]   [[0, 5]]   \n",
       "9   [tinker tailor soldier spy]   [Q681962]   [[4, 8]]   \n",
       "3                 [new zealand]      [Q664]   [[6, 8]]   \n",
       "\n",
       "                              pred_aliases                pred_spans  \\\n",
       "10           [river tiber, foaming, blood]  [[3, 5], [5, 6], [8, 9]]   \n",
       "15              [coordinate system, plane]        [[3, 5], [14, 15]]   \n",
       "8   [game of thrones season 1, white hair]          [[0, 5], [5, 7]]   \n",
       "9      [smiley, tinker tailor soldier spy]          [[2, 3], [4, 8]]   \n",
       "3            [active volcano, new zealand]          [[3, 5], [6, 8]]   \n",
       "\n",
       "                    pred_qids       pred_probs           error  \n",
       "10  [Q13712, Q7243541, Q7873]  [1.0, 1.0, 0.9]         foaming  \n",
       "15           [Q62912, Q62912]   [0.964, 0.956]           plane  \n",
       "8        [Q1658029, Q1048314]   [0.985, 0.515]      white hair  \n",
       "9           [Q11241, Q582811]   [0.885, 0.697]          smiley  \n",
       "3                 [Q8072, NC]   [0.735, 0.283]  active volcano  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['extra_mention']).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Bootleg may detect and label extraneous mentions that were not hand-labelled. Setting the threshold higher helps to reduce these predictions, as does using a 'NC' candidate for training, which Bootleg also supports . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare to TAGME \n",
    "\n",
    "To get a sense of how Bootleg is doing compared to other systems, we evaluate [TAGME](https://arxiv.org/pdf/1006.3498.pdf), an existing tool to extract and disambiguate mentions. To run TAGME, you need to get a (free) authorization token. Instructions for obtaining a token are [here](https://sobigdata.d4science.org/web/tagme/tagme-help). You will need to verify your account and then follow the \"access the VRE\") link. We've also provided the file with TAGME labels for a given threshold for download if you want to skip the authorization token.\n",
    "\n",
    "We note that unlike TAGME, Bootleg also outputs contextual entity embeddings which can be loaded for use in downstream tasks (e.g. relation extraction, question answering). Check out the Entity Embedding tutorial for more details! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tagme\n",
    "# Set the authorization token for subsequent calls.\n",
    "tagme.GCUBE_TOKEN = # FILL IN WITH YOUR TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagme_label_file = f'{root_dir}/data/nq/test_natural_questions_50_tagme.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a token, skip the cell below and load the pre-generated TAGME labels. If you do have a token, you can play with changing the threshold below and see how it affects the results. Increasing the threshold increases the precision but decreases the recall as TAGME, as TAGME will label fewer mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a mapping from Wikipedia pageids to Wikidata QIDs to get the QIDs predicted by TAGME \n",
    "wpid2qid = ujson.load(open(f'{root_dir}/data/wiki_entity_data/entity_mappings/wpid2qid.json'))\n",
    "\n",
    "# As the threshold increases, the precision increases, but the recall decreases\n",
    "tagme_annotate(in_file=nq_sample_orig, out_file=tagme_label_file, threshold=0.3, wpid2qid=wpid2qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.63 (49/78)\n",
      "Precision: 0.58 (49/84)\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_precision_and_recall\n",
    "tagme_errors = compute_precision_and_recall(orig_label_file=nq_sample_orig, \n",
    "                                            new_label_file=tagme_label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that TAGME has slightly worse recall than Bootleg, when the precisions are set to be comparable (changing either TAGME or Bootleg's threshold will change the recall/precision values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Annotate On-the-Fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To annotate individual sentences with Bootleg, we  also support annotate-on-the-fly mode. \n",
    "\n",
    "**Note that Annotator is not optimized and is only intended to be used for quick experimentation and for demos. We recommend using the above pipeline (`extract_mentions` and `model_eval` functions) for evaluating datasets. These functions leverage multiprocessing, caching of preprocessed data, and batching to speed up evaluation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we create an annotator object. This loads the model and entity databases. We use the `config_args` loaded from the previous step. Note it takes several minutes for the initial load of the model and the entity data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-15 15:53:25,247 PyTorch version 1.5.0 available.\n",
      "2020-09-15 15:53:34,675 Loading embeddings...\n",
      "2020-09-15 15:53:58,247 Finished loading embeddings.\n",
      "2020-09-15 15:54:19,386 Loading candidate mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7970529/7970529 [00:11<00:00, 670665.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-15 15:54:31,279 Loaded candidate mapping with 7970529 aliases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bootleg.annotator import Annotator\n",
    "\n",
    "ann = Annotator(config_args=config_args, cand_map=cand_map, device='cuda' if not use_cpu else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to TAGME, we allow setting a threshold to only return mentions with labels greater than some probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.set_threshold(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in sentences to see what Bootleg predicts! Bootleg outputs the QIDs (or \"NC\" for \"No Candidate\"), the associated probabilities, and the title for each mention. The QIDs map to Wikidata -- to look them up you can use https://www.wikidata.org/wiki/Q1454 and replace the QID. \"NC\" means Bootleg did not find a good match among the candidates in the candidate list given the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q1517373', 'Q1454'],\n",
       " [1.0, 0.9959885478019714],\n",
       " ['Outer Banks', 'North Carolina'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"where is the outer banks in north carolina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q487330'], [0.8602001070976257], ['Fiddler on the Roof'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"cast of characters in fiddler on the roof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the entity disambiguation problem can be quite tricky -- in the above example we predict the song \"Fiddler on the Roof\" the music instead of the hand-label of the movie (https://www.wikidata.org/wiki/Q934036). Giving additional cues may help though -- for instance, if we add \"the movie\", the prediction changes to the movie! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q934036'], [0.7369491457939148], ['Fiddler on the Roof (film)'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"cast of characters in the movie fiddler on the roof\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctx",
   "language": "python",
   "name": "ctx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
