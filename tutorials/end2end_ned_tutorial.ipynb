{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NED Tutorial\n",
    "\n",
    "In this tutorial, we walk through how to use Bootleg as an end-to-end pipeline to detect and label entities in a set of sentences. First, we show how to use Bootleg to detect and disambiguate mentions to entities. We then compare to an existing system named TAGME. Finally, we show how to use Bootleg to annotate individual sentences on the fly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how Bootleg performs on more natural language than we find in Wikipedia, we hand label the mentions and corresponding entities in 50 questions sampled from the [Natural Questions dataset (Google)](https://ai.google.com/research/NaturalQuestions). \n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- Pretrained Bootleg model and config [here](https://bootleg-emb.s3.amazonaws.com/models/2020_10_22/bootleg_wiki.tar.gz)*\n",
    "- Sample of Natural Questions with hand-labelled entities [here](https://bootleg-emb.s3.amazonaws.com/data/nq.tar.gz)\n",
    "- Entity data [here](https://bootleg-emb.s3.amazonaws.com/data/wiki_entity_data.tar.gz)*\n",
    "- Embedding data [here](https://bootleg-emb.s3.amazonaws.com/data/emb_data.tar.gz)*\n",
    "- Pretrained BERT model [here](https://bootleg-emb.s3.amazonaws.com/pretrained_bert_models.tar.gz)*\n",
    "\n",
    "*Same file as in benchmark tutorial and does not need to be re-downloaded.\n",
    "\n",
    "For convenience, you can run the commands below (from the root directory of the repo) to download all the above files and unpack them to `models`, `data`, and `pretrained_bert_models` directories. It will take several minutes to download all the files. \n",
    "\n",
    "    bash download_model.sh \n",
    "    bash download_data.sh \n",
    "    bash download_bert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/my_env_dawn/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/dfs/scratch0/lorr1/my_env_dawn/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import ujson\n",
    "from utils import load_mentions, tagme_annotate\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "root_dir = # FILL IN FULL PATH TO ROOT REPO DIRECTORY HERE \n",
    "cand_map = f'{root_dir}/data/wiki_entity_data/entity_mappings/alias2qids_wiki.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU with at least 12GB of memory available, set the below to `False` to run inference on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cpu = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect Mentions\n",
    "Bootleg uses a simple mention extraction algorithm that extracts mentions using a given candidate map. We will use a Wikipedia candidate map that we mined using Wikipedia anchor links and Wikidata aliases for a total of ~8 million mentions (provided in the Requirements section of this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input dataset for the end-to-end pipeline, we assume a jsonlines file with a single dictionary with the key \"sentence\" and value as the text of the sentence, per line. For instance, you may have a file with the lines:\n",
    "\n",
    "    {\"sentence\": \"who did the voice of the magician in frosty the snowman\"}\n",
    "    {\"sentence\": \"what is considered the outer banks in north carolina\"}\n",
    "    \n",
    "Below, we have additional keys to keep track of the hand-labelled mentions, but this is purely for evaluating the quality of the end-to-end pipeline and is not needed in the common use cases of using Bootleg to detect and label mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_sample_orig = f'{root_dir}/data/nq/test_natural_questions_50.jsonl'\n",
    "nq_sample_bootleg = f'{root_dir}/data/nq/test_natural_questions_50_bootleg.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 17:17:42,065 Loading candidate mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8034754/8034754 [00:16<00:00, 482916.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 17:17:58,709 Loaded candidate mapping with 8034754 aliases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 17:18:12,381 Using 8 workers...\n",
      "2020-10-21 17:18:12,382 Reading in /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/data/nq/test_natural_questions_50.jsonl\n",
      "2020-10-21 17:18:12,658 Wrote out data chunks in 0.27s\n",
      "2020-10-21 17:18:12,659 Calling subprocess...\n",
      "2020-10-21 17:18:13,711 Merging files...\n",
      "2020-10-21 17:18:13,756 Removing temporary files...\n",
      "2020-10-21 17:18:13,978 Finished in 1.6176435947418213 seconds. Wrote out to /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/data/nq/test_natural_questions_50_bootleg.jsonl\n"
     ]
    }
   ],
   "source": [
    "from bootleg.extract_mentions import extract_mentions\n",
    "extract_mentions(in_filepath=nq_sample_orig, out_filepath=nq_sample_bootleg, cand_map_file=cand_map, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at a sample of the extracted mentions, we can compare the mention extraction phase to the hand-labelled mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>aliases_hand</th>\n",
       "      <th>spans_hand</th>\n",
       "      <th>aliases_bootleg</th>\n",
       "      <th>spans_bootleg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the pair of hand drums used in indian classical music is called</td>\n",
       "      <td>[indian classical music]</td>\n",
       "      <td>[[7, 10]]</td>\n",
       "      <td>[hand, drums, indian classical music]</td>\n",
       "      <td>[[3, 4], [4, 5], [7, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>is it a bank holiday today in spain</td>\n",
       "      <td>[bank holiday, spain]</td>\n",
       "      <td>[[3, 5], [7, 8]]</td>\n",
       "      <td>[bank holiday, spain]</td>\n",
       "      <td>[[3, 5], [7, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>who opened and closed the 1960 winter olympics</td>\n",
       "      <td>[1960 winter olympics]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "      <td>[1960 winter olympics]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is there an active volcano in new zealand</td>\n",
       "      <td>[new zealand]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[active volcano, new zealand]</td>\n",
       "      <td>[[3, 5], [6, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the u.s. supreme court hears appeals from circuit courts</td>\n",
       "      <td>[u.s. supreme court, circuit courts]</td>\n",
       "      <td>[[1, 4], [7, 9]]</td>\n",
       "      <td>[us supreme court, circuit courts]</td>\n",
       "      <td>[[1, 4], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>who played the bank robber in dirty harry</td>\n",
       "      <td>[dirty harry]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[bank robber, dirty harry]</td>\n",
       "      <td>[[3, 5], [6, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hitchhiker 's guide to the galaxy slartibartfast quotes</td>\n",
       "      <td>[hitchhiker 's guide to the galaxy, slartibartfast]</td>\n",
       "      <td>[[0, 6], [6, 7]]</td>\n",
       "      <td>[hitchhiker s guide to the galaxy, slartibartfast]</td>\n",
       "      <td>[[0, 6], [6, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>what was dennis hopper 's bike in easy rider</td>\n",
       "      <td>[dennis hopper, easy rider]</td>\n",
       "      <td>[[2, 4], [7, 9]]</td>\n",
       "      <td>[dennis hopper, bike, easy rider]</td>\n",
       "      <td>[[2, 4], [5, 6], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>who is mariah carey talking about in we belong together</td>\n",
       "      <td>[mariah carey, we belong together]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "      <td>[mariah carey, we belong together]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>why does the author say that the vampire in nosferatu is named count orlok and not count dracula</td>\n",
       "      <td>[nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[[9, 10], [12, 14], [16, 18]]</td>\n",
       "      <td>[vampire, nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[[7, 8], [9, 10], [12, 14], [16, 18]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>cast of characters in fiddler on the roof</td>\n",
       "      <td>[fiddler on the roof]</td>\n",
       "      <td>[[4, 8]]</td>\n",
       "      <td>[fiddler on the roof]</td>\n",
       "      <td>[[4, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>game of thrones season 1 white hair girl</td>\n",
       "      <td>[game of thrones season 1]</td>\n",
       "      <td>[[0, 5]]</td>\n",
       "      <td>[game of thrones season 1, white hair]</td>\n",
       "      <td>[[0, 5], [5, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>when was the wizard of oz made in technicolor</td>\n",
       "      <td>[wizard of oz, technicolor]</td>\n",
       "      <td>[[3, 6], [8, 9]]</td>\n",
       "      <td>[the wizard of oz, in technicolor]</td>\n",
       "      <td>[[2, 6], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>landmark supreme court cases dealing with the first amendment</td>\n",
       "      <td>[supreme court, first amendment]</td>\n",
       "      <td>[[1, 3], [7, 9]]</td>\n",
       "      <td>[landmark, supreme court, first amendment]</td>\n",
       "      <td>[[0, 1], [1, 3], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>reasons why south africa should include renewable energy in its energy mix</td>\n",
       "      <td>[south africa, renewable energy]</td>\n",
       "      <td>[[2, 4], [6, 8]]</td>\n",
       "      <td>[south africa, renewable energy, energy mix]</td>\n",
       "      <td>[[2, 4], [6, 8], [10, 12]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            sentence  \\\n",
       "21                                   the pair of hand drums used in indian classical music is called   \n",
       "17                                                               is it a bank holiday today in spain   \n",
       "26                                                    who opened and closed the 1960 winter olympics   \n",
       "7                                                          is there an active volcano in new zealand   \n",
       "5                                           the u.s. supreme court hears appeals from circuit courts   \n",
       "19                                                         who played the bank robber in dirty harry   \n",
       "11                                           hitchhiker 's guide to the galaxy slartibartfast quotes   \n",
       "12                                                      what was dennis hopper 's bike in easy rider   \n",
       "47                                           who is mariah carey talking about in we belong together   \n",
       "6   why does the author say that the vampire in nosferatu is named count orlok and not count dracula   \n",
       "49                                                         cast of characters in fiddler on the roof   \n",
       "22                                                          game of thrones season 1 white hair girl   \n",
       "38                                                     when was the wizard of oz made in technicolor   \n",
       "37                                     landmark supreme court cases dealing with the first amendment   \n",
       "35                        reasons why south africa should include renewable energy in its energy mix   \n",
       "\n",
       "                                           aliases_hand  \\\n",
       "21                             [indian classical music]   \n",
       "17                                [bank holiday, spain]   \n",
       "26                               [1960 winter olympics]   \n",
       "7                                         [new zealand]   \n",
       "5                  [u.s. supreme court, circuit courts]   \n",
       "19                                        [dirty harry]   \n",
       "11  [hitchhiker 's guide to the galaxy, slartibartfast]   \n",
       "12                          [dennis hopper, easy rider]   \n",
       "47                   [mariah carey, we belong together]   \n",
       "6               [nosferatu, count orlok, count dracula]   \n",
       "49                                [fiddler on the roof]   \n",
       "22                           [game of thrones season 1]   \n",
       "38                          [wizard of oz, technicolor]   \n",
       "37                     [supreme court, first amendment]   \n",
       "35                     [south africa, renewable energy]   \n",
       "\n",
       "                       spans_hand  \\\n",
       "21                      [[7, 10]]   \n",
       "17               [[3, 5], [7, 8]]   \n",
       "26                       [[5, 8]]   \n",
       "7                        [[6, 8]]   \n",
       "5                [[1, 4], [7, 9]]   \n",
       "19                       [[6, 8]]   \n",
       "11               [[0, 6], [6, 7]]   \n",
       "12               [[2, 4], [7, 9]]   \n",
       "47              [[2, 4], [7, 10]]   \n",
       "6   [[9, 10], [12, 14], [16, 18]]   \n",
       "49                       [[4, 8]]   \n",
       "22                       [[0, 5]]   \n",
       "38               [[3, 6], [8, 9]]   \n",
       "37               [[1, 3], [7, 9]]   \n",
       "35               [[2, 4], [6, 8]]   \n",
       "\n",
       "                                       aliases_bootleg  \\\n",
       "21               [hand, drums, indian classical music]   \n",
       "17                               [bank holiday, spain]   \n",
       "26                              [1960 winter olympics]   \n",
       "7                        [active volcano, new zealand]   \n",
       "5                   [us supreme court, circuit courts]   \n",
       "19                          [bank robber, dirty harry]   \n",
       "11  [hitchhiker s guide to the galaxy, slartibartfast]   \n",
       "12                   [dennis hopper, bike, easy rider]   \n",
       "47                  [mariah carey, we belong together]   \n",
       "6     [vampire, nosferatu, count orlok, count dracula]   \n",
       "49                               [fiddler on the roof]   \n",
       "22              [game of thrones season 1, white hair]   \n",
       "38                  [the wizard of oz, in technicolor]   \n",
       "37          [landmark, supreme court, first amendment]   \n",
       "35        [south africa, renewable energy, energy mix]   \n",
       "\n",
       "                            spans_bootleg  \n",
       "21              [[3, 4], [4, 5], [7, 10]]  \n",
       "17                       [[3, 5], [7, 8]]  \n",
       "26                               [[5, 8]]  \n",
       "7                        [[3, 5], [6, 8]]  \n",
       "5                        [[1, 4], [7, 9]]  \n",
       "19                       [[3, 5], [6, 8]]  \n",
       "11                       [[0, 6], [6, 7]]  \n",
       "12               [[2, 4], [5, 6], [7, 9]]  \n",
       "47                      [[2, 4], [7, 10]]  \n",
       "6   [[7, 8], [9, 10], [12, 14], [16, 18]]  \n",
       "49                               [[4, 8]]  \n",
       "22                       [[0, 5], [5, 7]]  \n",
       "38                       [[2, 6], [7, 9]]  \n",
       "37               [[0, 1], [1, 3], [7, 9]]  \n",
       "35             [[2, 4], [6, 8], [10, 12]]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_mentions_df = load_mentions(nq_sample_orig)\n",
    "bootleg_mentions_df = load_mentions(nq_sample_bootleg)\n",
    "\n",
    "# join dataframes and sample\n",
    "pd.merge(orig_mentions_df, bootleg_mentions_df, on=['sentence'], suffixes=['_hand', '_bootleg']).sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample above, we see that generally Bootleg detects the same mentions as the hand-labelled mentions, however sometimes Bootleg extracts extra mentions (e.g \"foaming\" in \"i see the river tiber foaming with much blood\"). This is expected as we would rather the mention detection step filter out too few mentions than too many. It will be the job of the backbone model and postprocessing to filter out these extra mentions, by either thresholding the prediction probability or predicting a candidate that represents \"No Candidate\" (we refer to this as \"NC\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disambiguate Mentions to Entities\n",
    "\n",
    "We run inference using a pretrained Bootleg model to disambiguate the extracted mentions to Wikidata QIDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model config so we can set additional parameters and load the saved model during evaluation. We need to update the config parameters to point to the downloaded model checkpoint and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg import run\n",
    "from bootleg.utils.parser_utils import get_full_config\n",
    "\n",
    "config_path = f'{root_dir}/models/bootleg_wiki/bootleg_config.json'\n",
    "config_args = get_full_config(config_path)\n",
    "\n",
    "# decrease number of data threads as this is a small file\n",
    "config_args.run_config.dataset_threads = 2\n",
    "\n",
    "# set the model checkpoint path \n",
    "config_args.run_config.init_checkpoint = f'{root_dir}/models/bootleg_wiki/bootleg_model.pt'\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args.data_config.entity_dir = f'{root_dir}/data/wiki_entity_data'\n",
    "config_args.data_config.alias_cand_map = 'alias2qids_wiki.json'\n",
    "\n",
    "# set the data path and RSS500 test file \n",
    "config_args.data_config.data_dir = f'{root_dir}/data/nq'\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args.data_config.test_dataset.file = 'test_natural_questions_50_bootleg.jsonl'\n",
    "\n",
    "# set the embedding paths \n",
    "config_args.data_config.emb_dir =  f'{root_dir}/data/emb_data'\n",
    "config_args.data_config.word_embedding.cache_dir =  f'{root_dir}/pretrained_bert_models'\n",
    "\n",
    "# set the save directory \n",
    "config_args.run_config.save_dir = f'{root_dir}/results'\n",
    "\n",
    "# set whether to run inference on the CPU\n",
    "config_args.run_config.cpu = use_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation in `dump_embs` mode to dump predictions and contextualized entity embeddings. Note that this command is about 10 times slower using a notebook than on the command line. To speed up the next command, run the following on the command line first. Then come back and run the next cell.\n",
    "\n",
    "```\n",
    "python3 -m bootleg.run --mode dump_embs \\\n",
    "    --config_script <root_dir>/models/bootleg_wiki/bootleg_config.json \\\n",
    "    --run_config.dataset_threads 2 \\\n",
    "    --run_config.init_checkpoint <root_dir>/models/bootleg_wiki/bootleg_model.pt \\\n",
    "    --data_config.entity_dir <root_dir>/data/wiki_entity_data \\\n",
    "    --data_config.alias_cand_map alias2qids_wiki.json \\\n",
    "    --data_config.data_dir <root_dir>/data/nq \\\n",
    "    --data_config.test_dataset.file test_natural_questions_50_bootleg.jsonl \\\n",
    "    --data_config.emb_dir <root_dir>/data/emb_data \\\n",
    "    --data_config.word_embedding.cache_dir <root_dir>/pretrained_bert_models \\\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 18:36:44,109 Loading entity_symbols...\n",
      "2020-10-21 18:37:30,833 Loaded entity_symbols with 5310039 entities.\n",
      "2020-10-21 18:37:30,861 Loading slices...\n",
      "2020-10-21 18:48:04,167 Finished loading slices.\n",
      "2020-10-21 18:49:28,392 Loading dataset...\n",
      "2020-10-21 19:23:58,480 Finished loading dataset.\n",
      "2020-10-21 19:24:05,298 Loading embeddings...\n",
      "2020-10-21 19:24:29,538 Finished loading embeddings.\n",
      "2020-10-21 19:24:29,729 Loading model from /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/models/bootleg_wiki/bootleg_model.pt...\n",
      "2020-10-21 19:24:55,067 Successfully loaded model from /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/models/bootleg_wiki/bootleg_model.pt starting from checkpoint epoch 1 and step 0.\n",
      "2020-10-21 19:24:55,141 ************************DUMPING PREDICTIONS FOR test_natural_questions_50_bootleg.jsonl************************\n",
      "2020-10-21 19:24:55,230 64 samples, 4 batches, 50 len dataset\n",
      "2020-10-21 19:25:07,864 Writing predictions to /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/results/20200914_104853/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_labels.jsonl...\n",
      "2020-10-21 19:25:07,878 Total number of mentions across all sentences: 104\n",
      "2020-10-21 19:25:08,643 Finished writing predictions to /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/results/20200914_104853/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_labels.jsonl\n",
      "2020-10-21 19:25:08,730 Saving contextual entity embeddings to /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/results/20200914_104853/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_embs.npy\n",
      "2020-10-21 19:25:08,731 Wrote predictions to /dfs/scratch0/lorr1/bootleg/bootleg-internal/new_tutorial_data/results/20200914_104853/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_labels.jsonl\n"
     ]
    }
   ],
   "source": [
    "bootleg_label_file, bootleg_emb_file = run.model_eval(args=config_args, mode=\"dump_embs\", logger=logger, is_writer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the overall quality of the end-to-end pipeline via precision / recall metrics, where the *recall* indicates what proportion of the hand-labelled mentions Bootleg correctly detects and disambiguates, and *precision* indicates what proportion of the mentions that Bootleg labels are correct. For instance, if Bootleg only labelled the few mentions it was very confident in, then it would have a low recall and high precision.\n",
    "\n",
    "To detect if mentions match the hand-labelled mention spans, we allow for +1/-1 word in the left span boundaries (e.g., 'the wizard of oz' and 'wizard of oz' are counted as the same mention). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.73 (57/78)\n",
      "Precision: 0.58 (57/99)\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_precision_and_recall\n",
    "\n",
    "bootleg_errors = compute_precision_and_recall(orig_label_file=nq_sample_orig, \n",
    "                                              new_label_file=bootleg_label_file, \n",
    "                                              threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze three classes of errors in the end-to-end pipeline below: \n",
    "1. *Missing mentions*: Fail to extract the mention \n",
    "2. *Wrong entity*: Correctly extract the mention but disambiguate to the wrong candidate  \n",
    "3. *Extra mentions*: Label a mention that is not hand-labelled as a mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>who opened and closed the 1960 winter olympics</td>\n",
       "      <td>[1960 winter olympics]</td>\n",
       "      <td>[Q9634]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "      <td>[1960 winter olympics]</td>\n",
       "      <td>[[5, 8]]</td>\n",
       "      <td>[NC]</td>\n",
       "      <td>[0.236]</td>\n",
       "      <td>1960 winter olympics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>who played in the last 3 nba finals</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[Q842375]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[NC]</td>\n",
       "      <td>[0.143]</td>\n",
       "      <td>nba finals</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_idx                                        sentence  \\\n",
       "0        26  who opened and closed the 1960 winter olympics   \n",
       "1        32             who played in the last 3 nba finals   \n",
       "\n",
       "             gold_aliases  gold_qids gold_spans            pred_aliases  \\\n",
       "0  [1960 winter olympics]    [Q9634]   [[5, 8]]  [1960 winter olympics]   \n",
       "1            [nba finals]  [Q842375]   [[6, 8]]            [nba finals]   \n",
       "\n",
       "  pred_spans pred_qids pred_probs                 error  \n",
       "0   [[5, 8]]      [NC]    [0.236]  1960 winter olympics  \n",
       "1   [[6, 8]]      [NC]    [0.143]            nba finals  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['missing_mention'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mentions above get filtered because we set the probability threshold to 0.3 to help filter extra mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1970 world cup semi final italy vs germany</td>\n",
       "      <td>[1970 world cup, italy, germany]</td>\n",
       "      <td>[Q132664, Q676899, Q43310]</td>\n",
       "      <td>[[0, 3], [5, 6], [7, 8]]</td>\n",
       "      <td>[1970 world cup, semi, italy, germany]</td>\n",
       "      <td>[[0, 3], [3, 4], [5, 6], [7, 8]]</td>\n",
       "      <td>[Q132664, Q992994, Q676899, Q37285]</td>\n",
       "      <td>[0.995, 0.811, 0.916, 0.508]</td>\n",
       "      <td>germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38</td>\n",
       "      <td>when was the wizard of oz made in technicolor</td>\n",
       "      <td>[wizard of oz, technicolor]</td>\n",
       "      <td>[Q193695, Q674564]</td>\n",
       "      <td>[[3, 6], [8, 9]]</td>\n",
       "      <td>[the wizard of oz, in technicolor]</td>\n",
       "      <td>[[2, 6], [7, 9]]</td>\n",
       "      <td>[Q193695, Q17412490]</td>\n",
       "      <td>[0.438, 1.0]</td>\n",
       "      <td>technicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36</td>\n",
       "      <td>when was the first freeway built in los angeles</td>\n",
       "      <td>[los angeles]</td>\n",
       "      <td>[Q65]</td>\n",
       "      <td>[[7, 9]]</td>\n",
       "      <td>[freeway, los angeles]</td>\n",
       "      <td>[[4, 5], [7, 9]]</td>\n",
       "      <td>[Q46622, Q104994]</td>\n",
       "      <td>[0.718, 0.716]</td>\n",
       "      <td>los angeles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>where is israel located on the world map</td>\n",
       "      <td>[israel, world map]</td>\n",
       "      <td>[Q801, Q653848]</td>\n",
       "      <td>[[2, 3], [6, 8]]</td>\n",
       "      <td>[israel, world map]</td>\n",
       "      <td>[[2, 3], [6, 8]]</td>\n",
       "      <td>[Q23792, Q653848]</td>\n",
       "      <td>[0.383, 0.473]</td>\n",
       "      <td>israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>where did britain create colonies for its empire</td>\n",
       "      <td>[britain, empire]</td>\n",
       "      <td>[Q161885, Q8680]</td>\n",
       "      <td>[[2, 3], [7, 8]]</td>\n",
       "      <td>[britain, colonies, its empire]</td>\n",
       "      <td>[[2, 3], [4, 5], [6, 8]]</td>\n",
       "      <td>[Q8680, NC, Q8680]</td>\n",
       "      <td>[0.402, 0.159, 0.468]</td>\n",
       "      <td>britain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx                                          sentence  \\\n",
       "4         18        1970 world cup semi final italy vs germany   \n",
       "12        38     when was the wizard of oz made in technicolor   \n",
       "11        36   when was the first freeway built in los angeles   \n",
       "5         23          where is israel located on the world map   \n",
       "3         16  where did britain create colonies for its empire   \n",
       "\n",
       "                        gold_aliases                   gold_qids  \\\n",
       "4   [1970 world cup, italy, germany]  [Q132664, Q676899, Q43310]   \n",
       "12       [wizard of oz, technicolor]          [Q193695, Q674564]   \n",
       "11                     [los angeles]                       [Q65]   \n",
       "5                [israel, world map]             [Q801, Q653848]   \n",
       "3                  [britain, empire]            [Q161885, Q8680]   \n",
       "\n",
       "                  gold_spans                            pred_aliases  \\\n",
       "4   [[0, 3], [5, 6], [7, 8]]  [1970 world cup, semi, italy, germany]   \n",
       "12          [[3, 6], [8, 9]]      [the wizard of oz, in technicolor]   \n",
       "11                  [[7, 9]]                  [freeway, los angeles]   \n",
       "5           [[2, 3], [6, 8]]                     [israel, world map]   \n",
       "3           [[2, 3], [7, 8]]         [britain, colonies, its empire]   \n",
       "\n",
       "                          pred_spans                            pred_qids  \\\n",
       "4   [[0, 3], [3, 4], [5, 6], [7, 8]]  [Q132664, Q992994, Q676899, Q37285]   \n",
       "12                  [[2, 6], [7, 9]]                 [Q193695, Q17412490]   \n",
       "11                  [[4, 5], [7, 9]]                    [Q46622, Q104994]   \n",
       "5                   [[2, 3], [6, 8]]                    [Q23792, Q653848]   \n",
       "3           [[2, 3], [4, 5], [6, 8]]                   [Q8680, NC, Q8680]   \n",
       "\n",
       "                      pred_probs        error  \n",
       "4   [0.995, 0.811, 0.916, 0.508]      germany  \n",
       "12                  [0.438, 1.0]  technicolor  \n",
       "11                [0.718, 0.716]  los angeles  \n",
       "5                 [0.383, 0.473]       israel  \n",
       "3          [0.402, 0.159, 0.468]      britain  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['wrong_entity']).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the errors Bootleg makes is predicting too general of a candidate (e.g. Oregon State Beavers instead of Oregon State Beavers baseball). Other errors are due to ambiguous sentences (e.g. \"cast of characters in fiddler on the roof\" -> should this be the movie or the musical?). Finally another bucket of errors suggests that we need to boost certain training signals -- this is an area we're actively pursuing in Bootleg with an investigation of model guidability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>who plays claire underwood 's mom on house of cards</td>\n",
       "      <td>[claire underwood, house of cards]</td>\n",
       "      <td>[Q14915624, Q3330940]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "      <td>[claire underwood, mom, house of cards]</td>\n",
       "      <td>[[2, 4], [5, 6], [7, 10]]</td>\n",
       "      <td>[Q14915624, Q7566, Q578361]</td>\n",
       "      <td>[1.0, 0.354, 0.893]</td>\n",
       "      <td>mom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>the pair of hand drums used in indian classical music is called</td>\n",
       "      <td>[indian classical music]</td>\n",
       "      <td>[Q1323698]</td>\n",
       "      <td>[[7, 10]]</td>\n",
       "      <td>[hand, drums, indian classical music]</td>\n",
       "      <td>[[3, 4], [4, 5], [7, 10]]</td>\n",
       "      <td>[Q1552740, Q221769, Q1323698]</td>\n",
       "      <td>[0.575, 0.317, 0.926]</td>\n",
       "      <td>drums</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>33</td>\n",
       "      <td>uk national debt as percentage of gdp by year</td>\n",
       "      <td>[uk national debt, gdp]</td>\n",
       "      <td>[Q611713, Q12638]</td>\n",
       "      <td>[[0, 3], [6, 7]]</td>\n",
       "      <td>[uk, national debt, gdp]</td>\n",
       "      <td>[[0, 1], [1, 3], [6, 7]]</td>\n",
       "      <td>[Q145, Q611713, Q12638]</td>\n",
       "      <td>[0.581, 0.612, 0.961]</td>\n",
       "      <td>uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>what was dennis hopper 's bike in easy rider</td>\n",
       "      <td>[dennis hopper, easy rider]</td>\n",
       "      <td>[Q102711, Q503638]</td>\n",
       "      <td>[[2, 4], [7, 9]]</td>\n",
       "      <td>[dennis hopper, bike, easy rider]</td>\n",
       "      <td>[[2, 4], [5, 6], [7, 9]]</td>\n",
       "      <td>[Q102711, Q11442, Q5331186]</td>\n",
       "      <td>[1.0, 0.574, 0.917]</td>\n",
       "      <td>bike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>48</td>\n",
       "      <td>what was the japanese motivation for bombing pearl harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[Q188712, Q127091]</td>\n",
       "      <td>[[3, 4], [7, 9]]</td>\n",
       "      <td>[japanese, motivation, bombing, pearl harbor]</td>\n",
       "      <td>[[3, 4], [4, 5], [6, 7], [7, 9]]</td>\n",
       "      <td>[Q188712, Q644302, Q52418, Q52418]</td>\n",
       "      <td>[0.345, 0.963, 0.862, 0.516]</td>\n",
       "      <td>motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx                                                         sentence  \\\n",
       "14        29              who plays claire underwood 's mom on house of cards   \n",
       "9         21  the pair of hand drums used in indian classical music is called   \n",
       "15        33                    uk national debt as percentage of gdp by year   \n",
       "4         12                     what was dennis hopper 's bike in easy rider   \n",
       "21        48        what was the japanese motivation for bombing pearl harbor   \n",
       "\n",
       "                          gold_aliases              gold_qids  \\\n",
       "14  [claire underwood, house of cards]  [Q14915624, Q3330940]   \n",
       "9             [indian classical music]             [Q1323698]   \n",
       "15             [uk national debt, gdp]      [Q611713, Q12638]   \n",
       "4          [dennis hopper, easy rider]     [Q102711, Q503638]   \n",
       "21            [japanese, pearl harbor]     [Q188712, Q127091]   \n",
       "\n",
       "           gold_spans                                   pred_aliases  \\\n",
       "14  [[2, 4], [7, 10]]        [claire underwood, mom, house of cards]   \n",
       "9           [[7, 10]]          [hand, drums, indian classical music]   \n",
       "15   [[0, 3], [6, 7]]                       [uk, national debt, gdp]   \n",
       "4    [[2, 4], [7, 9]]              [dennis hopper, bike, easy rider]   \n",
       "21   [[3, 4], [7, 9]]  [japanese, motivation, bombing, pearl harbor]   \n",
       "\n",
       "                          pred_spans                           pred_qids  \\\n",
       "14         [[2, 4], [5, 6], [7, 10]]         [Q14915624, Q7566, Q578361]   \n",
       "9          [[3, 4], [4, 5], [7, 10]]       [Q1552740, Q221769, Q1323698]   \n",
       "15          [[0, 1], [1, 3], [6, 7]]             [Q145, Q611713, Q12638]   \n",
       "4           [[2, 4], [5, 6], [7, 9]]         [Q102711, Q11442, Q5331186]   \n",
       "21  [[3, 4], [4, 5], [6, 7], [7, 9]]  [Q188712, Q644302, Q52418, Q52418]   \n",
       "\n",
       "                      pred_probs       error  \n",
       "14           [1.0, 0.354, 0.893]         mom  \n",
       "9          [0.575, 0.317, 0.926]       drums  \n",
       "15         [0.581, 0.612, 0.961]          uk  \n",
       "4            [1.0, 0.574, 0.917]        bike  \n",
       "21  [0.345, 0.963, 0.862, 0.516]  motivation  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['extra_mention']).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Bootleg may detect and label extraneous mentions that were not hand-labelled. Setting the threshold higher helps to reduce these predictions, as does using a 'NC' candidate for training, which Bootleg also supports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare to TAGME \n",
    "\n",
    "To get a sense of how Bootleg is doing compared to other systems, we evaluate [TAGME](https://arxiv.org/pdf/1006.3498.pdf), an existing tool to extract and disambiguate mentions. To run TAGME, you need to get a (free) authorization token. Instructions for obtaining a token are [here](https://sobigdata.d4science.org/web/tagme/tagme-help). You will need to verify your account and then follow the \"access the VRE\") link. We've also provided the file with TAGME labels for a given threshold for download if you want to skip the authorization token.\n",
    "\n",
    "We note that unlike TAGME, Bootleg also outputs contextual entity embeddings which can be loaded for use in downstream tasks (e.g. relation extraction, question answering). Check out the Entity Embedding tutorial for more details! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tagme\n",
    "# Set the authorization token for subsequent calls.\n",
    "tagme.GCUBE_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagme_label_file = f'{root_dir}/data/nq/test_natural_questions_50_tagme.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a token, skip the cell below and load the pre-generated TAGME labels. If you do have a token, you can play with changing the threshold below and see how it affects the results. Increasing the threshold increases the precision but decreases the recall as TAGME, as TAGME will label fewer mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a mapping from Wikipedia pageids to Wikidata QIDs to get the QIDs predicted by TAGME \n",
    "wpid2qid = ujson.load(open(f'{root_dir}/data/wiki_entity_data/entity_mappings/wpid2qid.json'))\n",
    "\n",
    "# As the threshold increases, the precision increases, but the recall decreases\n",
    "tagme_annotate(in_file=nq_sample_orig, out_file=tagme_label_file, threshold=0.3, wpid2qid=wpid2qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.63 (49/78)\n",
      "Precision: 0.58 (49/84)\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_precision_and_recall\n",
    "tagme_errors = compute_precision_and_recall(orig_label_file=nq_sample_orig, \n",
    "                                            new_label_file=tagme_label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that TAGME has slightly worse recall than Bootleg, when the precisions are set to be comparable (changing either TAGME or Bootleg's threshold will change the recall/precision values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Annotate On-the-Fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To annotate individual sentences with Bootleg, we  also support annotate-on-the-fly mode. \n",
    "\n",
    "**Note that Annotator is not optimized and is only intended to be used for quick experimentation and for demos. We recommend using the above pipeline (`extract_mentions` and `model_eval` functions) for evaluating datasets. These functions leverage multiprocessing, caching of preprocessed data, and batching to speed up evaluation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we create an annotator object. This loads the model and entity databases. We use the `config_args` loaded from the previous step. Note it takes several minutes for the initial load of the model and the entity data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "2020-10-21 19:44:43,128 Loading embeddings...\n",
      "2020-10-21 19:45:07,344 Finished loading embeddings.\n",
      "2020-10-21 19:45:40,203 Loading candidate mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8034754/8034754 [00:32<00:00, 251047.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-21 19:46:12,213 Loaded candidate mapping with 8034754 aliases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from bootleg.annotator import Annotator\n",
    "\n",
    "ann = Annotator(config_args=config_args, cand_map=cand_map, device='cuda' if not use_cpu else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to TAGME, we allow setting a threshold to only return mentions with labels greater than some probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.set_threshold(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in sentences to see what Bootleg predicts! Bootleg outputs the QIDs (or \"NC\" for \"No Candidate\"), the associated probabilities, and the title for each mention. The QIDs map to Wikidata -- to look them up you can use https://www.wikidata.org/wiki/Q1454 and replace the QID. \"NC\" means Bootleg did not find a good match among the candidates in the candidate list given the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q1517373', 'Q1454'],\n",
       " [1.0, 0.9984428286552429],\n",
       " ['Outer Banks', 'North Carolina'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"where is the outer banks in north carolina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q487330'], [0.8815685510635376], ['Fiddler on the Roof'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"cast of characters in fiddler on the roof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the entity disambiguation problem can be quite tricky -- in the above example we predict the song \"Fiddler on the Roof\" the music instead of the hand-label of the movie (https://www.wikidata.org/wiki/Q934036). Giving additional cues may help though -- for instance, if we add \"the movie\", the prediction changes to the movie! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q934036'], [0.770247220993042], ['Fiddler on the Roof (film)'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"cast of characters in the movie fiddler on the roof\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
