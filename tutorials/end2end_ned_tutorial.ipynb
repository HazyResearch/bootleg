{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NED Tutorial\n",
    "\n",
    "In this tutorial, we walk through how to use Bootleg as an end-to-end pipeline to detect and label entities in a set of sentences. First, we show how to use Bootleg to detect and disambiguate mentions to entities. We then compare to an existing system named TAGME. \n",
    "\n",
    "This tutorial assumes you want to use Bootleg on full datasets. You can also use Bootleg in annotator mode:\n",
    "\n",
    "```\n",
    "pip install bootleg\n",
    "from bootleg.end2end.bootleg_annotator import BootlegAnnotator\n",
    "ann = BootlegAnnotator()\n",
    "ann.label_mentions(\"Bob Dylan release Desire\")[\"titles\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how Bootleg performs on more natural language than we find in Wikipedia, we hand label the mentions and corresponding entities in 50 questions sampled from the [Natural Questions dataset (Google)](https://ai.google.com/research/NaturalQuestions). We will evaluate our *uncased* Bootleg model. However, we have manually cased the data in case you want to try our cased model instead.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- Pretrained Bootleg uncased model and config [here](https://bootleg-data.s3-us-west-2.amazonaws.com/models/lateset/bootleg_uncased.tar.gz). Cased model and config [here](https://bootleg-data.s3-us-west-2.amazonaws.com/models/lateset/bootleg_cased.tar.gz)\n",
    "- Sample of Natural Questions with hand-labelled entities [here](https://bootleg-data.s3-us-west-2.amazonaws.com/data/lateset/nq.tar.gz)\n",
    "- Entity data [here](https://bootleg-data.s3-us-west-2.amazonaws.com/data/lateset/entity_db.tar.gz)\n",
    "\n",
    "For convenience, you can run the commands below (from the root directory of the repo) to download all the above files and unpack them to `models` and `data` directories. It will take several minutes to download all the files. \n",
    "\n",
    "```\n",
    "    # use cased for cased model\n",
    "    bash tutorials/download_model.sh uncased\n",
    "    bash tutorials/download_data.sh\n",
    "```\n",
    "\n",
    "You can also run directly in this notebook by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!sh download_model.sh uncased\n",
    "!sh download_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "# Set to logging.DEBUG for more logging output\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# root_dir = FILL IN FULL PATH TO DIRECTORY WHERE DATA IS DOWNLOADED (e.g., root_dir/data and root_dir/models)\n",
    "root_dir = Path(\"tutorial_data\")\n",
    "cand_map = root_dir / 'data/entity_db/entity_mappings/alias2qids.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU with at least 12GB of memory available, set the below to 0 to run inference on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect Mentions\n",
    "Bootleg uses a simple mention extraction algorithm that extracts mentions using a given candidate map. We will use a Wikipedia candidate map that we mined using Wikipedia anchor links and Wikidata aliases for a total of ~15 million mentions (provided in the Requirements section of this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input dataset for the end-to-end pipeline, we assume a jsonlines file with a single dictionary with the key \"sentence\" and value as the text of the sentence, per line. For instance, you may have a file with the lines:\n",
    "\n",
    "    {\"sentence\": \"Who did the voice of the magician in Frosty the Snowman\"}\n",
    "    {\"sentence\": \"What is considered the Outer Banks in North Carolina\"}\n",
    "    \n",
    "Below, we have additional keys to keep track of the hand-labelled mentions, but this is purely for evaluating the quality of the end-to-end pipeline and is not needed in the common use cases of using Bootleg to detect and label mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_sample_orig = root_dir / 'data/nq/test_50.jsonl'\n",
    "nq_sample_bootleg = root_dir / 'data/nq/test_50_bootleg.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bootleg.end2end.extract_mentions import extract_mentions\n",
    "verbose = False\n",
    "extract_mentions(in_filepath=nq_sample_orig, out_filepath=nq_sample_bootleg, cand_map_file=cand_map, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at a sample of the extracted mentions, we can compare the mention extraction phase to the hand-labelled mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>aliases_hand</th>\n",
       "      <th>spans_hand</th>\n",
       "      <th>aliases_bootleg</th>\n",
       "      <th>spans_bootleg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who did the voice of the magician in Frosty the Snowman</td>\n",
       "      <td>[frosty the snowman]</td>\n",
       "      <td>[[8, 11]]</td>\n",
       "      <td>[voice of, magician, frosty the snowman]</td>\n",
       "      <td>[[3, 5], [6, 7], [8, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is considered the Outer Banks in North Carolina</td>\n",
       "      <td>[outer banks, north carolina]</td>\n",
       "      <td>[[4, 6], [7, 9]]</td>\n",
       "      <td>[outer banks, north carolina]</td>\n",
       "      <td>[[4, 6], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Nashville sound brought a polished and cosmopolitan sound to country music by</td>\n",
       "      <td>[nashville sound, country music]</td>\n",
       "      <td>[[1, 3], [10, 12]]</td>\n",
       "      <td>[nashville sound, music by]</td>\n",
       "      <td>[[1, 3], [11, 13]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What channel is the Premier League on in France</td>\n",
       "      <td>[premier league, france]</td>\n",
       "      <td>[[4, 6], [8, 9]]</td>\n",
       "      <td>[premier league, france]</td>\n",
       "      <td>[[4, 6], [8, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I Love It ( feat . Charli XCX ) Icona Pop</td>\n",
       "      <td>[i love it, charli xcx, icona pop]</td>\n",
       "      <td>[[0, 3], [6, 8], [9, 11]]</td>\n",
       "      <td>[charli xcx, icona pop]</td>\n",
       "      <td>[[6, 8], [9, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The U.S. Supreme Court hears appeals from circuit courts</td>\n",
       "      <td>[u.s. supreme court, circuit courts]</td>\n",
       "      <td>[[1, 4], [7, 9]]</td>\n",
       "      <td>[us supreme court, circuit courts]</td>\n",
       "      <td>[[1, 4], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Why does the author say that the vampire in Nosferatu is named Count Orlok and not Count Dracula</td>\n",
       "      <td>[nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[[9, 10], [12, 14], [16, 18]]</td>\n",
       "      <td>[vampire, nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[[7, 8], [9, 10], [12, 14], [16, 18]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Is there an active volcano in New Zealand</td>\n",
       "      <td>[new zealand]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[volcano, new zealand]</td>\n",
       "      <td>[[4, 5], [6, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Once Upon a Time Season 6 episode list</td>\n",
       "      <td>[once upon a time season 6]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "      <td>[upon a time, season 6, episode list]</td>\n",
       "      <td>[[1, 4], [4, 6], [6, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who is the former co-chairman Goldman Sachs who became a U.S. Secretary of the Treasury</td>\n",
       "      <td>[goldman sachs, us secretary of the treasury]</td>\n",
       "      <td>[[5, 7], [10, 15]]</td>\n",
       "      <td>[goldman sachs, us secretary of the treasury]</td>\n",
       "      <td>[[5, 7], [10, 15]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Who plays Norman Bates in the TV show</td>\n",
       "      <td>[norman bates]</td>\n",
       "      <td>[[2, 4]]</td>\n",
       "      <td>[norman bates]</td>\n",
       "      <td>[[2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hitchhiker 's Guide to the Galaxy Slartibartfast quotes</td>\n",
       "      <td>[hitchhiker 's guide to the galaxy, slartibartfast]</td>\n",
       "      <td>[[0, 6], [6, 7]]</td>\n",
       "      <td>[hitchhiker s guide to the galaxy]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What was Dennis Hopper 's bike in Easy Rider</td>\n",
       "      <td>[dennis hopper, easy rider]</td>\n",
       "      <td>[[2, 4], [7, 9]]</td>\n",
       "      <td>[dennis hopper, easy rider]</td>\n",
       "      <td>[[2, 4], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Where was 10 Things I Hate About You filmed school</td>\n",
       "      <td>[10 things i hate about you]</td>\n",
       "      <td>[[2, 8]]</td>\n",
       "      <td>[10 things i hate about you]</td>\n",
       "      <td>[[2, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Where does the last name Aponte come from</td>\n",
       "      <td>[aponte]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[aponte]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            sentence  \\\n",
       "0                                            Who did the voice of the magician in Frosty the Snowman   \n",
       "1                                               What is considered the Outer Banks in North Carolina   \n",
       "2                  The Nashville sound brought a polished and cosmopolitan sound to country music by   \n",
       "3                                                    What channel is the Premier League on in France   \n",
       "4                                                          I Love It ( feat . Charli XCX ) Icona Pop   \n",
       "5                                           The U.S. Supreme Court hears appeals from circuit courts   \n",
       "6   Why does the author say that the vampire in Nosferatu is named Count Orlok and not Count Dracula   \n",
       "7                                                          Is there an active volcano in New Zealand   \n",
       "8                                                             Once Upon a Time Season 6 episode list   \n",
       "9            Who is the former co-chairman Goldman Sachs who became a U.S. Secretary of the Treasury   \n",
       "10                                                             Who plays Norman Bates in the TV show   \n",
       "11                                           Hitchhiker 's Guide to the Galaxy Slartibartfast quotes   \n",
       "12                                                      What was Dennis Hopper 's bike in Easy Rider   \n",
       "13                                                Where was 10 Things I Hate About You filmed school   \n",
       "14                                                         Where does the last name Aponte come from   \n",
       "\n",
       "                                           aliases_hand  \\\n",
       "0                                  [frosty the snowman]   \n",
       "1                         [outer banks, north carolina]   \n",
       "2                      [nashville sound, country music]   \n",
       "3                              [premier league, france]   \n",
       "4                    [i love it, charli xcx, icona pop]   \n",
       "5                  [u.s. supreme court, circuit courts]   \n",
       "6               [nosferatu, count orlok, count dracula]   \n",
       "7                                         [new zealand]   \n",
       "8                           [once upon a time season 6]   \n",
       "9         [goldman sachs, us secretary of the treasury]   \n",
       "10                                       [norman bates]   \n",
       "11  [hitchhiker 's guide to the galaxy, slartibartfast]   \n",
       "12                          [dennis hopper, easy rider]   \n",
       "13                         [10 things i hate about you]   \n",
       "14                                             [aponte]   \n",
       "\n",
       "                       spans_hand  \\\n",
       "0                       [[8, 11]]   \n",
       "1                [[4, 6], [7, 9]]   \n",
       "2              [[1, 3], [10, 12]]   \n",
       "3                [[4, 6], [8, 9]]   \n",
       "4       [[0, 3], [6, 8], [9, 11]]   \n",
       "5                [[1, 4], [7, 9]]   \n",
       "6   [[9, 10], [12, 14], [16, 18]]   \n",
       "7                        [[6, 8]]   \n",
       "8                        [[0, 6]]   \n",
       "9              [[5, 7], [10, 15]]   \n",
       "10                       [[2, 4]]   \n",
       "11               [[0, 6], [6, 7]]   \n",
       "12               [[2, 4], [7, 9]]   \n",
       "13                       [[2, 8]]   \n",
       "14                       [[5, 6]]   \n",
       "\n",
       "                                     aliases_bootleg  \\\n",
       "0           [voice of, magician, frosty the snowman]   \n",
       "1                      [outer banks, north carolina]   \n",
       "2                        [nashville sound, music by]   \n",
       "3                           [premier league, france]   \n",
       "4                            [charli xcx, icona pop]   \n",
       "5                 [us supreme court, circuit courts]   \n",
       "6   [vampire, nosferatu, count orlok, count dracula]   \n",
       "7                             [volcano, new zealand]   \n",
       "8              [upon a time, season 6, episode list]   \n",
       "9      [goldman sachs, us secretary of the treasury]   \n",
       "10                                    [norman bates]   \n",
       "11                [hitchhiker s guide to the galaxy]   \n",
       "12                       [dennis hopper, easy rider]   \n",
       "13                      [10 things i hate about you]   \n",
       "14                                          [aponte]   \n",
       "\n",
       "                            spans_bootleg  \n",
       "0               [[3, 5], [6, 7], [8, 11]]  \n",
       "1                        [[4, 6], [7, 9]]  \n",
       "2                      [[1, 3], [11, 13]]  \n",
       "3                        [[4, 6], [8, 9]]  \n",
       "4                       [[6, 8], [9, 11]]  \n",
       "5                        [[1, 4], [7, 9]]  \n",
       "6   [[7, 8], [9, 10], [12, 14], [16, 18]]  \n",
       "7                        [[4, 5], [6, 8]]  \n",
       "8                [[1, 4], [4, 6], [6, 8]]  \n",
       "9                      [[5, 7], [10, 15]]  \n",
       "10                               [[2, 4]]  \n",
       "11                               [[0, 6]]  \n",
       "12                       [[2, 4], [7, 9]]  \n",
       "13                               [[2, 8]]  \n",
       "14                               [[5, 6]]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import load_mentions\n",
    "\n",
    "orig_mentions_df = load_mentions(nq_sample_orig)\n",
    "bootleg_mentions_df = load_mentions(nq_sample_bootleg)\n",
    "\n",
    "# join dataframes and sample\n",
    "res = pd.merge(orig_mentions_df, bootleg_mentions_df, on=['sentence'], suffixes=['_hand', '_bootleg'])\n",
    "display(res.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample above, we see that generally Bootleg detects the same mentions as the hand-labelled mentions, however sometimes Bootleg extracts extra mentions (e.g \"colonies\" in \"Where did Britain create colonies for its empire\"). This is expected as we would rather the mention detection step filter out too few mentions than too many. It will be the job of the backbone model and postprocessing to filter out these extra mentions, by either thresholding the prediction probability or predicting a candidate that represents \"No Candidate\" (we refer to this as \"NC\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disambiguate Mentions to Entities\n",
    "\n",
    "We run inference using a pretrained Bootleg model to disambiguate the extracted mentions to Wikidata QIDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model config so we can set additional parameters and load the saved model during evaluation. We need to update the config parameters to point to the downloaded model checkpoint and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.utils.parser.parser_utils import parse_boot_and_emm_args\n",
    "from bootleg.utils.utils import load_yaml_file\n",
    "from bootleg.run import run_model\n",
    "\n",
    "config_in_path = root_dir / 'models/bootleg_uncased/bootleg_config.yaml'\n",
    "\n",
    "config_args = load_yaml_file(config_in_path)\n",
    "\n",
    "# decrease number of data threads as this is a small file\n",
    "config_args[\"run_config\"][\"dataset_threads\"] = 2\n",
    "config_args[\"run_config\"][\"log_level\"] = \"info\"\n",
    "# set the model checkpoint path \n",
    "config_args[\"emmental\"][\"model_path\"] = str(root_dir / 'models/bootleg_uncased/bootleg_wiki.pth')\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args[\"data_config\"][\"entity_dir\"] = str(root_dir / 'data/entity_db')\n",
    "config_args[\"data_config\"][\"alias_cand_map\"] = \"alias2qids.json\"\n",
    "\n",
    "# set the data path and kore50 test file \n",
    "config_args[\"data_config\"][\"data_dir\"] = str(root_dir / 'data/nq')\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args[\"data_config\"][\"test_dataset\"][\"file\"] = nq_sample_bootleg.name\n",
    "\n",
    "# set the embedding paths \n",
    "config_args[\"data_config\"][\"emb_dir\"] =  str(root_dir / 'data/entity_db')\n",
    "config_args[\"data_config\"][\"word_embedding\"][\"cache_dir\"] =  str(root_dir / 'data/pretrained_bert_models')\n",
    "\n",
    "# set the devie if on CPU\n",
    "config_args[\"emmental\"][\"device\"] = device\n",
    "\n",
    "# save the new args (helps if you want to run things via command line)\n",
    "config_args = parse_boot_and_emm_args(config_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:34:16,174 Setting logging directory to: bootleg_logs/wiki_full_ft/2021_03_17/15_34_15/2d3cede7\n",
      "2021-03-17 15:34:16,359 Loading Emmental default config from /dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/emmental/emmental-default-config.yaml.\n",
      "2021-03-17 15:34:16,361 Updating Emmental config from user provided config.\n",
      "2021-03-17 15:34:16,362 Set random seed to 1234.\n",
      "2021-03-17 15:34:17,406 COMMAND: /dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/ipykernel_launcher.py -f /dfs/scratch0/lorr1/projects/:/afs/cs.stanford.edu/u/lorr1/.local/apt-cache/share/jupyter/runtime/kernel-3edac498-7b5f-4175-b525-87fb3287db3f.json\n",
      "2021-03-17 15:34:17,407 Saving config to bootleg_logs/wiki_full_ft/2021_03_17/15_34_15/2d3cede7/parsed_config.yaml\n",
      "2021-03-17 15:34:17,738 Git Hash: Not able to retrieve git hash\n",
      "2021-03-17 15:34:17,740 Loading entity symbols...\n",
      "2021-03-17 15:37:51,544 Starting to build data for test from ../tutorial_data/data/nq/test_50_bootleg.jsonl\n",
      "2021-03-17 15:37:51,732 Building dataset from scratch. Saving to ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1.\n",
      "2021-03-17 15:37:51,734 Starting to extract examples using 2 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/datasets/dataset.py:1020: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  guid_dtype = np.dtype(\n",
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/layers/alias_to_ent_encoder.py:98: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  alias2entity_table = torch.from_numpy(alias2entity_table)\n",
      "Reading in ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_input/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 399.17it/s]\n",
      "Reading in ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_input/out_1.jsonl: 100%|██████████| 25/25 [00:00<00:00, 461.42it/s]\n",
      "/dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/numpy/core/memmap.py:230: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  descr = dtypedescr(dtype)\n",
      "Processing ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_output/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 1096.91it/s]\n",
      "Processing ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_output/out_1.jsonl: 100%|██████████| 24/24 [00:00<00:00, 1307.72it/s]\n",
      "Checking sentence uniqueness: 100%|██████████| 49/49 [00:00<00:00, 18163.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:40:17,446 Loading data from ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_data.bin and ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_label.bin\n",
      "2021-03-17 15:40:17,449 Building type labels from scatch.\n",
      "2021-03-17 15:43:03,786 Creating type prediction labeled data using 2 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing types: 100%|██████████| 25/25 [00:02<00:00, 11.74it/s]\n",
      "Processing types: 100%|██████████| 24/24 [00:02<00:00, 11.24it/s]\n",
      "Building type data: 100%|██████████| 2/2 [00:02<00:00,  1.09s/it]\n",
      "Verifying type labels: 100%|██████████| 49/49 [00:00<00:00, 21994.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:43:15,310 Final data initialization time for test is 323.7640495300293s\n",
      "2021-03-17 15:43:15,789 Built dataloader for test set with 49 and 1 threads samples (Shuffle=False, Batch size=32).\n",
      "2021-03-17 15:43:15,837 Building slice dataset for test from ../tutorial_data/data/nq/test_50_bootleg.jsonl.\n",
      "2021-03-17 15:43:15,882 Building dataset from scratch. Saving to ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1\n",
      "2021-03-17 15:43:15,884 Strating to extract examples with 2 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading in ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_input/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 13732.01it/s]\n",
      "Reading in ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_input/out_1.jsonl: 100%|██████████| 25/25 [00:00<00:00, 10298.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:43:16,707 Starting to build and save features with 2 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_output/out_1.jsonl: 100%|██████████| 25/25 [00:00<00:00, 3775.66it/s]\n",
      "Processing ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_output/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 434.06it/s]\n",
      "Checking sentence uniqueness: 100%|██████████| 50/50 [00:00<00:00, 11840.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:43:17,619 Loading data from ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_slices_1f126b5224.bin and ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_slices_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sent idx to row idx mapping: 100%|██████████| 50/50 [00:00<00:00, 11491.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:43:17,766 Final slice data initialization time from test is 1.9294226169586182s\n",
      "2021-03-17 15:43:17,767 Updating Emmental config from user provided config.\n",
      "2021-03-17 15:43:17,768 Set random seed to 1234.\n",
      "2021-03-17 15:43:17,778 Starting Bootleg Model\n",
      "2021-03-17 15:43:17,779 Created emmental model Bootleg that contains task set().\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:43:23,390 Loading embeddings...\n",
      "2021-03-17 15:46:59,266 Created task: NED\n",
      "2021-03-17 15:46:59,268 Moving bert module to CPU.\n",
      "2021-03-17 15:46:59,279 Moving embedding_payload module to CPU.\n",
      "2021-03-17 15:46:59,281 Moving attn_network module to CPU.\n",
      "2021-03-17 15:46:59,286 Moving pred_layer module to CPU.\n",
      "2021-03-17 15:46:59,287 Moving learned module to CPU.\n",
      "2021-03-17 15:46:59,287 Moving title_static module to CPU.\n",
      "2021-03-17 15:46:59,288 Moving learned_type module to CPU.\n",
      "2021-03-17 15:46:59,289 Moving learned_type_wiki module to CPU.\n",
      "2021-03-17 15:46:59,290 Moving learned_type_relations module to CPU.\n",
      "2021-03-17 15:46:59,291 Moving adj_index module to CPU.\n",
      "2021-03-17 15:47:03,644 Created task: Type\n",
      "2021-03-17 15:47:03,647 Moving bert module to CPU.\n",
      "2021-03-17 15:47:03,651 Moving embedding_payload module to CPU.\n",
      "2021-03-17 15:47:03,652 Moving attn_network module to CPU.\n",
      "2021-03-17 15:47:03,655 Moving pred_layer module to CPU.\n",
      "2021-03-17 15:47:03,655 Moving learned module to CPU.\n",
      "2021-03-17 15:47:03,656 Moving title_static module to CPU.\n",
      "2021-03-17 15:47:03,657 Moving learned_type module to CPU.\n",
      "2021-03-17 15:47:03,658 Moving learned_type_wiki module to CPU.\n",
      "2021-03-17 15:47:03,659 Moving learned_type_relations module to CPU.\n",
      "2021-03-17 15:47:03,659 Moving adj_index module to CPU.\n",
      "2021-03-17 15:47:03,660 Moving type_prediction module to CPU.\n",
      "2021-03-17 15:47:48,083 [Bootleg] Model loaded from ../tutorial_data/models/bootleg_uncased/bootleg_wiki.pth\n",
      "2021-03-17 15:47:48,084 Moving bert module to CPU.\n",
      "2021-03-17 15:47:48,091 Moving embedding_payload module to CPU.\n",
      "2021-03-17 15:47:48,093 Moving attn_network module to CPU.\n",
      "2021-03-17 15:47:48,097 Moving pred_layer module to CPU.\n",
      "2021-03-17 15:47:48,099 Moving learned module to CPU.\n",
      "2021-03-17 15:47:48,100 Moving title_static module to CPU.\n",
      "2021-03-17 15:47:48,101 Moving learned_type module to CPU.\n",
      "2021-03-17 15:47:48,102 Moving learned_type_wiki module to CPU.\n",
      "2021-03-17 15:47:48,104 Moving learned_type_relations module to CPU.\n",
      "2021-03-17 15:47:48,105 Moving adj_index module to CPU.\n",
      "2021-03-17 15:47:48,106 Moving type_prediction module to CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Bootleg (test): 100%|██████████| 2/2 [00:08<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-17 15:48:01,694 Finished dumping. Merging results across accumulation steps.\n",
      "2021-03-17 15:48:01,735 Bootleg labels saved at bootleg_logs/wiki_full_ft/2021_03_17/15_34_15/2d3cede7/test_50_bootleg/bootleg_wiki/bootleg_labels.jsonl\n"
     ]
    }
   ],
   "source": [
    "bootleg_label_file, _ = run_model(mode=\"dump_preds\", config=config_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Note that Bootleg automatically handles prepping of new data files for running. These are all saved in `data_config.data_dir`/`data_config.data_prep_dir`. If you change the contents of the underlying `jsonl` file _without_ removing the saved prep file or setting `data_config.overwrite_preprocessed_data` to be `True`, Bootleg will reuse the old prepped file.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the overall quality of the end-to-end pipeline via precision / recall metrics, where the *recall* indicates what proportion of the hand-labelled mentions Bootleg correctly detects and disambiguates, and *precision* indicates what proportion of the mentions that Bootleg labels are correct. For instance, if Bootleg only labelled the few mentions it was very confident in, then it would have a low recall and high precision.\n",
    "\n",
    "To detect if mentions match the hand-labelled mention spans, we report weak and exact match metrics. Weak means the predicted and gold span boundaries just need to overlap for an entity (e.g., predicted mention 'the wizard of oz' is counted as correct for the gold mention 'wizard of oz' if the correct entity is predicted). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAK MATCHING\n",
      "precision = 61 / 80 = 0.7625\n",
      "recall = 61 / 78 = 0.782051282051282\n",
      "f1 = 0.7721518987341772\n",
      "\n",
      "EXACT MATCHING\n",
      "precision = 61 / 80 = 0.7625\n",
      "recall = 61 / 78 = 0.782051282051282\n",
      "f1 = 0.7721518987341772\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_metrics\n",
    "bootleg_end2end_errors = compute_metrics(gold_file=nq_sample_orig,       \n",
    "                                 pred_file=bootleg_label_file, \n",
    "                                 threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine errors in the end-to-end pipeline below. As you increase the threshold in the `compute_metrics` command, entities with a prediction probability less than the threshold will be filtered out. If too few entities are predicted, lowering the threshold may help.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>I Love It ( feat . Charli XCX ) Icona Pop</td>\n",
       "      <td>[i love it, charli xcx, icona pop]</td>\n",
       "      <td>[Q3273659, Q5084390, Q808703]</td>\n",
       "      <td>[[0, 3], [6, 8], [9, 11]]</td>\n",
       "      <td>[charli xcx, icona pop]</td>\n",
       "      <td>[Q5084390, Q808703]</td>\n",
       "      <td>[[5, 9], [9, 11]]</td>\n",
       "      <td>[1.0, 0.9873091578]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>Who proposed the coordinate system to describe the position of a point in a plane accurately</td>\n",
       "      <td>[coordinate system]</td>\n",
       "      <td>[Q62912]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "      <td>[coordinate system]</td>\n",
       "      <td>[Q11210]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Landmark Supreme Court cases dealing with the First Amendment</td>\n",
       "      <td>[supreme court, first amendment]</td>\n",
       "      <td>[Q11201, Q12616]</td>\n",
       "      <td>[[1, 3], [7, 9]]</td>\n",
       "      <td>[supreme court cases, first amendment]</td>\n",
       "      <td>[Q6646863, Q12616]</td>\n",
       "      <td>[[1, 4], [7, 9]]</td>\n",
       "      <td>[1.0, 0.9999812841]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>Once Upon a Time Season 6 episode list</td>\n",
       "      <td>[once upon a time season 6]</td>\n",
       "      <td>[Q23301616]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "      <td>[season 6, episode list]</td>\n",
       "      <td>[Q2404330]</td>\n",
       "      <td>[[4, 6]]</td>\n",
       "      <td>[0.5048642159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13</td>\n",
       "      <td>Where was 10 Things I Hate About You filmed school</td>\n",
       "      <td>[10 things i hate about you]</td>\n",
       "      <td>[Q169082]</td>\n",
       "      <td>[[2, 8]]</td>\n",
       "      <td>[10 things i hate about you]</td>\n",
       "      <td>[Q169074]</td>\n",
       "      <td>[[2, 8]]</td>\n",
       "      <td>[0.5601187348]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>Who does Oregon state play in the College World Series</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[Q7101349, Q787505]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[Q787505]</td>\n",
       "      <td>[[7, 10]]</td>\n",
       "      <td>[0.8851752281]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>Why does the author say that the vampire in Nosferatu is named Count Orlok and not Count Dracula</td>\n",
       "      <td>[nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[Q151895, Q1442062, Q3266236]</td>\n",
       "      <td>[[9, 10], [12, 14], [16, 18]]</td>\n",
       "      <td>[vampire, nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[Q7912955, Q151895, Q1442062, Q3266236]</td>\n",
       "      <td>[[7, 8], [9, 10], [12, 14], [16, 18]]</td>\n",
       "      <td>[0.7404002547, 0.9794661999, 1.0, 0.9972344041]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>Where does the last name Vigil come from</td>\n",
       "      <td>[vigil]</td>\n",
       "      <td>[Q16878937]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[vigil]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>Reasons why South Africa should include renewable energy in its energy mix</td>\n",
       "      <td>[south africa, renewable energy]</td>\n",
       "      <td>[Q258, Q12705]</td>\n",
       "      <td>[[2, 4], [6, 8]]</td>\n",
       "      <td>[reasons why, south africa, renewable energy, energy mix]</td>\n",
       "      <td>[Q7028249, Q258, Q12705, Q1341346]</td>\n",
       "      <td>[[0, 2], [2, 4], [6, 8], [10, 12]]</td>\n",
       "      <td>[1.0, 0.999255836, 0.8151838183, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>48</td>\n",
       "      <td>What was the Japanese motivation for bombing Pearl Harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[Q188712, Q127091]</td>\n",
       "      <td>[[3, 4], [7, 9]]</td>\n",
       "      <td>[motivation, pearl harbor]</td>\n",
       "      <td>[Q644302, Q127091]</td>\n",
       "      <td>[[4, 5], [7, 9]]</td>\n",
       "      <td>[0.9905021191, 0.757401526]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx  \\\n",
       "14         4   \n",
       "7         42   \n",
       "5         37   \n",
       "18         8   \n",
       "22        13   \n",
       "1         31   \n",
       "16         6   \n",
       "6         40   \n",
       "3         35   \n",
       "10        48   \n",
       "\n",
       "                                                                                                text  \\\n",
       "14                                                         I Love It ( feat . Charli XCX ) Icona Pop   \n",
       "7       Who proposed the coordinate system to describe the position of a point in a plane accurately   \n",
       "5                                      Landmark Supreme Court cases dealing with the First Amendment   \n",
       "18                                                            Once Upon a Time Season 6 episode list   \n",
       "22                                                Where was 10 Things I Hate About You filmed school   \n",
       "1                                             Who does Oregon state play in the College World Series   \n",
       "16  Why does the author say that the vampire in Nosferatu is named Count Orlok and not Count Dracula   \n",
       "6                                                           Where does the last name Vigil come from   \n",
       "3                         Reasons why South Africa should include renewable energy in its energy mix   \n",
       "10                                         What was the Japanese motivation for bombing Pearl Harbor   \n",
       "\n",
       "                               gold_aliases                      gold_qids  \\\n",
       "14       [i love it, charli xcx, icona pop]  [Q3273659, Q5084390, Q808703]   \n",
       "7                       [coordinate system]                       [Q62912]   \n",
       "5          [supreme court, first amendment]               [Q11201, Q12616]   \n",
       "18              [once upon a time season 6]                    [Q23301616]   \n",
       "22             [10 things i hate about you]                      [Q169082]   \n",
       "1      [oregon state, college world series]            [Q7101349, Q787505]   \n",
       "16  [nosferatu, count orlok, count dracula]  [Q151895, Q1442062, Q3266236]   \n",
       "6                                   [vigil]                    [Q16878937]   \n",
       "3          [south africa, renewable energy]                 [Q258, Q12705]   \n",
       "10                 [japanese, pearl harbor]             [Q188712, Q127091]   \n",
       "\n",
       "                       gold_spans  \\\n",
       "14      [[0, 3], [6, 8], [9, 11]]   \n",
       "7                        [[3, 5]]   \n",
       "5                [[1, 3], [7, 9]]   \n",
       "18                       [[0, 6]]   \n",
       "22                       [[2, 8]]   \n",
       "1               [[2, 4], [7, 10]]   \n",
       "16  [[9, 10], [12, 14], [16, 18]]   \n",
       "6                        [[5, 6]]   \n",
       "3                [[2, 4], [6, 8]]   \n",
       "10               [[3, 4], [7, 9]]   \n",
       "\n",
       "                                                 pred_aliases  \\\n",
       "14                                    [charli xcx, icona pop]   \n",
       "7                                         [coordinate system]   \n",
       "5                      [supreme court cases, first amendment]   \n",
       "18                                   [season 6, episode list]   \n",
       "22                               [10 things i hate about you]   \n",
       "1                        [oregon state, college world series]   \n",
       "16           [vampire, nosferatu, count orlok, count dracula]   \n",
       "6                                                     [vigil]   \n",
       "3   [reasons why, south africa, renewable energy, energy mix]   \n",
       "10                                 [motivation, pearl harbor]   \n",
       "\n",
       "                                  pred_qids  \\\n",
       "14                      [Q5084390, Q808703]   \n",
       "7                                  [Q11210]   \n",
       "5                        [Q6646863, Q12616]   \n",
       "18                               [Q2404330]   \n",
       "22                                [Q169074]   \n",
       "1                                 [Q787505]   \n",
       "16  [Q7912955, Q151895, Q1442062, Q3266236]   \n",
       "6                                        []   \n",
       "3        [Q7028249, Q258, Q12705, Q1341346]   \n",
       "10                       [Q644302, Q127091]   \n",
       "\n",
       "                               pred_spans  \\\n",
       "14                      [[5, 9], [9, 11]]   \n",
       "7                                [[3, 5]]   \n",
       "5                        [[1, 4], [7, 9]]   \n",
       "18                               [[4, 6]]   \n",
       "22                               [[2, 8]]   \n",
       "1                               [[7, 10]]   \n",
       "16  [[7, 8], [9, 10], [12, 14], [16, 18]]   \n",
       "6                                      []   \n",
       "3      [[0, 2], [2, 4], [6, 8], [10, 12]]   \n",
       "10                       [[4, 5], [7, 9]]   \n",
       "\n",
       "                                         pred_probs  \n",
       "14                              [1.0, 0.9873091578]  \n",
       "7                                             [1.0]  \n",
       "5                               [1.0, 0.9999812841]  \n",
       "18                                   [0.5048642159]  \n",
       "22                                   [0.5601187348]  \n",
       "1                                    [0.8851752281]  \n",
       "16  [0.7404002547, 0.9794661999, 1.0, 0.9972344041]  \n",
       "6                                                []  \n",
       "3             [1.0, 0.999255836, 0.8151838183, 1.0]  \n",
       "10                      [0.9905021191, 0.757401526]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_end2end_errors).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the errors Bootleg makes is predicting too general of a candidate (e.g. Oregon State Beavers instead of Oregon State Beavers baseball). Other errors are due to ambiguous sentences (e.g. \"cast of characters in fiddler on the roof\" -> should this be the movie or the musical?). Finally another bucket of errors suggests that we need to boost certain training signals -- this is an area we're actively pursuing in Bootleg with an investigation of model guidability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare to TAGME \n",
    "\n",
    "To get a sense of how Bootleg is doing compared to other systems, we evaluate [TAGME](https://arxiv.org/pdf/1006.3498.pdf), an existing tool to extract and disambiguate mentions. To run TAGME, you need to get a (free) authorization token. Instructions for obtaining a token are [here](https://sobigdata.d4science.org/web/tagme/tagme-help). You will need to verify your account and then follow the \"access the VRE\") link. We've also provided the file with TAGME labels for a given threshold for download if you want to skip the authorization token.\n",
    "\n",
    "We note that unlike TAGME, Bootleg also outputs contextual entity embeddings which can be loaded for use in downstream tasks (e.g. relation extraction, question answering). Check out the Entity Embedding tutorial for more details! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tagme\n",
    "# Set the authorization token for subsequent calls.\n",
    "tagme.GCUBE_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagme_label_file = root_dir / '/data/nq/test_50_tagme.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a token, skip the cell below and load the pre-generated TAGME labels. If you do have a token, you can play with changing the threshold below and see how it affects the results. Increasing the threshold increases the precision but decreases the recall as TAGME, as TAGME will label fewer mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No wikidata id found for Frosty the Snowman (film)\n",
      "No wikidata id found for The Bachelor (U.S. TV series)\n",
      "No wikidata id found for House of Cards (U.S. TV series)\n"
     ]
    }
   ],
   "source": [
    "from utils import tagme_annotate\n",
    "# As the threshold increases, the precision increases, but the recall decreases\n",
    "tagme_annotate(in_file=nq_sample_orig, out_file=tagme_label_file, threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not set the threshold here when computing metrics for TAGME as TAGME predictions are already thresholded in the `tagme_annotate` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAK MATCHING\n",
      "precision = 53 / 101 = 0.5247524752475248\n",
      "recall = 53 / 78 = 0.6794871794871795\n",
      "f1 = 0.5921787709497207\n",
      "\n",
      "EXACT MATCHING\n",
      "precision = 52 / 101 = 0.5148514851485149\n",
      "recall = 52 / 78 = 0.6666666666666666\n",
      "f1 = 0.5810055865921788\n"
     ]
    }
   ],
   "source": [
    "tagme_errors = compute_metrics(gold_file=nq_sample_orig, pred_file=tagme_label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several Wikidata ids are not recovered due to out of date Wikipedia titles in TAGME predictions. Even when including these three samples as correct predictions, we see that Bootleg is able to outperform TAGME in both precision and recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
