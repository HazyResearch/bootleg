{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NED Tutorial\n",
    "\n",
    "In this tutorial, we walk through how to use Bootleg as an end-to-end pipeline to detect and label entities in a set of sentences. First, we show how to use Bootleg to detect and disambiguate mentions to entities. We then compare to an existing system named TAGME. Finally, we show how to use Bootleg to annotate individual sentences on the fly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how Bootleg performs on more natural language than we find in Wikipedia, we hand label the mentions and corresponding entities in 50 questions sampled from the [Natural Questions dataset (Google)](https://ai.google.com/research/NaturalQuestions). \n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- The sample of Natural Questions with hand-labelled entities [here](https://bootleg-emb.s3.amazonaws.com/data/nq.tar.gz)\n",
    "- Entity profile information [here](https://bootleg-emb.s3.amazonaws.com/entity_db.tar.gz)*\n",
    "- Pretrained Bootleg model and config [here](https://bootleg-emb.s3.amazonaws.com/models/2020_08_25/bootleg_wiki.tar.gz)*\n",
    "- Embedding data [here](https://bootleg-emb.s3.amazonaws.com/emb_data.tar.gz)*\n",
    "\n",
    "*Same file as in benchmark tutorial and does not need to be re-downloaded.\n",
    "\n",
    "For convenience, you can run the command below (from the `tutorials` directory) to download all the above files and unpack them to the provided directory. It will take several minutes to download all the files. \n",
    "\n",
    "    bash download_all.sh <NAME_OF_DIRECTORY_TO_SAVE_DATA>\n",
    "    \n",
    "You will need to assign the variable `input_dir` in this notebook to the path where you download the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import ujson\n",
    "from utils import load_mentions, tagme_annotate\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "input_dir = # FILL IN FULL PATH TO DATA DIRECTORY WHERE FILES ARE DOWNLOADED HERE \n",
    "\n",
    "cand_map = f'{input_dir}/entity_db/entity_mappings/alias2qids_wiki.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a GPU with at least 12GB of memory available, set the below to `True` to run inference on the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect Mentions\n",
    "Bootleg uses a simple mention extraction algorithm that extracts mentions using a given candidate map. We will use a Wikipedia candidate map that we mined using Wikipedia anchor links and Wikidata aliases for a total of ~8 million mentions (provided in the Requirements section of this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input dataset for the end-to-end pipeline, we assume a jsonlines file with a single dictionary with the key \"sentence\" and value as the text of the sentence, per line. For instance, you may have a file with the lines:\n",
    "\n",
    "    {\"sentence\": \"who did the voice of the magician in frosty the snowman\"}\n",
    "    {\"sentence\": \"what is considered the outer banks in north carolina\"}\n",
    "    \n",
    "Below, we have additional keys to keep track of the hand-labelled mentions, but this is purely for evaluating the quality of the end-to-end pipeline and is not needed in the common use cases of using Bootleg to detect and label mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_sample_orig = f'{input_dir}/nq/test_natural_questions_50.jsonl'\n",
    "nq_sample_bootleg = f'{input_dir}/nq/test_natural_questions_50_bootleg.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-09 10:22:27,651 Loading candidate mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7970529/7970529 [00:17<00:00, 455341.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-09 10:22:45,161 Loaded candidate mapping with 7970529 aliases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-09 10:22:58,305 Using 8 workers...\n",
      "2020-09-09 10:22:58,307 Reading in /dfs/scratch1/mleszczy/bootleg-test-09092020/tutorials/data/nq/test_natural_questions_50.jsonl\n",
      "2020-09-09 10:22:58,619 Wrote out data chunks in 0.31s\n",
      "2020-09-09 10:22:58,620 Calling subprocess...\n",
      "2020-09-09 10:22:59,346 Merging files...\n",
      "2020-09-09 10:22:59,384 Removing temporary files...\n",
      "2020-09-09 10:22:59,691 Finished in 1.3895530700683594 seconds. Wrote out to /dfs/scratch1/mleszczy/bootleg-test-09092020/tutorials/data/nq/test_natural_questions_50_bootleg.jsonl\n"
     ]
    }
   ],
   "source": [
    "from bootleg.extract_mentions import extract_mentions\n",
    "extract_mentions(in_filepath=nq_sample_orig, out_filepath=nq_sample_bootleg, cand_map_file=cand_map, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at a sample of the extracted mentions, we can compare the mention extraction phase to the hand-labelled mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>aliases_hand</th>\n",
       "      <th>spans_hand</th>\n",
       "      <th>aliases_bootleg</th>\n",
       "      <th>spans_bootleg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hitchhiker 's guide to the galaxy slartibartfast quotes</td>\n",
       "      <td>[hitchhiker 's guide to the galaxy, slartibartfast]</td>\n",
       "      <td>[0:6, 6:7]</td>\n",
       "      <td>[hitchhiker s guide to the galaxy, slartibartfast]</td>\n",
       "      <td>[0:6, 6:7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>when did rangers last win the scottish cup</td>\n",
       "      <td>[rangers, scottish cup]</td>\n",
       "      <td>[2:3, 6:8]</td>\n",
       "      <td>[rangers, scottish cup]</td>\n",
       "      <td>[2:3, 6:8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the pair of hand drums used in indian classical music is called</td>\n",
       "      <td>[indian classical music]</td>\n",
       "      <td>[7:10]</td>\n",
       "      <td>[drums, indian classical music]</td>\n",
       "      <td>[4:5, 7:10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>who controls the past controls the future rage against the machine</td>\n",
       "      <td>[rage against the machine]</td>\n",
       "      <td>[7:11]</td>\n",
       "      <td>[rage against the machine]</td>\n",
       "      <td>[7:11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>who was president of the united states in 1938</td>\n",
       "      <td>[president of the united states]</td>\n",
       "      <td>[2:7]</td>\n",
       "      <td>[president of the united states]</td>\n",
       "      <td>[2:7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i love it ( feat . charli xcx ) icona pop</td>\n",
       "      <td>[i love it, charli xcx, icona pop]</td>\n",
       "      <td>[0:3, 6:8, 9:11]</td>\n",
       "      <td>[i love it, charli xcx, icona pop]</td>\n",
       "      <td>[0:3, 6:8, 9:11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>which of these was not an export of ancient greece</td>\n",
       "      <td>[ancient greece]</td>\n",
       "      <td>[8:10]</td>\n",
       "      <td>[ancient greece]</td>\n",
       "      <td>[8:10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>what is the worth of the catholic church</td>\n",
       "      <td>[catholic church]</td>\n",
       "      <td>[6:8]</td>\n",
       "      <td>[worth, catholic church]</td>\n",
       "      <td>[3:4, 6:8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1970 world cup semi final italy vs germany</td>\n",
       "      <td>[1970 world cup, italy, germany]</td>\n",
       "      <td>[0:3, 5:6, 7:8]</td>\n",
       "      <td>[1970 world cup, semi, italy, germany]</td>\n",
       "      <td>[0:3, 3:4, 5:6, 7:8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>game of thrones season 1 white hair girl</td>\n",
       "      <td>[game of thrones season 1]</td>\n",
       "      <td>[0:5]</td>\n",
       "      <td>[game of thrones season 1, white hair]</td>\n",
       "      <td>[0:5, 5:7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>what was the japanese motivation for bombing pearl harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[3:4, 7:9]</td>\n",
       "      <td>[japanese, motivation, bombing, pearl harbor]</td>\n",
       "      <td>[3:4, 4:5, 6:7, 7:9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>reasons why south africa should include renewable energy in its energy mix</td>\n",
       "      <td>[south africa, renewable energy]</td>\n",
       "      <td>[2:4, 6:8]</td>\n",
       "      <td>[south africa, renewable energy, energy mix]</td>\n",
       "      <td>[2:4, 6:8, 10:12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who did the voice of the magician in frosty the snowman</td>\n",
       "      <td>[frosty the snowman]</td>\n",
       "      <td>[8:11]</td>\n",
       "      <td>[the voice, the magician, frosty the snowman]</td>\n",
       "      <td>[2:4, 5:7, 8:11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>where is the movie call me by your name filmed</td>\n",
       "      <td>[call me by your name]</td>\n",
       "      <td>[4:9]</td>\n",
       "      <td>[call me by your name]</td>\n",
       "      <td>[4:9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>why does the author say that the vampire in nosferatu is named count orlok and not count dracula</td>\n",
       "      <td>[nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[9:10, 12:14, 16:18]</td>\n",
       "      <td>[vampire, nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[7:8, 9:10, 12:14, 16:18]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                            sentence  \\\n",
       "11                                           hitchhiker 's guide to the galaxy slartibartfast quotes   \n",
       "45                                                        when did rangers last win the scottish cup   \n",
       "21                                   the pair of hand drums used in indian classical music is called   \n",
       "46                                who controls the past controls the future rage against the machine   \n",
       "41                                                    who was president of the united states in 1938   \n",
       "4                                                          i love it ( feat . charli xcx ) icona pop   \n",
       "25                                                which of these was not an export of ancient greece   \n",
       "20                                                          what is the worth of the catholic church   \n",
       "18                                                        1970 world cup semi final italy vs germany   \n",
       "22                                                          game of thrones season 1 white hair girl   \n",
       "48                                         what was the japanese motivation for bombing pearl harbor   \n",
       "35                        reasons why south africa should include renewable energy in its energy mix   \n",
       "0                                            who did the voice of the magician in frosty the snowman   \n",
       "39                                                    where is the movie call me by your name filmed   \n",
       "6   why does the author say that the vampire in nosferatu is named count orlok and not count dracula   \n",
       "\n",
       "                                           aliases_hand            spans_hand  \\\n",
       "11  [hitchhiker 's guide to the galaxy, slartibartfast]            [0:6, 6:7]   \n",
       "45                              [rangers, scottish cup]            [2:3, 6:8]   \n",
       "21                             [indian classical music]                [7:10]   \n",
       "46                           [rage against the machine]                [7:11]   \n",
       "41                     [president of the united states]                 [2:7]   \n",
       "4                    [i love it, charli xcx, icona pop]      [0:3, 6:8, 9:11]   \n",
       "25                                     [ancient greece]                [8:10]   \n",
       "20                                    [catholic church]                 [6:8]   \n",
       "18                     [1970 world cup, italy, germany]       [0:3, 5:6, 7:8]   \n",
       "22                           [game of thrones season 1]                 [0:5]   \n",
       "48                             [japanese, pearl harbor]            [3:4, 7:9]   \n",
       "35                     [south africa, renewable energy]            [2:4, 6:8]   \n",
       "0                                  [frosty the snowman]                [8:11]   \n",
       "39                               [call me by your name]                 [4:9]   \n",
       "6               [nosferatu, count orlok, count dracula]  [9:10, 12:14, 16:18]   \n",
       "\n",
       "                                       aliases_bootleg  \\\n",
       "11  [hitchhiker s guide to the galaxy, slartibartfast]   \n",
       "45                             [rangers, scottish cup]   \n",
       "21                     [drums, indian classical music]   \n",
       "46                          [rage against the machine]   \n",
       "41                    [president of the united states]   \n",
       "4                   [i love it, charli xcx, icona pop]   \n",
       "25                                    [ancient greece]   \n",
       "20                            [worth, catholic church]   \n",
       "18              [1970 world cup, semi, italy, germany]   \n",
       "22              [game of thrones season 1, white hair]   \n",
       "48       [japanese, motivation, bombing, pearl harbor]   \n",
       "35        [south africa, renewable energy, energy mix]   \n",
       "0        [the voice, the magician, frosty the snowman]   \n",
       "39                              [call me by your name]   \n",
       "6     [vampire, nosferatu, count orlok, count dracula]   \n",
       "\n",
       "                spans_bootleg  \n",
       "11                 [0:6, 6:7]  \n",
       "45                 [2:3, 6:8]  \n",
       "21                [4:5, 7:10]  \n",
       "46                     [7:11]  \n",
       "41                      [2:7]  \n",
       "4            [0:3, 6:8, 9:11]  \n",
       "25                     [8:10]  \n",
       "20                 [3:4, 6:8]  \n",
       "18       [0:3, 3:4, 5:6, 7:8]  \n",
       "22                 [0:5, 5:7]  \n",
       "48       [3:4, 4:5, 6:7, 7:9]  \n",
       "35          [2:4, 6:8, 10:12]  \n",
       "0            [2:4, 5:7, 8:11]  \n",
       "39                      [4:9]  \n",
       "6   [7:8, 9:10, 12:14, 16:18]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_mentions_df = load_mentions(nq_sample_orig)\n",
    "bootleg_mentions_df = load_mentions(nq_sample_bootleg)\n",
    "\n",
    "# join dataframes and sample\n",
    "pd.merge(orig_mentions_df, bootleg_mentions_df, on=['sentence'], suffixes=['_hand', '_bootleg']).sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample above, we see that generally Bootleg detects the same mentions as the hand-labelled mentions, however sometimes Bootleg extracts extra mentions (e.g \"worth\" in \"what is the worth of the catholic church\"). This is expected as we would rather the mention detection step filter out too few mentions than too many. It will be the job of the backbone model and postprocessing to filter out these extra mentions, by either thresholding the prediction probability or predicting a candidate that represents \"No Candidate\" (we refer to this as \"NC\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disambiguate Mentions to Entities\n",
    "\n",
    "We run inference using a pretrained Bootleg model to disambiguate the extracted mentions to Wikidata QIDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model config so we can set additional parameters and load the saved model during evaluation. We need to update the config parameters to point to the downloaded model checkpoint and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg import run\n",
    "from bootleg.utils.parser_utils import get_full_config\n",
    "\n",
    "# full path to directory where files are downloaded\n",
    "config_path = f'{input_dir}/bootleg_wiki/bootleg_config.json'\n",
    "config_args = get_full_config(config_path)\n",
    "\n",
    "# set the model checkpoint path \n",
    "config_args.run_config.init_checkpoint = f'{input_dir}/bootleg_wiki/bootleg_model.pt'\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args.data_config.entity_dir = f'{input_dir}/entity_db'\n",
    "config_args.data_config.alias_cand_map = 'alias2qids_wiki.json'\n",
    "\n",
    "# set the data path and RSS500 test file \n",
    "config_args.data_config.data_dir = f'{input_dir}/nq'\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args.data_config.test_dataset.file = 'test_natural_questions_50_bootleg.jsonl'\n",
    "\n",
    "# set the embedding paths \n",
    "config_args.data_config.emb_dir =  f'{input_dir}/emb_data'\n",
    "config_args.data_config.word_embedding.cache_dir =  f'{input_dir}/emb_data'\n",
    "\n",
    "# set the save directory \n",
    "config_args.run_config.save_dir = f'{input_dir}/results'\n",
    "\n",
    "# set whether to run inference on the CPU\n",
    "config_args.run_config.cpu = use_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation in `dump_embs` mode to dump predictions and contextualized entity embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-09 13:15:16,824 PyTorch version 1.5.0 available.\n",
      "2020-09-09 13:15:17,933 Loading entity_symbols...\n",
      "2020-09-09 13:16:03,416 Loaded entity_symbols with 5222808 entities.\n",
      "2020-09-09 13:16:04,593 Loading slices...\n",
      "2020-09-09 13:16:04,599 Finished loading slices.\n",
      "2020-09-09 13:16:04,603 Loading dataset...\n",
      "2020-09-09 13:16:04,606 Finished loading dataset.\n",
      "2020-09-09 13:16:18,836 Sampled 50 indices from dataset (dev/test) for evaluation.\n",
      "2020-09-09 13:16:19,055 Loading embeddings...\n",
      "2020-09-09 13:16:44,103 Finished loading embeddings.\n",
      "2020-09-09 13:16:44,224 Loading model from data/bootleg_wiki/bootleg_model.pt...\n",
      "2020-09-09 13:16:46,763 Successfully loaded model from data/bootleg_wiki/bootleg_model.pt starting from checkpoint epoch 2 and step 0.\n",
      "2020-09-09 13:16:46,805 ************************DUMPING PREDICTIONS FOR test_natural_questions_50_bootleg.jsonl************************\n",
      "2020-09-09 13:16:47,021 64 samples, 2 batches\n",
      "2020-09-09 13:16:51,935 Writing predictions...\n",
      "2020-09-09 13:16:51,939 Total number of mentions across all sentences: 100\n",
      "2020-09-09 13:16:51,985 Finished writing predictions to data/results/20200909_131516/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_labels.jsonl\n",
      "2020-09-09 13:16:52,042 Saving contextual entity embeddings to data/results/20200909_131516/test_natural_questions_50_bootleg/eval/bootleg_model/bootleg_embs.npy\n"
     ]
    }
   ],
   "source": [
    "bootleg_label_file, bootleg_emb_file = run.model_eval(args=config_args, mode=\"dump_embs\", logger=logger, is_writer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the overall quality of the end-to-end pipeline via precision / recall metrics, where the *recall* indicates what proportion of the hand-labelled mentions Bootleg correctly detects and disambiguates, and *precision* indicates what proportion of the mentions that Bootleg labels are correct. For instance, if Bootleg only labelled the few mentions it was very confident in, then it would have a low recall and high precision.\n",
    "\n",
    "To detect if mentions match the hand-labelled mention spans, we allow for +1/-1 word in the left span boundaries (e.g., 'the wizard of oz' and 'wizard of oz' are counted as the same mention). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.71 (55/78)\n",
      "Precision: 0.6 (55/92)\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_precision_and_recall\n",
    "\n",
    "bootleg_errors = compute_precision_and_recall(orig_label_file=nq_sample_orig, \n",
    "                                              new_label_file=bootleg_label_file, \n",
    "                                              threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze three classes of errors in the end-to-end pipeline below: \n",
    "1. *Missing mentions*: Fail to extract the mention \n",
    "2. *Wrong entity*: Correctly extract the mention but disambiguate to the wrong candidate  \n",
    "3. *Extra mentions*: Label a mention that is not hand-labelled as a mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>is there an active volcano in new zealand</td>\n",
       "      <td>[new zealand]</td>\n",
       "      <td>[Q664]</td>\n",
       "      <td>[6:8]</td>\n",
       "      <td>[active volcano, new zealand]</td>\n",
       "      <td>[3:5, 6:8]</td>\n",
       "      <td>[Q8072, NC]</td>\n",
       "      <td>[0.735, 0.283]</td>\n",
       "      <td>new zealand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>where is israel located on the world map</td>\n",
       "      <td>[israel, world map]</td>\n",
       "      <td>[Q801, Q653848]</td>\n",
       "      <td>[2:3, 6:8]</td>\n",
       "      <td>[israel, world map]</td>\n",
       "      <td>[2:3, 6:8]</td>\n",
       "      <td>[NC, Q653848]</td>\n",
       "      <td>[0.164, 0.908]</td>\n",
       "      <td>israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>who played in the last 3 nba finals</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[Q842375]</td>\n",
       "      <td>[6:8]</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[6:8]</td>\n",
       "      <td>[NC]</td>\n",
       "      <td>[0.115]</td>\n",
       "      <td>nba finals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>when did rangers last win the scottish cup</td>\n",
       "      <td>[rangers, scottish cup]</td>\n",
       "      <td>[Q19597, Q308822]</td>\n",
       "      <td>[2:3, 6:8]</td>\n",
       "      <td>[rangers, scottish cup]</td>\n",
       "      <td>[2:3, 6:8]</td>\n",
       "      <td>[Q19597, NC]</td>\n",
       "      <td>[0.696, 0.116]</td>\n",
       "      <td>scottish cup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_idx                                    sentence  \\\n",
       "0         7   is there an active volcano in new zealand   \n",
       "1        23    where is israel located on the world map   \n",
       "2        32         who played in the last 3 nba finals   \n",
       "3        45  when did rangers last win the scottish cup   \n",
       "\n",
       "              gold_aliases          gold_qids  gold_spans  \\\n",
       "0            [new zealand]             [Q664]       [6:8]   \n",
       "1      [israel, world map]    [Q801, Q653848]  [2:3, 6:8]   \n",
       "2             [nba finals]          [Q842375]       [6:8]   \n",
       "3  [rangers, scottish cup]  [Q19597, Q308822]  [2:3, 6:8]   \n",
       "\n",
       "                    pred_aliases  pred_spans      pred_qids      pred_probs  \\\n",
       "0  [active volcano, new zealand]  [3:5, 6:8]    [Q8072, NC]  [0.735, 0.283]   \n",
       "1            [israel, world map]  [2:3, 6:8]  [NC, Q653848]  [0.164, 0.908]   \n",
       "2                   [nba finals]       [6:8]           [NC]         [0.115]   \n",
       "3        [rangers, scottish cup]  [2:3, 6:8]   [Q19597, NC]  [0.696, 0.116]   \n",
       "\n",
       "          error  \n",
       "0   new zealand  \n",
       "1        israel  \n",
       "2    nba finals  \n",
       "3  scottish cup  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['missing_mention'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mentions above get filtered because we set the probability threshold to 0.3 to help filter extra mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>who played smiley in tinker tailor soldier spy</td>\n",
       "      <td>[tinker tailor soldier spy]</td>\n",
       "      <td>[Q681962]</td>\n",
       "      <td>[4:8]</td>\n",
       "      <td>[smiley, tinker tailor soldier spy]</td>\n",
       "      <td>[2:3, 4:8]</td>\n",
       "      <td>[Q11241, Q582811]</td>\n",
       "      <td>[0.885, 0.697]</td>\n",
       "      <td>tinker tailor soldier spy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>why does the author say that the vampire in nosferatu is named count orlok and not count dracula</td>\n",
       "      <td>[nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[Q151895, Q1442062, Q3266236]</td>\n",
       "      <td>[9:10, 12:14, 16:18]</td>\n",
       "      <td>[vampire, nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[7:8, 9:10, 12:14, 16:18]</td>\n",
       "      <td>[Q1425557, Q151895, Q1442062, Q41542]</td>\n",
       "      <td>[0.766, 0.863, 1.0, 0.52]</td>\n",
       "      <td>count dracula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>the pair of hand drums used in indian classical music is called</td>\n",
       "      <td>[indian classical music]</td>\n",
       "      <td>[Q1323698]</td>\n",
       "      <td>[7:10]</td>\n",
       "      <td>[drums, indian classical music]</td>\n",
       "      <td>[4:5, 7:10]</td>\n",
       "      <td>[Q128309, Q1770695]</td>\n",
       "      <td>[0.685, 0.942]</td>\n",
       "      <td>indian classical music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38</td>\n",
       "      <td>when was the wizard of oz made in technicolor</td>\n",
       "      <td>[wizard of oz, technicolor]</td>\n",
       "      <td>[Q193695, Q674564]</td>\n",
       "      <td>[3:6, 8:9]</td>\n",
       "      <td>[the wizard of oz, technicolor]</td>\n",
       "      <td>[2:6, 8:9]</td>\n",
       "      <td>[Q60447411, Q674564]</td>\n",
       "      <td>[0.676, 0.904]</td>\n",
       "      <td>wizard of oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>31</td>\n",
       "      <td>who does oregon state play in the college world series</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[Q7101349, Q787505]</td>\n",
       "      <td>[2:4, 7:10]</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[2:4, 7:10]</td>\n",
       "      <td>[Q2893390, Q787505]</td>\n",
       "      <td>[0.583, 0.7]</td>\n",
       "      <td>oregon state</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx  \\\n",
       "7         24   \n",
       "3          6   \n",
       "6         21   \n",
       "13        38   \n",
       "9         31   \n",
       "\n",
       "                                                                                            sentence  \\\n",
       "7                                                     who played smiley in tinker tailor soldier spy   \n",
       "3   why does the author say that the vampire in nosferatu is named count orlok and not count dracula   \n",
       "6                                    the pair of hand drums used in indian classical music is called   \n",
       "13                                                     when was the wizard of oz made in technicolor   \n",
       "9                                             who does oregon state play in the college world series   \n",
       "\n",
       "                               gold_aliases                      gold_qids  \\\n",
       "7               [tinker tailor soldier spy]                      [Q681962]   \n",
       "3   [nosferatu, count orlok, count dracula]  [Q151895, Q1442062, Q3266236]   \n",
       "6                  [indian classical music]                     [Q1323698]   \n",
       "13              [wizard of oz, technicolor]             [Q193695, Q674564]   \n",
       "9      [oregon state, college world series]            [Q7101349, Q787505]   \n",
       "\n",
       "              gold_spans                                      pred_aliases  \\\n",
       "7                  [4:8]               [smiley, tinker tailor soldier spy]   \n",
       "3   [9:10, 12:14, 16:18]  [vampire, nosferatu, count orlok, count dracula]   \n",
       "6                 [7:10]                   [drums, indian classical music]   \n",
       "13            [3:6, 8:9]                   [the wizard of oz, technicolor]   \n",
       "9            [2:4, 7:10]              [oregon state, college world series]   \n",
       "\n",
       "                   pred_spans                              pred_qids  \\\n",
       "7                  [2:3, 4:8]                      [Q11241, Q582811]   \n",
       "3   [7:8, 9:10, 12:14, 16:18]  [Q1425557, Q151895, Q1442062, Q41542]   \n",
       "6                 [4:5, 7:10]                    [Q128309, Q1770695]   \n",
       "13                 [2:6, 8:9]                   [Q60447411, Q674564]   \n",
       "9                 [2:4, 7:10]                    [Q2893390, Q787505]   \n",
       "\n",
       "                   pred_probs                      error  \n",
       "7              [0.885, 0.697]  tinker tailor soldier spy  \n",
       "3   [0.766, 0.863, 1.0, 0.52]              count dracula  \n",
       "6              [0.685, 0.942]     indian classical music  \n",
       "13             [0.676, 0.904]               wizard of oz  \n",
       "9                [0.583, 0.7]               oregon state  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['wrong_entity']).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the errors Bootleg makes is predicting too general of a candidate (e.g. Oregon State Beavers instead of Oregon State Beavers baseball). Other errors are due to ambiguous sentences (e.g. \"cast of characters in fiddler on the roof\" -> should this be the movie or the musical?). Finally another bucket of errors suggests that we need to boost certain training signals -- this is an area we're actively pursuing in Bootleg with an investigation of model guidability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_probs</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>48</td>\n",
       "      <td>what was the japanese motivation for bombing pearl harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[Q188712, Q127091]</td>\n",
       "      <td>[3:4, 7:9]</td>\n",
       "      <td>[japanese, motivation, bombing, pearl harbor]</td>\n",
       "      <td>[3:4, 4:5, 6:7, 7:9]</td>\n",
       "      <td>[Q184425, Q644302, Q52418, Q52418]</td>\n",
       "      <td>[0.744, 0.986, 0.985, 0.706]</td>\n",
       "      <td>motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>36</td>\n",
       "      <td>when was the first freeway built in los angeles</td>\n",
       "      <td>[los angeles]</td>\n",
       "      <td>[Q65]</td>\n",
       "      <td>[7:9]</td>\n",
       "      <td>[freeway, los angeles]</td>\n",
       "      <td>[4:5, 7:9]</td>\n",
       "      <td>[Q46622, Q65]</td>\n",
       "      <td>[0.867, 0.506]</td>\n",
       "      <td>freeway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>why does the author say that the vampire in nosferatu is named count orlok and not count dracula</td>\n",
       "      <td>[nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[Q151895, Q1442062, Q3266236]</td>\n",
       "      <td>[9:10, 12:14, 16:18]</td>\n",
       "      <td>[vampire, nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[7:8, 9:10, 12:14, 16:18]</td>\n",
       "      <td>[Q1425557, Q151895, Q1442062, Q41542]</td>\n",
       "      <td>[0.766, 0.863, 1.0, 0.52]</td>\n",
       "      <td>vampire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27</td>\n",
       "      <td>i see the river tiber foaming with much blood</td>\n",
       "      <td>[river tiber]</td>\n",
       "      <td>[Q13712]</td>\n",
       "      <td>[3:5]</td>\n",
       "      <td>[river tiber, foaming, blood]</td>\n",
       "      <td>[3:5, 5:6, 8:9]</td>\n",
       "      <td>[Q13712, Q7243541, Q7873]</td>\n",
       "      <td>[1.0, 1.0, 0.9]</td>\n",
       "      <td>blood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>1970 world cup semi final italy vs germany</td>\n",
       "      <td>[1970 world cup, italy, germany]</td>\n",
       "      <td>[Q132664, Q676899, Q43310]</td>\n",
       "      <td>[0:3, 5:6, 7:8]</td>\n",
       "      <td>[1970 world cup, semi, italy, germany]</td>\n",
       "      <td>[0:3, 3:4, 5:6, 7:8]</td>\n",
       "      <td>[Q132664, Q40008974, Q676899, Q43310]</td>\n",
       "      <td>[0.866, 0.332, 0.364, 0.354]</td>\n",
       "      <td>semi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx  \\\n",
       "16        48   \n",
       "14        36   \n",
       "2          6   \n",
       "11        27   \n",
       "5         18   \n",
       "\n",
       "                                                                                            sentence  \\\n",
       "16                                         what was the japanese motivation for bombing pearl harbor   \n",
       "14                                                   when was the first freeway built in los angeles   \n",
       "2   why does the author say that the vampire in nosferatu is named count orlok and not count dracula   \n",
       "11                                                     i see the river tiber foaming with much blood   \n",
       "5                                                         1970 world cup semi final italy vs germany   \n",
       "\n",
       "                               gold_aliases                      gold_qids  \\\n",
       "16                 [japanese, pearl harbor]             [Q188712, Q127091]   \n",
       "14                            [los angeles]                          [Q65]   \n",
       "2   [nosferatu, count orlok, count dracula]  [Q151895, Q1442062, Q3266236]   \n",
       "11                            [river tiber]                       [Q13712]   \n",
       "5          [1970 world cup, italy, germany]     [Q132664, Q676899, Q43310]   \n",
       "\n",
       "              gold_spans                                      pred_aliases  \\\n",
       "16            [3:4, 7:9]     [japanese, motivation, bombing, pearl harbor]   \n",
       "14                 [7:9]                            [freeway, los angeles]   \n",
       "2   [9:10, 12:14, 16:18]  [vampire, nosferatu, count orlok, count dracula]   \n",
       "11                 [3:5]                     [river tiber, foaming, blood]   \n",
       "5        [0:3, 5:6, 7:8]            [1970 world cup, semi, italy, germany]   \n",
       "\n",
       "                   pred_spans                              pred_qids  \\\n",
       "16       [3:4, 4:5, 6:7, 7:9]     [Q184425, Q644302, Q52418, Q52418]   \n",
       "14                 [4:5, 7:9]                          [Q46622, Q65]   \n",
       "2   [7:8, 9:10, 12:14, 16:18]  [Q1425557, Q151895, Q1442062, Q41542]   \n",
       "11            [3:5, 5:6, 8:9]              [Q13712, Q7243541, Q7873]   \n",
       "5        [0:3, 3:4, 5:6, 7:8]  [Q132664, Q40008974, Q676899, Q43310]   \n",
       "\n",
       "                      pred_probs       error  \n",
       "16  [0.744, 0.986, 0.985, 0.706]  motivation  \n",
       "14                [0.867, 0.506]     freeway  \n",
       "2      [0.766, 0.863, 1.0, 0.52]     vampire  \n",
       "11               [1.0, 1.0, 0.9]       blood  \n",
       "5   [0.866, 0.332, 0.364, 0.354]        semi  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_errors['extra_mention']).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Bootleg may detect and label extraneous mentions that were not hand-labelled. Setting the threshold higher helps to reduce these predictions, as does using a 'NC' candidate for training, which Bootleg also supports . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare to TAGME \n",
    "\n",
    "To get a sense of how Bootleg is doing compared to other systems, we evaluate [TAGME](https://arxiv.org/pdf/1006.3498.pdf), an existing tool to extract and disambiguate mentions. To run TAGME, you need to get a (free) authorization token. Instructions for obtaining a token are [here](https://sobigdata.d4science.org/web/tagme/tagme-help). You will need to verify your account and then follow the \"access the VRE\") link. We've also provided the file with TAGME labels for a given threshold for download if you want to skip the authorization token.\n",
    "\n",
    "We note that unlike TAGME, Bootleg also outputs contextual entity embeddings which can be loaded for use in downstream tasks (e.g. relation extraction, question answering). Check out the Entity Embedding tutorial for more details! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tagme\n",
    "# Set the authorization token for subsequent calls.\n",
    "tagme.GCUBE_TOKEN = # FILL IN WITH YOUR TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagme_label_file = f'{input_dir}/nq/test_natural_questions_50_tagme.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a token, skip the cell below and load the pre-generated TAGME labels. If you do have a token, you can play with changing the threshold below and see how it affects the results. Increasing the threshold increases the precision but decreases the recall as TAGME, as TAGME will label fewer mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a mapping from Wikipedia pageids to Wikidata QIDs to get the QIDs predicted by TAGME \n",
    "wpid2qid = ujson.load(open(f'{input_dir}/entity_db/entity_mappings/wpid2qid.json'))\n",
    "\n",
    "# As the threshold increases, the precision increases, but the recall decreases\n",
    "tagme_annotate(in_file=nq_sample_orig, out_file=tagme_label_file, threshold=0.3, wpid2qid=wpid2qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.63 (49/78)\n",
      "Precision: 0.58 (49/84)\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_precision_and_recall\n",
    "tagme_errors = compute_precision_and_recall(orig_label_file=nq_sample_orig, \n",
    "                                            new_label_file=tagme_label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that TAGME has slightly worse recall than Bootleg, when the precisions are set to be comparable (changing either TAGME or Bootleg's threshold will change the recall/precision values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Annotate On-the-Fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To annotate individual sentences with Bootleg, we  also support annotate-on-the-fly mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we create an annotator object. This loads the model and entity databases. We use the `config_args` loaded from the previous step. Note it takes several minutes for the initial load of the model and the entity data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-09 13:18:12,429 Loading embeddings...\n",
      "2020-09-09 13:18:36,539 Finished loading embeddings.\n",
      "2020-09-09 13:19:03,200 Loading candidate mapping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7970529/7970529 [00:14<00:00, 557925.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-09 13:19:17,498 Loaded candidate mapping with 7970529 aliases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bootleg.annotator import Annotator\n",
    "\n",
    "ann = Annotator(config_args=config_args, cand_map=cand_map, device='cuda' if not use_cpu else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to TAGME, we allow setting a threshold to only return mentions with labels greater than some probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.set_threshold(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in sentences to see what Bootleg predicts! Bootleg outputs the QIDs (or \"NC\" for \"No Candidate\"), the associated probabilities, and the title for each mention. The QIDs map to Wikidata -- to look them up you can use https://www.wikidata.org/wiki/Q1454 and replace the QID. \"NC\" means Bootleg did not find a good match among the candidates in the candidate list given the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q1517373', 'Q1454'],\n",
       " [1.0, 0.9959885478019714],\n",
       " ['Outer Banks', 'North Carolina'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"where is the outer banks in north carolina\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q487330'], [0.8602001070976257], ['Fiddler on the Roof'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"cast of characters in fiddler on the roof\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the entity disambiguation problem can be quite tricky -- in the above example we predict the song \"Fiddler on the Roof\" the music instead of the hand-label of the movie (https://www.wikidata.org/wiki/Q934036). Giving additional cues may help though -- for instance, if we add \"the movie\", the prediction changes to the movie! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Q934036'], [0.7369491457939148], ['Fiddler on the Roof (film)'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions(\"cast of characters in the movie fiddler on the roof\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctx",
   "language": "python",
   "name": "ctx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
