{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End NED Tutorial\n",
    "\n",
    "In this tutorial, we walk through how to use Bootleg as an end-to-end pipeline to detect and label entities in a set of sentences. First, we show how to use Bootleg to detect and disambiguate mentions to entities. We then compare to an existing system named TAGME. \n",
    "\n",
    "This tutorial assumes you want to use Bootleg on full datasets. You can also use Bootleg in annotator mode:\n",
    "\n",
    "```\n",
    "pip install bootleg\n",
    "from bootleg.end2end.bootleg_annotator import BootlegAnnotator\n",
    "ann = BootlegAnnotator()\n",
    "ann.label_mentions(\"Bob Dylan release Desire\")[\"titles\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how Bootleg performs on more natural language than we find in Wikipedia, we hand label the mentions and corresponding entities in 50 questions sampled from the [Natural Questions dataset (Google)](https://ai.google.com/research/NaturalQuestions). We will evaluate our *uncased* Bootleg model. However, we have manually cased the data in case you want to try our cased model instead.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- Pretrained Bootleg uncased model and config [here](https://bootleg-data.s3.amazonaws.com/models/lateset/bootleg_uncased.tar.gz). Cased model and config [here](https://bootleg-data.s3.amazonaws.com/models/lateset/bootleg_cased.tar.gz)\n",
    "- Sample of Natural Questions with hand-labelled entities [here](https://bootleg-data.s3.amazonaws.com/data/lateset/nq.tar.gz)\n",
    "- Entity data [here](https://bootleg-data.s3.amazonaws.com/data/lateset/wiki_entity_data.tar.gz)\n",
    "- Embedding data [here](https://bootleg-data.s3.amazonaws.com/data/lateset/emb_data.tar.gz)\n",
    "\n",
    "For convenience, you can run the commands below (from the root directory of the repo) to download all the above files and unpack them to `models` and `data` directories. It will take several minutes to download all the files. \n",
    "\n",
    "```\n",
    "    # use cased for cased model\n",
    "    bash tutorials/download_model.sh uncased\n",
    "    bash tutorials/download_data.sh\n",
    "```\n",
    "\n",
    "You can also run directly in this notebook by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!sh download_model.sh uncased\n",
    "!sh download_data.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "# Set to logging.DEBUG for more logging output\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# root_dir = FILL IN FULL PATH TO DIRECTORY WHERE DATA IS DOWNLOADED (e.g., root_dir/data and root_dir/models)\n",
    "root_dir = Path(\".\")\n",
    "cand_map = root_dir / 'data/wiki_entity_data/entity_mappings/alias2qids_wiki_filt.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU with at least 12GB of memory available, set the below to 0 to run inference on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect Mentions\n",
    "Bootleg uses a simple mention extraction algorithm that extracts mentions using a given candidate map. We will use a Wikipedia candidate map that we mined using Wikipedia anchor links and Wikidata aliases for a total of ~15 million mentions (provided in the Requirements section of this notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the input dataset for the end-to-end pipeline, we assume a jsonlines file with a single dictionary with the key \"sentence\" and value as the text of the sentence, per line. For instance, you may have a file with the lines:\n",
    "\n",
    "    {\"sentence\": \"Who did the voice of the magician in Frosty the Snowman\"}\n",
    "    {\"sentence\": \"What is considered the Outer Banks in North Carolina\"}\n",
    "    \n",
    "Below, we have additional keys to keep track of the hand-labelled mentions, but this is purely for evaluating the quality of the end-to-end pipeline and is not needed in the common use cases of using Bootleg to detect and label mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_sample_orig = root_dir / 'data/nq/test_50.jsonl'\n",
    "nq_sample_bootleg = root_dir / 'data/nq/test_50_bootleg.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15202497/15202497 [00:28<00:00, 529759.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.53it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 40.21it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 36.73it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 33.45it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 34.35it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 31.11it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 31.36it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 25.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from bootleg.end2end.extract_mentions import extract_mentions\n",
    "verbose = False\n",
    "extract_mentions(in_filepath=nq_sample_orig, out_filepath=nq_sample_bootleg, cand_map_file=cand_map, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at a sample of the extracted mentions, we can compare the mention extraction phase to the hand-labelled mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>aliases_hand</th>\n",
       "      <th>spans_hand</th>\n",
       "      <th>aliases_bootleg</th>\n",
       "      <th>spans_bootleg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Who controls the past controls the future Rage Against the Machine</td>\n",
       "      <td>[rage against the machine]</td>\n",
       "      <td>[[7, 11]]</td>\n",
       "      <td>[rage against the machine]</td>\n",
       "      <td>[[7, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Where does the last name Aponte come from</td>\n",
       "      <td>[aponte]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[aponte]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is the worth of the Catholic Church</td>\n",
       "      <td>[catholic church]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[worth, catholic church]</td>\n",
       "      <td>[[3, 4], [6, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Is it a bank holiday today in Spain</td>\n",
       "      <td>[bank holiday, spain]</td>\n",
       "      <td>[[3, 5], [7, 8]]</td>\n",
       "      <td>[bank holiday, spain]</td>\n",
       "      <td>[[3, 5], [7, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Who played in the last 3 NBA Finals</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "      <td>[nba finals]</td>\n",
       "      <td>[[6, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is considered the Outer Banks in North Carolina</td>\n",
       "      <td>[outer banks, north carolina]</td>\n",
       "      <td>[[4, 6], [7, 9]]</td>\n",
       "      <td>[outer banks, north carolina]</td>\n",
       "      <td>[[4, 6], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Who is Mariah Carey talking about in We Belong Together</td>\n",
       "      <td>[mariah carey, we belong together]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "      <td>[mariah carey]</td>\n",
       "      <td>[[2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Who played Smiley in Tinker Tailor Soldier Spy</td>\n",
       "      <td>[tinker tailor soldier spy]</td>\n",
       "      <td>[[4, 8]]</td>\n",
       "      <td>[smiley, tinker tailor soldier spy]</td>\n",
       "      <td>[[2, 3], [4, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The representative of the British crown in NZ</td>\n",
       "      <td>[british crown, nz]</td>\n",
       "      <td>[[4, 6], [7, 8]]</td>\n",
       "      <td>[british crown, nz]</td>\n",
       "      <td>[[4, 6], [7, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who did the voice of the magician in Frosty the Snowman</td>\n",
       "      <td>[frosty the snowman]</td>\n",
       "      <td>[[8, 11]]</td>\n",
       "      <td>[voice of, magician, frosty the snowman]</td>\n",
       "      <td>[[3, 5], [6, 7], [8, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Landmark Supreme Court cases dealing with the First Amendment</td>\n",
       "      <td>[supreme court, first amendment]</td>\n",
       "      <td>[[1, 3], [7, 9]]</td>\n",
       "      <td>[supreme court, first amendment]</td>\n",
       "      <td>[[1, 3], [7, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Once Upon a Time Season 6 episode list</td>\n",
       "      <td>[once upon a time season 6]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "      <td>[upon a time, season 6, episode list]</td>\n",
       "      <td>[[1, 4], [4, 6], [6, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Who plays Norman Bates in the TV show</td>\n",
       "      <td>[norman bates]</td>\n",
       "      <td>[[2, 4]]</td>\n",
       "      <td>[norman bates]</td>\n",
       "      <td>[[2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I see the river Tiber foaming with much blood</td>\n",
       "      <td>[river tiber]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "      <td>[river tiber]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1970 World Cup semi final Italy vs Germany</td>\n",
       "      <td>[1970 world cup, italy, germany]</td>\n",
       "      <td>[[0, 3], [5, 6], [7, 8]]</td>\n",
       "      <td>[1970 world cup, italy, germany]</td>\n",
       "      <td>[[0, 3], [5, 6], [7, 8]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              sentence  \\\n",
       "46  Who controls the past controls the future Rage Against the Machine   \n",
       "14                           Where does the last name Aponte come from   \n",
       "20                            What is the worth of the Catholic Church   \n",
       "17                                 Is it a bank holiday today in Spain   \n",
       "32                                 Who played in the last 3 NBA Finals   \n",
       "1                 What is considered the Outer Banks in North Carolina   \n",
       "47             Who is Mariah Carey talking about in We Belong Together   \n",
       "24                      Who played Smiley in Tinker Tailor Soldier Spy   \n",
       "44                       The representative of the British crown in NZ   \n",
       "0              Who did the voice of the magician in Frosty the Snowman   \n",
       "37       Landmark Supreme Court cases dealing with the First Amendment   \n",
       "8                               Once Upon a Time Season 6 episode list   \n",
       "10                               Who plays Norman Bates in the TV show   \n",
       "27                       I see the river Tiber foaming with much blood   \n",
       "18                          1970 World Cup semi final Italy vs Germany   \n",
       "\n",
       "                          aliases_hand                spans_hand  \\\n",
       "46          [rage against the machine]                 [[7, 11]]   \n",
       "14                            [aponte]                  [[5, 6]]   \n",
       "20                   [catholic church]                  [[6, 8]]   \n",
       "17               [bank holiday, spain]          [[3, 5], [7, 8]]   \n",
       "32                        [nba finals]                  [[6, 8]]   \n",
       "1        [outer banks, north carolina]          [[4, 6], [7, 9]]   \n",
       "47  [mariah carey, we belong together]         [[2, 4], [7, 10]]   \n",
       "24         [tinker tailor soldier spy]                  [[4, 8]]   \n",
       "44                 [british crown, nz]          [[4, 6], [7, 8]]   \n",
       "0                 [frosty the snowman]                 [[8, 11]]   \n",
       "37    [supreme court, first amendment]          [[1, 3], [7, 9]]   \n",
       "8          [once upon a time season 6]                  [[0, 6]]   \n",
       "10                      [norman bates]                  [[2, 4]]   \n",
       "27                       [river tiber]                  [[3, 5]]   \n",
       "18    [1970 world cup, italy, germany]  [[0, 3], [5, 6], [7, 8]]   \n",
       "\n",
       "                             aliases_bootleg              spans_bootleg  \n",
       "46                [rage against the machine]                  [[7, 11]]  \n",
       "14                                  [aponte]                   [[5, 6]]  \n",
       "20                  [worth, catholic church]           [[3, 4], [6, 8]]  \n",
       "17                     [bank holiday, spain]           [[3, 5], [7, 8]]  \n",
       "32                              [nba finals]                   [[6, 8]]  \n",
       "1              [outer banks, north carolina]           [[4, 6], [7, 9]]  \n",
       "47                            [mariah carey]                   [[2, 4]]  \n",
       "24       [smiley, tinker tailor soldier spy]           [[2, 3], [4, 8]]  \n",
       "44                       [british crown, nz]           [[4, 6], [7, 8]]  \n",
       "0   [voice of, magician, frosty the snowman]  [[3, 5], [6, 7], [8, 11]]  \n",
       "37          [supreme court, first amendment]           [[1, 3], [7, 9]]  \n",
       "8      [upon a time, season 6, episode list]   [[1, 4], [4, 6], [6, 8]]  \n",
       "10                            [norman bates]                   [[2, 4]]  \n",
       "27                             [river tiber]                   [[3, 5]]  \n",
       "18          [1970 world cup, italy, germany]   [[0, 3], [5, 6], [7, 8]]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import load_mentions\n",
    "\n",
    "orig_mentions_df = load_mentions(nq_sample_orig)\n",
    "bootleg_mentions_df = load_mentions(nq_sample_bootleg)\n",
    "\n",
    "# join dataframes and sample\n",
    "res = pd.merge(orig_mentions_df, bootleg_mentions_df, on=['sentence'], suffixes=['_hand', '_bootleg'])\n",
    "display(res.sample(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample above, we see that generally Bootleg detects the same mentions as the hand-labelled mentions, however sometimes Bootleg extracts extra mentions (e.g \"colonies\" in \"Where did Britain create colonies for its empire\"). This is expected as we would rather the mention detection step filter out too few mentions than too many. It will be the job of the backbone model and postprocessing to filter out these extra mentions, by either thresholding the prediction probability or predicting a candidate that represents \"No Candidate\" (we refer to this as \"NC\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Disambiguate Mentions to Entities\n",
    "\n",
    "We run inference using a pretrained Bootleg model to disambiguate the extracted mentions to Wikidata QIDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model config so we can set additional parameters and load the saved model during evaluation. We need to update the config parameters to point to the downloaded model checkpoint and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.utils.parser.parser_utils import parse_boot_and_emm_args\n",
    "from bootleg.utils.utils import load_yaml_file\n",
    "from bootleg.run import run_model\n",
    "\n",
    "config_in_path = root_dir / 'models/bootleg_uncased/bootleg_config.yaml'\n",
    "\n",
    "config_args = load_yaml_file(config_in_path)\n",
    "\n",
    "# decrease number of data threads as this is a small file\n",
    "config_args[\"run_config\"][\"dataset_threads\"] = 2\n",
    "config_args[\"run_config\"][\"log_level\"] = \"info\"\n",
    "# set the model checkpoint path \n",
    "config_args[\"emmental\"][\"model_path\"] = str(root_dir / 'models/bootleg_uncased/bootleg_wiki.pth')\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args[\"data_config\"][\"entity_dir\"] = str(root_dir / 'data/wiki_entity_data')\n",
    "config_args[\"data_config\"][\"alias_cand_map\"] = \"alias2qids_wiki_filt.json\"\n",
    "\n",
    "# set the data path and kore50 test file \n",
    "config_args[\"data_config\"][\"data_dir\"] = str(root_dir / 'data/nq')\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args[\"data_config\"][\"test_dataset\"][\"file\"] = nq_sample_bootleg.name\n",
    "\n",
    "# set the embedding paths \n",
    "config_args[\"data_config\"][\"emb_dir\"] =  str(root_dir / 'data/emb_data')\n",
    "config_args[\"data_config\"][\"word_embedding\"][\"cache_dir\"] =  str(root_dir / 'data/emb_data/pretrained_bert_models')\n",
    "\n",
    "# set the devie if on CPU\n",
    "config_args[\"emmental\"][\"device\"] = device\n",
    "\n",
    "# save the new args (helps if you want to run things via command line)\n",
    "config_args = parse_boot_and_emm_args(config_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 11:49:21,849 Logging was already initialized to use bootleg_logs/wiki_full_ft/2021_02_12/11_46_30/524a6d16.  To configure logging manually, call emmental.init_logging before initialiting Meta.\n",
      "2021-02-12 11:49:21,910 Loading Emmental default config from /dfs/scratch0/lorr1/projects/emmental/src/emmental/emmental-default-config.yaml.\n",
      "2021-02-12 11:49:21,912 Updating Emmental config from user provided config.\n",
      "2021-02-12 11:49:22,042 COMMAND: /dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/ipykernel_launcher.py -f /dfs/scratch0/lorr1/projects/bootleg/tutorials/:/afs/cs.stanford.edu/u/lorr1/.local/apt-cache/share/jupyter/runtime/kernel-94bc511c-5c97-4658-924c-58d7cc619f20.json\n",
      "2021-02-12 11:49:22,043 Saving config to bootleg_logs/wiki_full_ft/2021_02_12/11_46_30/524a6d16/parsed_config.yaml\n",
      "2021-02-12 11:49:22,358 Git Hash: Not able to retrieve git hash\n",
      "2021-02-12 11:49:22,360 Loading entity symbols...\n",
      "2021-02-12 11:51:20,374 Starting to build data for test from ../tutorial_data/data/nq/test_50_bootleg.jsonl\n",
      "2021-02-12 11:51:20,399 Building dataset from scratch. Saving to ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1.\n",
      "2021-02-12 11:51:20,400 Starting to extract examples using 2 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/datasets/dataset.py:913: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  guid_dtype = np.dtype(\n",
      "Reading in ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_input/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 426.92it/s]\n",
      "Reading in ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_input/out_1.jsonl: 100%|██████████| 25/25 [00:00<00:00, 481.79it/s]\n",
      "/dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/numpy/core/memmap.py:230: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  descr = dtypedescr(dtype)\n",
      "Processing ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_output/out_0.jsonl:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "guid:                            0 subsent 0\n",
      "examples:                        [CLS] who did the voice of the magician in frost ##y the snow ##man [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[4, 6], [7, 8], [9, 14]]\n",
      "aliases_to_predict:              [0, 1, 2]\n",
      "train_aliases_to_predict_arr:    [0, 1, 2]\n",
      "alias_list_pos:                  [0, 1, 2]\n",
      "aliases:                         ['voice of', 'magician', 'frosty the snowman']\n",
      "qids:                            ['Q-1', 'Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 4.  7.  9. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 5.  7. 13. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -2. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -2. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(0, 0, [ 0,  1,  2, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            3 subsent 0\n",
      "examples:                        [CLS] what channel is the premier league on in france [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[5, 7], [9, 10]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['premier league', 'france']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 5.  9. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 6.  9. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(3, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            6 subsent 0\n",
      "examples:                        [CLS] why does the author say that the vampire in nos ##fera ##tu is named count or ##lok and not count dracula [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[8, 9], [10, 13], [15, 18], [20, 22]]\n",
      "aliases_to_predict:              [0, 1, 2, 3]\n",
      "train_aliases_to_predict_arr:    [0, 1, 2, 3]\n",
      "alias_list_pos:                  [0, 1, 2, 3]\n",
      "aliases:                         ['vampire', 'nosferatu', 'count orlok', 'count dracula']\n",
      "qids:                            ['Q-1', 'Q-1', 'Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 8. 10. 15. 20. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 8. 12. 17. 21. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -2. -2. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -2. -2. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(6, 0, [ 0,  1,  2,  3, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            9 subsent 0\n",
      "examples:                        [CLS] who is the former co - chairman goldman sachs who became a u . s . secretary of the treasury [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[8, 10], [13, 21]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['goldman sachs', 'us secretary of the treasury']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 8. 13. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 9. 20. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(9, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            12 subsent 0\n",
      "examples:                        [CLS] what was dennis hopper ' s bike in easy rider [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[3, 5], [9, 11]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['dennis hopper', 'easy rider']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 3.  9. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 4. 10. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(12, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            15 subsent 0\n",
      "examples:                        [CLS] what teams are in the fa cup final [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[6, 9]]\n",
      "aliases_to_predict:              [0]\n",
      "train_aliases_to_predict_arr:    [0]\n",
      "alias_list_pos:                  [0]\n",
      "aliases:                         ['fa cup final']\n",
      "qids:                            ['Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 6. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 8. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(15, 0, [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Example ***\n",
      "guid:                            18 subsent 0\n",
      "examples:                        [CLS] 1970 world cup semi final italy vs germany [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[1, 4], [6, 7], [8, 9]]\n",
      "aliases_to_predict:              [0, 1, 2]\n",
      "train_aliases_to_predict_arr:    [0, 1, 2]\n",
      "alias_list_pos:                  [0, 1, 2]\n",
      "aliases:                         ['1970 world cup', 'italy', 'germany']\n",
      "qids:                            ['Q-1', 'Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 1.  6.  8. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 3.  6.  8. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -2. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -2. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(18, 0, [ 0,  1,  2, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            21 subsent 0\n",
      "examples:                        [CLS] the pair of hand drums used in indian classical music is called [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[4, 6], [8, 11]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['hand drums', 'indian classical music']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 4.  8. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 5. 10. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(21, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            24 subsent 0\n",
      "examples:                        [CLS] who played smiley in tin ##ker tailor soldier spy [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[3, 4], [5, 10]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['smiley', 'tinker tailor soldier spy']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 3.  5. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 3.  9. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(24, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_output/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 396.16it/s]\n",
      "Processing ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_output/out_1.jsonl:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Example ***\n",
      "guid:                            27 subsent 0\n",
      "examples:                        [CLS] i see the river ti ##ber foam ##ing with much blood [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[4, 7]]\n",
      "aliases_to_predict:              [0]\n",
      "train_aliases_to_predict_arr:    [0]\n",
      "alias_list_pos:                  [0]\n",
      "aliases:                         ['river tiber']\n",
      "qids:                            ['Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 4. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 6. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(27, 0, [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            30 subsent 0\n",
      "examples:                        [CLS] what is the t rex name in land before time [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[4, 6], [10, 11]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['t rex', 'time']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 4. 10. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 5. 10. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(30, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            33 subsent 0\n",
      "examples:                        [CLS] uk national debt as percentage of gdp by year [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[1, 4], [7, 8]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['uk national debt', 'gdp']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 1.  7. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 3.  7. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(33, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            36 subsent 0\n",
      "examples:                        [CLS] when was the first freeway built in los angeles [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[5, 6], [8, 10]]\n",
      "aliases_to_predict:              [0, 1]\n",
      "train_aliases_to_predict_arr:    [0, 1]\n",
      "alias_list_pos:                  [0, 1]\n",
      "aliases:                         ['freeway', 'los angeles']\n",
      "qids:                            ['Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 5.  8. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 5.  9. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(36, 0, [ 0,  1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            39 subsent 0\n",
      "examples:                        [CLS] where is the movie call me by your name filmed [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[5, 10]]\n",
      "aliases_to_predict:              [0]\n",
      "train_aliases_to_predict_arr:    [0]\n",
      "alias_list_pos:                  [0]\n",
      "aliases:                         ['call me by your name']\n",
      "qids:                            ['Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 5. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 9. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(39, 0, [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            43 subsent 0\n",
      "examples:                        [CLS] when was last time england were in a world cup semi final [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[4, 5], [5, 6], [10, 13]]\n",
      "aliases_to_predict:              [0, 1, 2]\n",
      "train_aliases_to_predict_arr:    [0, 1, 2]\n",
      "alias_list_pos:                  [0, 1, 2]\n",
      "aliases:                         ['time', 'england', 'cup semi final']\n",
      "qids:                            ['Q-1', 'Q-1', 'Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 4.  5. 10. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 4.  5. 12. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -2. -2. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -2. -2. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(43, 0, [ 0,  1,  2, -1, -1, -1, -1, -1, -1, -1])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Example ***\n",
      "guid:                            46 subsent 0\n",
      "examples:                        [CLS] who controls the past controls the future rage against the machine [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[8, 12]]\n",
      "aliases_to_predict:              [0]\n",
      "train_aliases_to_predict_arr:    [0]\n",
      "alias_list_pos:                  [0]\n",
      "aliases:                         ['rage against the machine']\n",
      "qids:                            ['Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 8. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [11. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(46, 0, [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n",
      "*** Example ***\n",
      "guid:                            49 subsent 0\n",
      "examples:                        [CLS] cast of characters in fiddle ##r on the roof [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "spans:                           [[5, 10]]\n",
      "aliases_to_predict:              [0]\n",
      "train_aliases_to_predict_arr:    [0]\n",
      "alias_list_pos:                  [0]\n",
      "aliases:                         ['fiddler on the roof']\n",
      "qids:                            ['Q-1']\n",
      "*** Feature ***\n",
      "start_idx_in_sent:               [ 5. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "end_idx_in_sent:                 [ 9. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_cand_K_idx:                 [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "gold_eid:                        [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "for_dump_gold_cand_K_idx_train:  [-2. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "guid:                            [(49, 0, [ 0, -1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ../tutorial_data/data/nq/prep/prep_test_dataset_files/create_examples_output/out_1.jsonl: 100%|██████████| 24/24 [00:00<00:00, 459.03it/s]\n",
      "Checking sentence uniqueness: 100%|██████████| 49/49 [00:00<00:00, 29131.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 11:53:07,423 Loading data from ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_data.bin and ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_label.bin\n",
      "2021-02-12 11:53:07,426 Building type labels from scatch.\n",
      "2021-02-12 11:56:07,890 Creating type prediction labeled data using 2 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading values for marisa trie: 100%|██████████| 5832699/5832699 [00:05<00:00, 1153219.84it/s]\n",
      "Processing types: 100%|██████████| 25/25 [00:02<00:00,  9.96it/s] \n",
      "Processing types: 100%|██████████| 24/24 [00:02<00:00,  9.44it/s]\n",
      "Building type data: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\n",
      "Verifying type labels: 100%|██████████| 49/49 [00:00<00:00, 9301.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 11:56:25,007 Final data initialization time for test is 304.63114166259766s\n",
      "2021-02-12 11:56:25,111 Built dataloader for test set with 49 and 1 threads samples (Shuffle=False, Batch size=32).\n",
      "2021-02-12 11:56:25,184 Building slice dataset for test from ../tutorial_data/data/nq/test_50_bootleg.jsonl.\n",
      "2021-02-12 11:56:25,241 Building dataset from scratch. Saving to ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1\n",
      "2021-02-12 11:56:25,242 Strating to extract examples with 2 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading in ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_input/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 20056.92it/s]\n",
      "Reading in ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_input/out_1.jsonl: 100%|██████████| 25/25 [00:00<00:00, 21312.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 11:56:26,074 Starting to build and save features with 2 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_output/out_1.jsonl: 100%|██████████| 25/25 [00:00<00:00, 2276.84it/s]\n",
      "Processing ../tutorial_data/data/nq/prep/prep_test_slice_files/create_examples_output/out_0.jsonl: 100%|██████████| 25/25 [00:00<00:00, 532.32it/s]\n",
      "Checking sentence uniqueness: 100%|██████████| 50/50 [00:00<00:00, 13252.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 11:56:26,910 Loading data from ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_slices_1f126b5224.bin and ../tutorial_data/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_slices_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sent idx to row idx mapping: 100%|██████████| 50/50 [00:00<00:00, 12310.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 11:56:27,148 Final slice data initialization time from test is 1.9646449089050293s\n",
      "2021-02-12 11:56:27,150 Updating Emmental config from user provided config.\n",
      "2021-02-12 11:56:27,156 Starting Bootleg Model\n",
      "2021-02-12 11:56:27,158 Created emmental model Bootleg that contains task set().\n",
      "2021-02-12 11:56:31,852 Loading embeddings...\n",
      "2021-02-12 12:01:11,119 Created task: NED\n",
      "2021-02-12 12:01:11,123 Moving bert module to CPU.\n",
      "2021-02-12 12:01:11,128 Moving embedding_payload module to CPU.\n",
      "2021-02-12 12:01:11,129 Moving attn_network module to CPU.\n",
      "2021-02-12 12:01:11,132 Moving pred_layer module to CPU.\n",
      "2021-02-12 12:01:11,133 Moving learned module to CPU.\n",
      "2021-02-12 12:01:11,134 Moving title_static module to CPU.\n",
      "2021-02-12 12:01:11,135 Moving learned_type module to CPU.\n",
      "2021-02-12 12:01:11,137 Moving learned_type_wiki module to CPU.\n",
      "2021-02-12 12:01:11,137 Moving learned_type_relations module to CPU.\n",
      "2021-02-12 12:01:11,138 Moving adj_index module to CPU.\n",
      "2021-02-12 12:01:17,756 Created task: Type\n",
      "2021-02-12 12:01:17,758 Moving bert module to CPU.\n",
      "2021-02-12 12:01:17,763 Moving embedding_payload module to CPU.\n",
      "2021-02-12 12:01:17,764 Moving attn_network module to CPU.\n",
      "2021-02-12 12:01:17,767 Moving pred_layer module to CPU.\n",
      "2021-02-12 12:01:17,768 Moving learned module to CPU.\n",
      "2021-02-12 12:01:17,769 Moving title_static module to CPU.\n",
      "2021-02-12 12:01:17,770 Moving learned_type module to CPU.\n",
      "2021-02-12 12:01:17,772 Moving learned_type_wiki module to CPU.\n",
      "2021-02-12 12:01:17,773 Moving learned_type_relations module to CPU.\n",
      "2021-02-12 12:01:17,774 Moving adj_index module to CPU.\n",
      "2021-02-12 12:01:17,774 Moving type_prediction module to CPU.\n",
      "2021-02-12 12:02:42,682 [Bootleg] Model loaded from ../tutorial_data/models/bootleg_uncased/bootleg_wiki.pth\n",
      "2021-02-12 12:02:42,683 Moving bert module to CPU.\n",
      "2021-02-12 12:02:42,691 Moving embedding_payload module to CPU.\n",
      "2021-02-12 12:02:42,693 Moving attn_network module to CPU.\n",
      "2021-02-12 12:02:42,698 Moving pred_layer module to CPU.\n",
      "2021-02-12 12:02:42,699 Moving learned module to CPU.\n",
      "2021-02-12 12:02:42,700 Moving title_static module to CPU.\n",
      "2021-02-12 12:02:42,702 Moving learned_type module to CPU.\n",
      "2021-02-12 12:02:42,703 Moving learned_type_wiki module to CPU.\n",
      "2021-02-12 12:02:42,704 Moving learned_type_relations module to CPU.\n",
      "2021-02-12 12:02:42,706 Moving adj_index module to CPU.\n",
      "2021-02-12 12:02:42,706 Moving type_prediction module to CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Bootleg (test): 100%|██████████| 2/2 [00:09<00:00,  4.82s/it]\n",
      "100%|██████████| 49/49 [00:00<00:00, 10006.86it/s]\n",
      "Reading values for marisa trie: 100%|██████████| 50/50 [00:00<00:00, 297890.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 12:02:53,443 Merging sentences together with 2 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sent_idx, alias_list_pos mapping: 100%|██████████| 89/89 [00:00<00:00, 84207.77it/s]\n",
      "Reading values for marisa trie: 100%|██████████| 89/89 [00:00<00:00, 381690.24it/s]\n",
      "Writing data: 100%|██████████| 25/25 [00:00<00:00, 4436.54it/s]\n",
      "Writing data: 100%|██████████| 25/25 [00:02<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-12 12:04:59,707 Merging output files\n",
      "2021-02-12 12:05:01,509 Wrote predictions to bootleg_logs/wiki_full_ft/2021_02_12/11_46_30/524a6d16/test_50_bootleg/bootleg_wiki/bootleg_labels.jsonl\n"
     ]
    }
   ],
   "source": [
    "bootleg_label_file, _ = run_model(mode=\"dump_preds\", config=config_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the overall quality of the end-to-end pipeline via precision / recall metrics, where the *recall* indicates what proportion of the hand-labelled mentions Bootleg correctly detects and disambiguates, and *precision* indicates what proportion of the mentions that Bootleg labels are correct. For instance, if Bootleg only labelled the few mentions it was very confident in, then it would have a low recall and high precision.\n",
    "\n",
    "To detect if mentions match the hand-labelled mention spans, we report weak and exact match metrics. Weak means the predicted and gold span boundaries just need to overlap for an entity (e.g., predicted mention 'the wizard of oz' is counted as correct for the gold mention 'wizard of oz' if the correct entity is predicted). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAK MATCHING\n",
      "precision = 61 / 80 = 0.7625\n",
      "recall = 61 / 78 = 0.782051282051282\n",
      "f1 = 0.7721518987341772\n",
      "\n",
      "EXACT MATCHING\n",
      "precision = 61 / 80 = 0.7625\n",
      "recall = 61 / 78 = 0.782051282051282\n",
      "f1 = 0.7721518987341772\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_metrics\n",
    "bootleg_end2end_errors = compute_metrics(gold_file=nq_sample_orig,       \n",
    "                                 pred_file=bootleg_label_file, \n",
    "                                 threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine errors in the end-to-end pipeline below. As you increase the threshold in the `compute_metrics` command, entities with a prediction probability less than the threshold will be filtered out. If too few entities are predicted, lowering the threshold may help.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>gold_aliases</th>\n",
       "      <th>gold_qids</th>\n",
       "      <th>gold_spans</th>\n",
       "      <th>pred_aliases</th>\n",
       "      <th>pred_qids</th>\n",
       "      <th>pred_spans</th>\n",
       "      <th>pred_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>I Love It ( feat . Charli XCX ) Icona Pop</td>\n",
       "      <td>[i love it, charli xcx, icona pop]</td>\n",
       "      <td>[Q3273659, Q5084390, Q808703]</td>\n",
       "      <td>[[0, 3], [6, 8], [9, 11]]</td>\n",
       "      <td>[charli xcx, icona pop]</td>\n",
       "      <td>[Q5084390, Q808703]</td>\n",
       "      <td>[[5, 9], [9, 11]]</td>\n",
       "      <td>[1.0, 0.9873091578]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>Who proposed the coordinate system to describe the position of a point in a plane accurately</td>\n",
       "      <td>[coordinate system]</td>\n",
       "      <td>[Q62912]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "      <td>[coordinate system]</td>\n",
       "      <td>[Q11210]</td>\n",
       "      <td>[[3, 5]]</td>\n",
       "      <td>[1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>Landmark Supreme Court cases dealing with the First Amendment</td>\n",
       "      <td>[supreme court, first amendment]</td>\n",
       "      <td>[Q11201, Q12616]</td>\n",
       "      <td>[[1, 3], [7, 9]]</td>\n",
       "      <td>[supreme court cases, first amendment]</td>\n",
       "      <td>[Q6646863, Q12616]</td>\n",
       "      <td>[[1, 4], [7, 9]]</td>\n",
       "      <td>[1.0, 0.9999812841]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>Once Upon a Time Season 6 episode list</td>\n",
       "      <td>[once upon a time season 6]</td>\n",
       "      <td>[Q23301616]</td>\n",
       "      <td>[[0, 6]]</td>\n",
       "      <td>[season 6, episode list]</td>\n",
       "      <td>[Q2404330]</td>\n",
       "      <td>[[4, 6]]</td>\n",
       "      <td>[0.5048642159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>13</td>\n",
       "      <td>Where was 10 Things I Hate About You filmed school</td>\n",
       "      <td>[10 things i hate about you]</td>\n",
       "      <td>[Q169082]</td>\n",
       "      <td>[[2, 8]]</td>\n",
       "      <td>[10 things i hate about you]</td>\n",
       "      <td>[Q169074]</td>\n",
       "      <td>[[2, 8]]</td>\n",
       "      <td>[0.5601187348]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>Who does Oregon state play in the College World Series</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[Q7101349, Q787505]</td>\n",
       "      <td>[[2, 4], [7, 10]]</td>\n",
       "      <td>[oregon state, college world series]</td>\n",
       "      <td>[Q787505]</td>\n",
       "      <td>[[7, 10]]</td>\n",
       "      <td>[0.8851752281]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>Why does the author say that the vampire in Nosferatu is named Count Orlok and not Count Dracula</td>\n",
       "      <td>[nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[Q151895, Q1442062, Q3266236]</td>\n",
       "      <td>[[9, 10], [12, 14], [16, 18]]</td>\n",
       "      <td>[vampire, nosferatu, count orlok, count dracula]</td>\n",
       "      <td>[Q7912955, Q151895, Q1442062, Q3266236]</td>\n",
       "      <td>[[7, 8], [9, 10], [12, 14], [16, 18]]</td>\n",
       "      <td>[0.7404002547, 0.9794661999, 1.0, 0.9972344041]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>Where does the last name Vigil come from</td>\n",
       "      <td>[vigil]</td>\n",
       "      <td>[Q16878937]</td>\n",
       "      <td>[[5, 6]]</td>\n",
       "      <td>[vigil]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>Reasons why South Africa should include renewable energy in its energy mix</td>\n",
       "      <td>[south africa, renewable energy]</td>\n",
       "      <td>[Q258, Q12705]</td>\n",
       "      <td>[[2, 4], [6, 8]]</td>\n",
       "      <td>[reasons why, south africa, renewable energy, energy mix]</td>\n",
       "      <td>[Q7028249, Q258, Q12705, Q1341346]</td>\n",
       "      <td>[[0, 2], [2, 4], [6, 8], [10, 12]]</td>\n",
       "      <td>[1.0, 0.999255836, 0.8151838183, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>48</td>\n",
       "      <td>What was the Japanese motivation for bombing Pearl Harbor</td>\n",
       "      <td>[japanese, pearl harbor]</td>\n",
       "      <td>[Q188712, Q127091]</td>\n",
       "      <td>[[3, 4], [7, 9]]</td>\n",
       "      <td>[motivation, pearl harbor]</td>\n",
       "      <td>[Q644302, Q127091]</td>\n",
       "      <td>[[4, 5], [7, 9]]</td>\n",
       "      <td>[0.9905021191, 0.757401526]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_idx  \\\n",
       "14         4   \n",
       "7         42   \n",
       "5         37   \n",
       "18         8   \n",
       "22        13   \n",
       "1         31   \n",
       "16         6   \n",
       "6         40   \n",
       "3         35   \n",
       "10        48   \n",
       "\n",
       "                                                                                                text  \\\n",
       "14                                                         I Love It ( feat . Charli XCX ) Icona Pop   \n",
       "7       Who proposed the coordinate system to describe the position of a point in a plane accurately   \n",
       "5                                      Landmark Supreme Court cases dealing with the First Amendment   \n",
       "18                                                            Once Upon a Time Season 6 episode list   \n",
       "22                                                Where was 10 Things I Hate About You filmed school   \n",
       "1                                             Who does Oregon state play in the College World Series   \n",
       "16  Why does the author say that the vampire in Nosferatu is named Count Orlok and not Count Dracula   \n",
       "6                                                           Where does the last name Vigil come from   \n",
       "3                         Reasons why South Africa should include renewable energy in its energy mix   \n",
       "10                                         What was the Japanese motivation for bombing Pearl Harbor   \n",
       "\n",
       "                               gold_aliases                      gold_qids  \\\n",
       "14       [i love it, charli xcx, icona pop]  [Q3273659, Q5084390, Q808703]   \n",
       "7                       [coordinate system]                       [Q62912]   \n",
       "5          [supreme court, first amendment]               [Q11201, Q12616]   \n",
       "18              [once upon a time season 6]                    [Q23301616]   \n",
       "22             [10 things i hate about you]                      [Q169082]   \n",
       "1      [oregon state, college world series]            [Q7101349, Q787505]   \n",
       "16  [nosferatu, count orlok, count dracula]  [Q151895, Q1442062, Q3266236]   \n",
       "6                                   [vigil]                    [Q16878937]   \n",
       "3          [south africa, renewable energy]                 [Q258, Q12705]   \n",
       "10                 [japanese, pearl harbor]             [Q188712, Q127091]   \n",
       "\n",
       "                       gold_spans  \\\n",
       "14      [[0, 3], [6, 8], [9, 11]]   \n",
       "7                        [[3, 5]]   \n",
       "5                [[1, 3], [7, 9]]   \n",
       "18                       [[0, 6]]   \n",
       "22                       [[2, 8]]   \n",
       "1               [[2, 4], [7, 10]]   \n",
       "16  [[9, 10], [12, 14], [16, 18]]   \n",
       "6                        [[5, 6]]   \n",
       "3                [[2, 4], [6, 8]]   \n",
       "10               [[3, 4], [7, 9]]   \n",
       "\n",
       "                                                 pred_aliases  \\\n",
       "14                                    [charli xcx, icona pop]   \n",
       "7                                         [coordinate system]   \n",
       "5                      [supreme court cases, first amendment]   \n",
       "18                                   [season 6, episode list]   \n",
       "22                               [10 things i hate about you]   \n",
       "1                        [oregon state, college world series]   \n",
       "16           [vampire, nosferatu, count orlok, count dracula]   \n",
       "6                                                     [vigil]   \n",
       "3   [reasons why, south africa, renewable energy, energy mix]   \n",
       "10                                 [motivation, pearl harbor]   \n",
       "\n",
       "                                  pred_qids  \\\n",
       "14                      [Q5084390, Q808703]   \n",
       "7                                  [Q11210]   \n",
       "5                        [Q6646863, Q12616]   \n",
       "18                               [Q2404330]   \n",
       "22                                [Q169074]   \n",
       "1                                 [Q787505]   \n",
       "16  [Q7912955, Q151895, Q1442062, Q3266236]   \n",
       "6                                        []   \n",
       "3        [Q7028249, Q258, Q12705, Q1341346]   \n",
       "10                       [Q644302, Q127091]   \n",
       "\n",
       "                               pred_spans  \\\n",
       "14                      [[5, 9], [9, 11]]   \n",
       "7                                [[3, 5]]   \n",
       "5                        [[1, 4], [7, 9]]   \n",
       "18                               [[4, 6]]   \n",
       "22                               [[2, 8]]   \n",
       "1                               [[7, 10]]   \n",
       "16  [[7, 8], [9, 10], [12, 14], [16, 18]]   \n",
       "6                                      []   \n",
       "3      [[0, 2], [2, 4], [6, 8], [10, 12]]   \n",
       "10                       [[4, 5], [7, 9]]   \n",
       "\n",
       "                                         pred_probs  \n",
       "14                              [1.0, 0.9873091578]  \n",
       "7                                             [1.0]  \n",
       "5                               [1.0, 0.9999812841]  \n",
       "18                                   [0.5048642159]  \n",
       "22                                   [0.5601187348]  \n",
       "1                                    [0.8851752281]  \n",
       "16  [0.7404002547, 0.9794661999, 1.0, 0.9972344041]  \n",
       "6                                                []  \n",
       "3             [1.0, 0.999255836, 0.8151838183, 1.0]  \n",
       "10                      [0.9905021191, 0.757401526]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bootleg_end2end_errors).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the errors Bootleg makes is predicting too general of a candidate (e.g. Oregon State Beavers instead of Oregon State Beavers baseball). Other errors are due to ambiguous sentences (e.g. \"cast of characters in fiddler on the roof\" -> should this be the movie or the musical?). Finally another bucket of errors suggests that we need to boost certain training signals -- this is an area we're actively pursuing in Bootleg with an investigation of model guidability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare to TAGME \n",
    "\n",
    "To get a sense of how Bootleg is doing compared to other systems, we evaluate [TAGME](https://arxiv.org/pdf/1006.3498.pdf), an existing tool to extract and disambiguate mentions. To run TAGME, you need to get a (free) authorization token. Instructions for obtaining a token are [here](https://sobigdata.d4science.org/web/tagme/tagme-help). You will need to verify your account and then follow the \"access the VRE\") link. We've also provided the file with TAGME labels for a given threshold for download if you want to skip the authorization token.\n",
    "\n",
    "We note that unlike TAGME, Bootleg also outputs contextual entity embeddings which can be loaded for use in downstream tasks (e.g. relation extraction, question answering). Check out the Entity Embedding tutorial for more details! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tagme\n",
    "# Set the authorization token for subsequent calls.\n",
    "tagme.GCUBE_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagme_label_file = root_dir / '/data/nq/test_50_tagme.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a token, skip the cell below and load the pre-generated TAGME labels. If you do have a token, you can play with changing the threshold below and see how it affects the results. Increasing the threshold increases the precision but decreases the recall as TAGME, as TAGME will label fewer mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No wikidata id found for Frosty the Snowman (film)\n",
      "No wikidata id found for The Bachelor (U.S. TV series)\n",
      "No wikidata id found for House of Cards (U.S. TV series)\n"
     ]
    }
   ],
   "source": [
    "from utils import tagme_annotate\n",
    "# As the threshold increases, the precision increases, but the recall decreases\n",
    "tagme_annotate(in_file=nq_sample_orig, out_file=tagme_label_file, threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not set the threshold here when computing metrics for TAGME as TAGME predictions are already thresholded in the `tagme_annotate` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEAK MATCHING\n",
      "precision = 53 / 101 = 0.5247524752475248\n",
      "recall = 53 / 78 = 0.6794871794871795\n",
      "f1 = 0.5921787709497207\n",
      "\n",
      "EXACT MATCHING\n",
      "precision = 52 / 101 = 0.5148514851485149\n",
      "recall = 52 / 78 = 0.6666666666666666\n",
      "f1 = 0.5810055865921788\n"
     ]
    }
   ],
   "source": [
    "tagme_errors = compute_metrics(gold_file=nq_sample_orig, pred_file=tagme_label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several Wikidata ids are not recovered due to out of date Wikipedia titles in TAGME predictions. Even when including these three samples as correct predictions, we see that Bootleg is able to outperform TAGME in both precision and recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}