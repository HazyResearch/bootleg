{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ujson, os\n",
    "from tqdm.auto import tqdm\n",
    "from bootleg.symbols.entity_profile import EntityProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Profile Tutorial\n",
    "\n",
    "In this tutorial, we will show you how to modify and interact with our entity metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- Pretrained Bootleg uncased model and config [here](https://bootleg-ned-data.s3-us-west-1.amazonaws.com/models/lateset/bootleg_uncased.tar.gz).\n",
    "- Entity data [here](https://bootleg-ned-data.s3-us-west-1.amazonaws.com/data/lateset/entity_db.tar.gz)\n",
    "\n",
    "For convenience, you can run the commands below (from the root directory of the repo) to download all the above files and unpack them to `models` and `data` directories. It will take several minutes to download all the files.\n",
    "\n",
    "```\n",
    "    bash tutorials/download_model.sh uncased\n",
    "    bash tutorials/download_data.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up the entity profile\n",
    "Inside the cache directory is\n",
    "* entity_mappings: where aliases and entity information is stored. We also have the original unfiltered alias to candidate mapping we used for training on Wikipedia. For all other uses, we use the alias to candidate map called `alias2qids.json`, with higher quality aliases.\n",
    "* type_mappings: where type information is stored. There will be one subfolder per type system. In the `wiki` subfolder, we have a mapping from Wikidata title to Wikidata QID for the types. The `relations` subfolder is where we keep our relationship types and treat them as types in our model.\n",
    "* kg_mappings: where kg information is stored\n",
    "\n",
    "When we load a entity profile, we can put it in `edit_mode` to allow us to make changes. Don't forget to set that flag below to edit.\n",
    "\n",
    "See our read the docs [here](https://bootleg.readthedocs.io/en/latest/gettingstarted/entity_profile.html) for more information on our entity profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kg_mappings\n",
      "    qid2relations.json\n",
      "    kg_adj.txt\n",
      "    relation_vocab.json\n",
      "    config.json\n",
      "type_mappings\n",
      "    hyena_coarse\n",
      "        qid2typenames.json\n",
      "        type_vocab.json\n",
      "        qid2typeids.json\n",
      "        config.json\n",
      "    hyena\n",
      "        qid2typenames.json\n",
      "        qid2typeids.json\n",
      "        config.json\n",
      "        type_vocab.json\n",
      "    wiki\n",
      "        qid2typeids.json\n",
      "        type_vocab_to_wikidataqid.json\n",
      "        type_vocab.json\n",
      "        qid2typenames.json\n",
      "        config.json\n",
      "    relations\n",
      "        config.json\n",
      "        qid2typeids.json\n",
      "        qid2typenames.json\n",
      "        type_vocab.json\n",
      "entity_mappings\n",
      "    qid2title.json\n",
      "    config.json\n",
      "    alias2id.json\n",
      "    alias2qids.json\n",
      "    alias2id_unfiltered.json\n",
      "    qid2cnt.json\n",
      "    qid2eid.json\n",
      "    alias2qids_unfiltered.json\n",
      "    qid2desc.json\n"
     ]
    }
   ],
   "source": [
    "# MODIFY THE PATH TO THE DOWNLOADED ENTITY_DB DATA.\n",
    "entity_profile_cache = Path(\"../data/entity_db\")\n",
    "# Print out directory structure\n",
    "for fold in entity_profile_cache.iterdir():\n",
    "    # Skip showing our prep directory as that's used when loading a model\n",
    "    if fold.name in [\"prep\"]:\n",
    "        continue\n",
    "    print(fold.name)\n",
    "    for sub_file in fold.iterdir():\n",
    "        print(\"   \", sub_file.name)\n",
    "        if sub_file.is_dir():\n",
    "            for subsub_file in sub_file.iterdir():\n",
    "                print(\"       \", subsub_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `load_from_cache` to load in a profile. If you only want to modify or edit only type information or only kg information, we provide flags to turn off loading some data. In particular, the `no_kg` flag turns off KG information, `no_type` flag turns off type information, and `type_systems_to_load` will specify which types system subfolders to load (`None` means load all).\n",
    "\n",
    "**Note** that if you do not load up a subset of metadata, you cannot add, remove, or otherwise examine that data. If you set `no_kg = True`, for example, you can't add a new KG connection. This also means if you call `save`, that metadata will not be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Entity Symbols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building edit mode objs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15202497/15202497 [01:03<00:00, 237598.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Type Symbols from ../data/entity_db/type_mappings/hyena_coarse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building edit mode objs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5832699/5832699 [00:03<00:00, 1514773.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Type Symbols from ../data/entity_db/type_mappings/hyena\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building edit mode objs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5832699/5832699 [00:20<00:00, 283105.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Type Symbols from ../data/entity_db/type_mappings/wiki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building edit mode objs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5832699/5832699 [00:09<00:00, 590063.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Type Symbols from ../data/entity_db/type_mappings/relations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building edit mode objs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5832699/5832699 [00:21<00:00, 266170.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KG Symbols\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking relations and building edit mode objs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5832699/5832699 [02:19<00:00, 41914.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded full ep in 682.7707569599152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "# Load up ALL profile data - don't forget to set edit_mode = True\n",
    "# As edit_mode triggers the profile to build some index structures for fast editing,\n",
    "# the loading takes a few minutes for all of wiki\n",
    "ep = EntityProfile.load_from_cache(entity_profile_cache, edit_mode=True, verbose=True)\n",
    "print(f\"Loaded full ep in {time.time() - st}\")\n",
    "st = time.time()\n",
    "\n",
    "# # Load up NO KG information\n",
    "# ep = EntityProfile.load_from_cache(\n",
    "#     entity_profile_cache, edit_mode=True, verbose=True, no_kg=True\n",
    "# )\n",
    "# print(f\"Loaded full ep without KG in {time.time() - st}\")\n",
    "# st = time.time()\n",
    "\n",
    "# # Load up NO TYPE information\n",
    "# ep = EntityProfile.load_from_cache(\n",
    "#     entity_profile_cache, edit_mode=True, verbose=True, no_type=True\n",
    "# )\n",
    "# print(f\"Loaded full ep without type in {time.time() - st}\")\n",
    "# st = time.time()\n",
    "\n",
    "# # Load up only wiki type information\n",
    "# ep = EntityProfile.load_from_cache(\n",
    "#     entity_profile_cache,\n",
    "#     edit_mode=True,\n",
    "#     verbose=True,\n",
    "#     no_kg=True,\n",
    "#     type_systems_to_load=[\"wiki\"],\n",
    "# )\n",
    "# print(f\"Loaded full ep without KG and only wikidata type in {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what operations you can call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dir__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_read_profile_file', 'add_entity', 'add_mention', 'add_relation', 'add_type', 'get_all_connections', 'get_all_mentions', 'get_all_qids', 'get_all_types', 'get_all_typesystems', 'get_connections_by_relation', 'get_desc', 'get_eid', 'get_entities_of_type', 'get_mentions', 'get_mentions_with_scores', 'get_qid_cands', 'get_qid_count_cands', 'get_title', 'get_types', 'is_connected', 'load_from_cache', 'load_from_jsonl', 'mention_exists', 'prune_to_entities', 'qid_exists', 'reidentify_entity', 'remove_mention', 'remove_relation', 'remove_type', 'save', 'save_to_jsonl', 'update_entity']\n"
     ]
    }
   ],
   "source": [
    "object_methods = [\n",
    "    method_name for method_name in dir(ep) if callable(getattr(ep, method_name))\n",
    "]\n",
    "\n",
    "print(object_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Apple TV+\n",
      "Description: Apple TV+ is an ad-free subscription video on demand web television service of Apple Inc that debuted on November 1 , 2019 .\n",
      "Mentions: {'apple', 'appletv', 'apple tv', 'apple worldwide video', 'apple tv plus'}\n",
      "Type Systems: ['hyena_coarse', 'hyena', 'wiki', 'relations']\n",
      "Sample Wikidata Types: ['town in China', 'tehsil of India', 'subdistrict of China', 'faculty', 'pier']\n"
     ]
    }
   ],
   "source": [
    "# Get the title of an entity\n",
    "print(\"Title:\", ep.get_title(\"Q62446736\"))\n",
    "\n",
    "# Get the description of an entity\n",
    "print(\"Description:\", ep.get_desc(\"Q62446736\"))\n",
    "\n",
    "# Get mentions for an entity\n",
    "print(\"Mentions:\", ep.get_mentions(\"Q62446736\"))\n",
    "\n",
    "# Get type systems\n",
    "print(\"Type Systems:\", ep.get_all_typesystems())\n",
    "\n",
    "# Get some types\n",
    "print(\"Sample Wikidata Types:\", ep.get_all_types(\"wiki\")[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the types\n",
    "\n",
    "Suppose you think the QID Q62446736 should really be a computer type instead of a tv type. First we need to see what types the QID is and find a possible replacement type. Then we need to actually remove and add the types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Types: ['video streaming service']\n",
      "73\n",
      "['computer program', 'minicomputer', 'computer network', 'computer model', 'tablet computer', 'computer network protocol', 'computer model series', 'supercomputer', 'computer scientist', '3D computer graphics software', 'computer', 'vector supercomputer', 'computer system', 'computer form factor', 'personal computer', 'computer-aided engineering', 'computer memory', 'home computer', 'computer language', 'computer science term', 'computer monitor', 'microcomputer', 'first generation computer', 'decimal computer', 'computer key', 'computer programming', 'computer surveillance', 'portable computer', 'computer science', 'computer file', 'one-of-a-kind computer', 'computer architecture', 'computer file management', 'computer-aided design software', 'computer security software', 'computer hardware', 'single-board computer', 'computer-animated film', 'computer data storage', 'desktop computer', 'computer worm', 'computer magazine', 'computer algebra system', 'mainframe computer', 'computer graphics term', 'supercomputer operating system', 'computer museum', 'fictional computer', 'computer security', 'computer vulnerability', 'computer appliance', 'computer numbering format', 'chess computer', 'server computer', 'analog computer', 'computer graphics', 'external computer connector', 'Chrome OS computer', 'computer keyboard', 'computer display standard', 'computer terminal', 'mobile computer', 'computer data processing', 'computer software term', 'electro-mechanical computer', 'Computer security conference', 'computer shop', 'computer expo', 'computer reservations system', 'computer graphics_Q7600677', '3D computer graphics', 'computer-generated imagery', 'computer journalism']\n"
     ]
    }
   ],
   "source": [
    "# First get existing types\n",
    "qid = \"Q62446736\"\n",
    "type_system = \"wiki\"\n",
    "print(\"Existing Types:\", ep.get_types(qid, type_system))\n",
    "\n",
    "# Get all possible types with the word computer in it\n",
    "all_types = ep.get_all_types(type_system)\n",
    "\n",
    "comp_types = [t for t in all_types if \"computer\" in t.lower()]\n",
    "print(len(comp_types))\n",
    "print(comp_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Types: ['computer']\n"
     ]
    }
   ],
   "source": [
    "# Remove type\n",
    "ep.remove_type(qid, \"video streaming service\", type_system)\n",
    "# Add type\n",
    "ep.add_type(qid, \"computer\", type_system)\n",
    "\n",
    "print(\"Modified Types:\", ep.get_types(qid, type_system))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you think Q178194 should not have the relation P910 with Q8439242 anymore. Don't worry if you misspecify the relation pair. If the pair doesn't exist, we do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Connections: {'P31': ['Q59152282'], 'P137': ['Q312'], 'P127': ['Q312'], 'P17': ['Q30'], 'P407': ['Q1860'], 'P452': ['Q723685'], 'P749': ['Q312'], 'P1454': ['Q891723'], 'P910': ['Q49225405'], 'P1889': ['Q270285']}\n",
      "Modified Connections: {'P137': ['Q312'], 'P127': ['Q312'], 'P17': ['Q30'], 'P407': ['Q1860'], 'P452': ['Q723685'], 'P749': ['Q312'], 'P1454': ['Q891723'], 'P910': ['Q49225405'], 'P1889': ['Q270285']}\n"
     ]
    }
   ],
   "source": [
    "qid = \"Q62446736\"\n",
    "print(\"Existing Connections:\", ep.get_relation_between(qid))\n",
    "\n",
    "# Remove relation\n",
    "ep.remove_relation(qid, \"P31\", \"Q59152282\")\n",
    "\n",
    "print(\"Modified Connections:\", ep.get_relation_between(qid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a new entity\n",
    "\n",
    "To add a new entity, we need to provide the following json object to our entity profile\n",
    "```\n",
    "{\n",
    "    \"entity_id\": \"C000\",\n",
    "    \"mentions\": [[\"dog\", 10.0], [\"dogg\", 7.0], [\"animal\", 4.0]],\n",
    "    \"title\": \"Dog\",\n",
    "    \"description\": \"An animal that barks\",\n",
    "    \"types\": {\"hyena\": [\"animal\"], \"wiki\": [\"dog\"]},\n",
    "    \"relations\": [\n",
    "        {\"relation\": \"sibling\", \"object\": \"Q345\"},\n",
    "        {\"relation\": \"sibling\", \"object\": \"Q567\"},\n",
    "    ],\n",
    "}\n",
    "```\n",
    "\n",
    "The numeric values for the mentions represent the score of that mention. These can all be the same value. They are just used for sorting the mentions for an entity.\n",
    "\n",
    "If you do not have mentions, a good default is the title of the mention with a score of 1.\n",
    "\n",
    "**NOTE** We will lower case mentions and strip certain punctuation for mention extraction when adding mentions to the entity profile. See ``bootleg.utils.utils.get_lnrm`` for more info (we set strip and lower to be True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Some New Entity\"\n",
    "# The numeric value is the score associated with the mention\n",
    "mentions = [[\"computer\", 10.0], [\"sparkle device\", 12.0]]\n",
    "wiki_types = [\"computer\"]\n",
    "d = {\n",
    "    \"entity_id\": \"NQ1\",\n",
    "    \"mentions\": mentions,\n",
    "    \"title\": title,\n",
    "    \"description\": \"A computer that performs\",\n",
    "    \"types\": {\"wiki\": wiki_types},\n",
    "}\n",
    "if not ep.qid_exists(\"NQ1\"):\n",
    "    ep.add_entity(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unused entities\n",
    "\n",
    "Lastly, for space reasons, it'd be nice to remove the QIDs that are no longer needed in this dump. For that, we can call `prune_to_entities`. This operation will remove all entities not in the set of entities given. In will throw an error, however, if you ask it to remove an entity that doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1523/1523 [00:00<00:00, 318044.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get entities to keep based on those that have the types in `types_to_add`\n",
    "type_system = \"wiki\"\n",
    "types_to_add = [\n",
    "    \"computer\",\n",
    "    \"fruit\",\n",
    "    \"meat\",\n",
    "    \"country\",\n",
    "    \"national association football team\",\n",
    "]\n",
    "entities_of_type = set()\n",
    "for ty in types_to_add:\n",
    "    entities_of_type.update(set(ep.get_entities_of_type(ty, type_system)))\n",
    "\n",
    "# Make sure they are all in the dump\n",
    "for qid in tqdm(entities_of_type):\n",
    "    if not ep.qid_exists(qid):\n",
    "        print(f\"{qid} does not exists\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting number of entities: 5832700\n",
      "Pruning entity data\n",
      "Pruning hyena_coarse data\n",
      "Pruning hyena data\n",
      "Pruning wiki data\n",
      "Pruning relations data\n",
      "Pruning kg data\n",
      "Ending number of entities: 1523\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting number of entities: {len(ep.get_all_qids())}\")\n",
    "ep.prune_to_entities(entities_of_type)\n",
    "print(f\"Ending number of entities: {len(ep.get_all_qids())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new profile\n",
    "ep.save(entity_profile_cache.parent / \"new_profile_wiki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Model and Run\n",
    "\n",
    "A benefit of Bootleg is that you can easily add and remove entities and do not need to change the model in any way to accommodate them. If you are using a modified entity profile, all you need to do is adjust some paths in the model config to point to this new location. That's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up model path. This path should be the `models` subfolder.\n",
    "model_dir = Path(\"../models\")\n",
    "\n",
    "# Base model config to modify\n",
    "old_config_path = str(model_dir / \"bootleg_uncased/bootleg_config.yaml\")\n",
    "# Provide save path for the new bootleg config yaml file. This can be anywhere.\n",
    "new_config_save_path = \"np_bootleg_config.yaml\"\n",
    "# Base model pth path to modify\n",
    "model_path = str(model_dir / \"bootleg_uncased/bootleg_wiki.pth\")\n",
    "# Path where you saved the adjusted entity profile above\n",
    "new_entity_path = str(entity_profile_cache.parent / \"new_profile_wiki\")\n",
    "# Path where you want logs to be saved\n",
    "new_log_path = str(\"bootleg-logs/new-bootleg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped config to np_bootleg_config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def modify_config(\n",
    "    old_config_path, new_config_path, model_save_path, new_entity_path, new_log_path\n",
    "):\n",
    "    \"\"\"Modifies the old config with the new profile and model for running.\n",
    "\n",
    "    Args:\n",
    "        old_config_path: old config path\n",
    "        new_config_path: new config path\n",
    "        model_save_path: model save path\n",
    "        new_entity_path: new entity path\n",
    "        new_log_path: new log path\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    with open(old_config_path) as file:\n",
    "        old_config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    if \"emmental\" not in old_config:\n",
    "        old_config[\"emmental\"] = {}\n",
    "    old_config[\"emmental\"][\"model_path\"] = model_save_path\n",
    "    old_config[\"emmental\"][\"log_path\"] = new_log_path\n",
    "    old_config[\"data_config\"][\"entity_dir\"] = new_entity_path\n",
    "\n",
    "    with open(new_config_path, \"w\") as file:\n",
    "        yaml.dump(old_config, file)\n",
    "    print(f\"Dumped config to {new_config_path}\")\n",
    "\n",
    "\n",
    "modify_config(\n",
    "    old_config_path, new_config_save_path, model_path, new_entity_path, new_log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model\n",
    "Before running the annotator, we need to load and sanity check the config. We pass this into the annotator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data_config\": {\n",
      "        \"context_mask_perc\": 0.0,\n",
      "        \"data_dir\": \"\\/home\\/data\\/bootleg-data\\/wiki_title_0122\",\n",
      "        \"data_prep_dir\": \"prep\",\n",
      "        \"dev_dataset\": {\n",
      "            \"file\": \"merged_sample.jsonl\",\n",
      "            \"use_weak_label\": true\n",
      "        },\n",
      "        \"entity_dir\": \"..\\/data\\/new_profile_wiki\",\n",
      "        \"entity_kg_data\": {\n",
      "            \"kg_labels\": \"kg_mappings\\/qid2relations.json\",\n",
      "            \"kg_vocab\": \"kg_mappings\\/relation_vocab.json\",\n",
      "            \"use_entity_kg\": true\n",
      "        },\n",
      "        \"entity_type_data\": {\n",
      "            \"type_labels\": \"type_mappings\\/wiki\\/qid2typeids.json\",\n",
      "            \"type_vocab\": \"type_mappings\\/wiki\\/type_vocab.json\",\n",
      "            \"use_entity_types\": true\n",
      "        },\n",
      "        \"eval_slices\": [\n",
      "            \"unif_all\",\n",
      "            \"unif_NS_all\",\n",
      "            \"unif_HD\",\n",
      "            \"unif_TO\",\n",
      "            \"unif_TL\",\n",
      "            \"unif_TS\"\n",
      "        ],\n",
      "        \"max_ent_len\": 128,\n",
      "        \"max_seq_len\": 128,\n",
      "        \"max_seq_window_len\": 64,\n",
      "        \"overwrite_preprocessed_data\": false,\n",
      "        \"test_dataset\": {\n",
      "            \"file\": \"merged_sample.jsonl\",\n",
      "            \"use_weak_label\": true\n",
      "        },\n",
      "        \"train_dataset\": {\n",
      "            \"file\": \"train.jsonl\",\n",
      "            \"use_weak_label\": true\n",
      "        },\n",
      "        \"train_in_candidates\": true,\n",
      "        \"use_entity_desc\": true,\n",
      "        \"word_embedding\": {\n",
      "            \"bert_model\": \"bert-base-uncased\",\n",
      "            \"cache_dir\": \"bootleg-data\\/pretrained_bert_models\",\n",
      "            \"context_layers\": 6,\n",
      "            \"entity_layers\": 6\n",
      "        }\n",
      "    },\n",
      "    \"emmental\": {\n",
      "        \"checkpoint_all\": true,\n",
      "        \"checkpoint_freq\": 1,\n",
      "        \"checkpoint_metric\": \"NED\\/Bootleg\\/dev\\/final_loss\\/acc_boot:max\",\n",
      "        \"checkpointing\": true,\n",
      "        \"clear_intermediate_checkpoints\": false,\n",
      "        \"counter_unit\": \"batch\",\n",
      "        \"dataparallel\": false,\n",
      "        \"evaluation_freq\": 21432,\n",
      "        \"fp16\": true,\n",
      "        \"grad_clip\": 1.0,\n",
      "        \"gradient_accumulation_steps\": 1,\n",
      "        \"l2\": 0.01,\n",
      "        \"log_path\": \"bootleg-logs\\/new-bootleg\",\n",
      "        \"lr\": \"2e-5\",\n",
      "        \"lr_scheduler\": \"linear\",\n",
      "        \"model_path\": \"..\\/models\\/bootleg_uncased\\/bootleg_wiki.pth\",\n",
      "        \"n_steps\": 428648,\n",
      "        \"online_eval\": false,\n",
      "        \"use_exact_log_path\": true,\n",
      "        \"warmup_percentage\": 0.1,\n",
      "        \"write_loss_per_step\": true,\n",
      "        \"writer\": \"json\"\n",
      "    },\n",
      "    \"model_config\": {\n",
      "        \"hidden_size\": 200,\n",
      "        \"normalize\": true,\n",
      "        \"temperature\": 0.01\n",
      "    },\n",
      "    \"run_config\": {\n",
      "        \"dataloader_threads\": 2,\n",
      "        \"dataset_threads\": 20,\n",
      "        \"eval_batch_size\": 32,\n",
      "        \"log_level\": \"DEBUG\",\n",
      "        \"spawn_method\": \"forkserver\"\n",
      "    },\n",
      "    \"train_config\": {\n",
      "        \"batch_size\": 32\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load and sanity check config\n",
    "\n",
    "# !!! Set this to what config you want to use\n",
    "config_to_load = new_config_save_path\n",
    "\n",
    "# Load config\n",
    "with open(config_to_load) as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "print(ujson.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/local/0/senwu/.pyenv/versions/3.8.6/envs/venv38/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n",
      "[2021-10-15 20:00:17,218][INFO] emmental.meta:122 - Setting logging directory to: bootleg-logs/new-bootleg\n",
      "[2021-10-15 20:00:17,261][INFO] emmental.meta:64 - Loading Emmental default config from /lfs/raiders3/0/senwu/.pyenv/versions/3.8.6/envs/venv38/lib/python3.8/site-packages/emmental/emmental-default-config.yaml.\n",
      "[2021-10-15 20:00:17,262][INFO] emmental.meta:174 - Updating Emmental config from user provided config.\n",
      "[2021-10-15 20:00:17,263][INFO] emmental.utils.seed:27 - Set random seed to 1234.\n",
      "[2021-10-15 20:00:20,062][INFO] emmental.model:72 - Created emmental model Bootleg that contains task set().\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2021-10-15 20:00:29,370][INFO] emmental.task:78 - Created task: NED\n",
      "[2021-10-15 20:00:29,372][INFO] emmental.model:108 - Moving context_encoder module to CPU.\n",
      "[2021-10-15 20:00:29,376][INFO] emmental.model:108 - Moving entity_encoder module to CPU.\n",
      "[2021-10-15 20:00:29,892][INFO] emmental.model:877 - [Bootleg] Model loaded from ../models/bootleg_uncased/bootleg_wiki.pth\n",
      "[2021-10-15 20:00:29,894][INFO] emmental.model:108 - Moving context_encoder module to CPU.\n",
      "[2021-10-15 20:00:29,898][INFO] emmental.model:108 - Moving entity_encoder module to CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load new annotator with our config - notice how it does have to reprep some things\n",
    "from bootleg.end2end.bootleg_annotator import BootlegAnnotator\n",
    "\n",
    "# You can also pass `return_embs=True` to get the embeddings\n",
    "ann = BootlegAnnotator(config=config, device=-1, return_embs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sa', 'saint vincent', 'saint vincent and the grenadines', 'saint vincent and the grenadines national u20 football team', 'saint vincent and the grenadines national under20 football team', 'saint vincent and the grenadines national football team', 'saint vincent and the grenadines national team', 'saint vincent and the grenadines u20', 'saint vincent and grenadines', 'saint vincent amp the grenadines']\n"
     ]
    }
   ],
   "source": [
    "# These are some of the aliases our model will possible extract from sentences...they are all about computers!\n",
    "print(list(ann.all_aliases_trie.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qids': [['NQ1']],\n",
       " 'probs': [[1.0]],\n",
       " 'titles': [['Some New Entity']],\n",
       " 'cands': [[['NQ1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1',\n",
       "    '-1']]],\n",
       " 'cand_probs': [[array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]],\n",
       " 'spans': [[[3, 5]]],\n",
       " 'char_spans': [[[12, 26]]],\n",
       " 'aliases': [['sparkle device']]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract some mentions...\n",
    "# notice that there is less ambiguity as well because we removed a lot of QIDs from our dump\n",
    "ann.label_mentions(\"How did the sparkle device perform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Faster inference\n",
    "If you want more efficient inference of the annotator, we have the ability for the user to pass in a static entity\n",
    "embedding matrix so the model does not have to call a forward pass of the entity encoder.\n",
    "\n",
    "See our ```entity_embedding_tutorial.ipynb``` for how to call ```extract_all_entities```. The output of this\n",
    "can be passed into our annotator via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_emb_file = \"<path to file>\"\n",
    "ann = BootlegAnnotator(config=config, device=-1, return_embs=False, entity_emb_file=entity_emb_file)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
