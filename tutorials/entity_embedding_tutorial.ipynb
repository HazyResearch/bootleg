{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Embedding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we walk through how to generate Bootleg contextual entity embeddings for use in downstream tasks using a pretrained Bootleg model. We also demonstrate how to extract other Bootleg embeddings for downstream tasks when contextualized embeddings are not needed.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- Pretrained Bootleg uncased model and config [here](https://bootleg-data.s3-us-west-2.amazonaws.com/models/lateset/bootleg_uncased.tar.gz). Cased model and config [here](https://bootleg-data.s3-us-west-2.amazonaws.com/models/lateset/bootleg_cased.tar.gz)\n",
    "- Sample of Natural Questions with hand-labelled entities [here](https://bootleg-data.s3-us-west-2.amazonaws.com/data/lateset/nq.tar.gz)\n",
    "- Entity data [here](https://bootleg-data.s3-us-west-2.amazonaws.com/data/lateset/entity_db.tar.gz)\n",
    "\n",
    "For convenience, you can run the commands below (from the root directory of the repo) to download all the above files and unpack them to `models` and `data` directories. It will take several minutes to download all the files.\n",
    "\n",
    "```\n",
    "    # use cased for cased model\n",
    "    bash tutorials/download_model.sh uncased\n",
    "    bash tutorials/download_data.sh\n",
    "```\n",
    "\n",
    "You can also run directly in this notebook by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!sh download_model.sh uncased\n",
    "!sh download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare Model Config\n",
    "\n",
    "As with the other tutorials, we set up the config to point to the correct data directories and model checkpoint. We use the sample of [Natural Questions](https://ai.google.com/research/NaturalQuestions) with mentions extracted by Bootleg introduced in the End-to-End tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU with at least 12GB of memory available, set the below to 0 to run inference on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify the input directory where files were downloaded below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.utils.parser.parser_utils import parse_boot_and_emm_args\n",
    "from bootleg.utils.utils import load_yaml_file\n",
    "from bootleg.run import run_model\n",
    "\n",
    "# root_dir = FILL IN FULL PATH TO DIRECTORY WHERE DATA IS DOWNLOADED (e.g., root_dir/data and root_dir/models)\n",
    "root_dir = Path(\"../tutorial_data_ep\")\n",
    "\n",
    "config_in_path = str(root_dir / 'models/bootleg_uncased/bootleg_config.yaml')\n",
    "data_dir =  str(root_dir / 'data/nq')\n",
    "entity_dir = str(root_dir / 'data/entity_db')\n",
    "alias_map = \"alias2qids.json\"\n",
    "test_file = \"test_50_bootleg.jsonl\"\n",
    "\n",
    "config_args = load_yaml_file(config_in_path)\n",
    "\n",
    "# decrease number of data threads as this is a small file\n",
    "config_args[\"run_config\"][\"dataset_threads\"] = 2\n",
    "# set the model checkpoint path \n",
    "config_args[\"emmental\"][\"model_path\"] = str(root_dir / 'models/bootleg_uncased/bootleg_wiki.pth')\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args[\"data_config\"][\"entity_dir\"] = entity_dir\n",
    "config_args[\"data_config\"][\"alias_cand_map\"] = alias_map\n",
    "\n",
    "# set the data path and kore50 test file \n",
    "config_args[\"data_config\"][\"data_dir\"] = data_dir\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args[\"data_config\"][\"test_dataset\"][\"file\"] = test_file\n",
    "\n",
    "# set the embedding paths \n",
    "config_args[\"data_config\"][\"emb_dir\"] =  str(root_dir / 'data/entity_db')\n",
    "config_args[\"data_config\"][\"word_embedding\"][\"cache_dir\"] =  str(root_dir / 'data/pretrained_bert_models')\n",
    "\n",
    "# set the devie if on CPU\n",
    "config_args[\"emmental\"][\"device\"] = device\n",
    "\n",
    "config_args = parse_boot_and_emm_args(config_args) # or you can pass in the config_out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Contextual Entity Embeddings\n",
    "\n",
    "We now show how Bootleg contextualized embeddings can be loaded and used in downstream tasks. First we use the `dump_embs` mode to generate contextual entity embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 17:16:49,834 Logging was already initialized to use bootleg_logs/wiki_full_ft/2021_03_11/17_13_50/c2cd809c.  To configure logging manually, call emmental.init_logging before initialiting Meta.\n",
      "2021-03-11 17:16:49,888 Loading Emmental default config from /dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/emmental/emmental-default-config.yaml.\n",
      "2021-03-11 17:16:49,889 Updating Emmental config from user provided config.\n",
      "2021-03-11 17:16:49,890 Set random seed to 1234.\n",
      "2021-03-11 17:16:50,010 COMMAND: /dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/ipykernel_launcher.py -f /dfs/scratch0/lorr1/projects/:/afs/cs.stanford.edu/u/lorr1/.local/apt-cache/share/jupyter/runtime/kernel-1d6bf30d-8475-4b26-8c6d-c585b1302c91.json\n",
      "2021-03-11 17:16:50,011 Saving config to bootleg_logs/wiki_full_ft/2021_03_11/17_13_50/c2cd809c/parsed_config.yaml\n",
      "2021-03-11 17:16:50,419 Git Hash: Not able to retrieve git hash\n",
      "2021-03-11 17:16:50,421 Loading entity symbols...\n",
      "2021-03-11 17:19:08,203 Starting to build data for test from ../tutorial_data_ep/data/nq/test_50_bootleg.jsonl\n",
      "2021-03-11 17:19:08,211 Loading data from ../tutorial_data_ep/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_data.bin and ../tutorial_data_ep/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_label.bin\n",
      "2021-03-11 17:19:08,367 Final data initialization time for test is 0.16231799125671387s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/datasets/dataset.py:1014: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  guid_dtype = np.dtype(\n",
      "/dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/numpy/core/memmap.py:230: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  descr = dtypedescr(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 17:19:08,489 Built dataloader for test set with 49 and 1 threads samples (Shuffle=False, Batch size=32).\n",
      "2021-03-11 17:19:08,501 Building slice dataset for test from ../tutorial_data_ep/data/nq/test_50_bootleg.jsonl.\n",
      "2021-03-11 17:19:08,553 Loading data from ../tutorial_data_ep/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_slices_1f126b5224.bin and ../tutorial_data_ep/data/nq/prep/test_50_bootleg_bert-base-uncased_L100_A10_InC1_Aug1/ned_slices_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sent idx to row idx mapping: 100%|██████████| 50/50 [00:00<00:00, 10693.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 17:19:08,675 Final slice data initialization time from test is 0.17346429824829102s\n",
      "2021-03-11 17:19:08,676 Updating Emmental config from user provided config.\n",
      "2021-03-11 17:19:08,677 Set random seed to 1234.\n",
      "2021-03-11 17:19:08,683 Starting Bootleg Model\n",
      "2021-03-11 17:19:08,684 Created emmental model Bootleg that contains task set().\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 17:19:12,206 Loading embeddings...\n",
      "2021-03-11 17:19:40,563 Created task: NED\n",
      "2021-03-11 17:19:40,565 Moving bert module to CPU.\n",
      "2021-03-11 17:19:40,572 Moving embedding_payload module to CPU.\n",
      "2021-03-11 17:19:40,573 Moving attn_network module to CPU.\n",
      "2021-03-11 17:19:40,576 Moving pred_layer module to CPU.\n",
      "2021-03-11 17:19:40,577 Moving learned module to CPU.\n",
      "2021-03-11 17:19:40,578 Moving title_static module to CPU.\n",
      "2021-03-11 17:19:40,579 Moving learned_type module to CPU.\n",
      "2021-03-11 17:19:40,580 Moving learned_type_wiki module to CPU.\n",
      "2021-03-11 17:19:40,582 Moving learned_type_relations module to CPU.\n",
      "2021-03-11 17:19:40,583 Moving adj_index module to CPU.\n",
      "2021-03-11 17:19:47,806 Created task: Type\n",
      "2021-03-11 17:19:47,810 Moving bert module to CPU.\n",
      "2021-03-11 17:19:47,815 Moving embedding_payload module to CPU.\n",
      "2021-03-11 17:19:47,816 Moving attn_network module to CPU.\n",
      "2021-03-11 17:19:47,819 Moving pred_layer module to CPU.\n",
      "2021-03-11 17:19:47,820 Moving learned module to CPU.\n",
      "2021-03-11 17:19:47,821 Moving title_static module to CPU.\n",
      "2021-03-11 17:19:47,822 Moving learned_type module to CPU.\n",
      "2021-03-11 17:19:47,823 Moving learned_type_wiki module to CPU.\n",
      "2021-03-11 17:19:47,825 Moving learned_type_relations module to CPU.\n",
      "2021-03-11 17:19:47,825 Moving adj_index module to CPU.\n",
      "2021-03-11 17:19:47,826 Moving type_prediction module to CPU.\n",
      "2021-03-11 17:20:21,871 [Bootleg] Model loaded from ../tutorial_data_ep/models/bootleg_uncased/bootleg_wiki.pth\n",
      "2021-03-11 17:20:21,872 Moving bert module to CPU.\n",
      "2021-03-11 17:20:21,877 Moving embedding_payload module to CPU.\n",
      "2021-03-11 17:20:21,879 Moving attn_network module to CPU.\n",
      "2021-03-11 17:20:21,882 Moving pred_layer module to CPU.\n",
      "2021-03-11 17:20:21,883 Moving learned module to CPU.\n",
      "2021-03-11 17:20:21,884 Moving title_static module to CPU.\n",
      "2021-03-11 17:20:21,885 Moving learned_type module to CPU.\n",
      "2021-03-11 17:20:21,886 Moving learned_type_wiki module to CPU.\n",
      "2021-03-11 17:20:21,888 Moving learned_type_relations module to CPU.\n",
      "2021-03-11 17:20:21,888 Moving adj_index module to CPU.\n",
      "2021-03-11 17:20:21,890 Moving type_prediction module to CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Bootleg (test): 100%|██████████| 2/2 [00:09<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 17:20:34,497 Finished dumping. Merging results across accumulation steps.\n",
      "2021-03-11 17:20:34,539 Bootleg labels saved at bootleg_logs/wiki_full_ft/2021_03_11/17_13_50/c2cd809c/test_50_bootleg/bootleg_wiki/bootleg_labels.jsonl\n",
      "2021-03-11 17:20:34,540 Trying to merge numpy embedding arrays. If your machine is limited in memory, this may cause OOM errors. Is that happens, result files should be saved in bootleg_logs/wiki_full_ft/2021_03_11/17_13_50/c2cd809c/test_50_bootleg/bootleg_wiki/batch_results.\n",
      "2021-03-11 17:20:34,598 Bootleg embeddings saved at bootleg_logs/wiki_full_ft/2021_03_11/17_13_50/c2cd809c/test_50_bootleg/bootleg_wiki/bootleg_embs.npy\n"
     ]
    }
   ],
   "source": [
    "bootleg_label_file, bootleg_emb_file = run_model(mode=\"dump_embs\", config=config_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `dump_embs` mode, Bootleg saves the contextual entity embeddings corresponding to each mention in each sentence to a file. We return this file in the variable `bootleg_emb_file`. We can also see the full file path in the log (ends in `*npy`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "contextual_entity_embs = np.load(bootleg_emb_file)\n",
    "contextual_entity_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the contextual entity embedding above corresponds to an extracted mention in a sentence. In the above embedding there are 100 extracted mentions total with 512 dimensions for each corresponding contextual entity embedding.\n",
    "\n",
    "The mapping from mentions to rows in the contextual entity embedding is stored in `ctx_emb_ids` in the label file. We now check out the label file, which was also generated and returned from running `dump_embs` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Which of these was not an export of Ancient Greece\n",
      "mentions: ['ancient greece']\n",
      "contextual emb ids: [49]\n",
      "\n",
      "sentence: Who opened and closed the 1960 Winter Olympics\n",
      "mentions: ['1960 winter olympics']\n",
      "contextual emb ids: [50]\n",
      "\n",
      "sentence: I see the river Tiber foaming with much blood\n",
      "mentions: ['river tiber']\n",
      "contextual emb ids: [51]\n",
      "\n",
      "sentence: What causes a dead zone in the ocean\n",
      "mentions: ['dead zone']\n",
      "contextual emb ids: [52]\n",
      "\n",
      "sentence: Who plays Claire Underwood 's mom on House of Cards\n",
      "mentions: ['claire underwood', 'mom', 'house of cards']\n",
      "contextual emb ids: [53, 54, 55]\n",
      "\n",
      "sentence: What is the T Rex name in Land Before Time\n",
      "mentions: ['t rex', 'time']\n",
      "contextual emb ids: [56, 57]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open(bootleg_label_file) as f: \n",
    "    for i, line in enumerate(f): \n",
    "        print('sentence:', line['sentence'])\n",
    "        print('mentions:', line['aliases'])\n",
    "        print('contextual emb ids:', line['ctx_emb_ids'])\n",
    "        print()\n",
    "        if i == 5: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first sentence, we can find the corresponding contextual entity embedding for \"the voice\", \"the magician\", and \"frosty the snowman\" in rows 0, 1, and 2 of `contextual_entity_embs`, respectively. Similarly, we have unique row ids for the mentions in each of the other sentences. A downstream task can use this process to load the correct contextual entity embeddings for each mention in a simple dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Static Embeddings\n",
    "\n",
    "In addition to contextual entity embeddings, Bootleg learns static entity embeddings as well as type and relation embeddings. These can be useful in downstream tasks when contextual information is not available for the downstream task, or if we want the same entity embedding regardless of the context or position of the mention.\n",
    "\n",
    "We walk through how to extract the static, learned entity embeddings from a pretrained Bootleg model. First, we define a utility function to load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import emmental\n",
    "from bootleg.task_config import NED_TASK, TYPE_PRED_TASK\n",
    "from bootleg.tasks import ned_task, type_pred_task\n",
    "from bootleg.symbols.entity_symbols import EntitySymbols\n",
    "from emmental.model import EmmentalModel\n",
    "\n",
    "\n",
    "def load_model(config, device=-1):\n",
    "        if \"emmental\" in config:\n",
    "            config = parse_boot_and_emm_args(config)\n",
    "\n",
    "        emmental.init(\n",
    "            log_dir=config[\"meta_config\"][\"log_path\"], config=config\n",
    "        )\n",
    "\n",
    "        print(\"Reading entity database\")\n",
    "        entity_db = EntitySymbols.load_from_cache(\n",
    "            os.path.join(\n",
    "                config.data_config.entity_dir,\n",
    "                config.data_config.entity_map_dir,\n",
    "            ),\n",
    "            alias_cand_map_file=config.data_config.alias_cand_map,\n",
    "        )\n",
    "\n",
    "        # Create tasks\n",
    "        tasks = [NED_TASK]\n",
    "        if config.data_config.type_prediction.use_type_pred is True:\n",
    "            tasks.append(TYPE_PRED_TASK)\n",
    "\n",
    "        # Create tasks\n",
    "        model = EmmentalModel(name=\"Bootleg\")\n",
    "        model.add_task(ned_task.create_task(config, entity_db))\n",
    "        if TYPE_PRED_TASK in tasks:\n",
    "            model.add_task(type_pred_task.create_task(config, entity_db))\n",
    "            # Add the mention type embedding to the embedding payload\n",
    "            type_pred_task.update_ned_task(model)\n",
    "\n",
    "        print(\"Loading model\")\n",
    "        # Load the best model from the pretrained model\n",
    "        assert (\n",
    "            config[\"model_config\"][\"model_path\"] is not None\n",
    "        ), f\"Must have a model to load in the model_path for the BootlegAnnotator\"\n",
    "        model.load(config[\"model_config\"][\"model_path\"])\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained Bootleg model. This will take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-11 17:38:15,779 Logging was already initialized to use bootleg_logs/wiki_full_ft/2021_03_11/17_13_50/c2cd809c.  To configure logging manually, call emmental.init_logging before initialiting Meta.\n",
      "2021-03-11 17:38:15,839 Loading Emmental default config from /dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/emmental/emmental-default-config.yaml.\n",
      "2021-03-11 17:38:15,840 Updating Emmental config from user provided config.\n",
      "2021-03-11 17:38:15,842 Set random seed to 1234.\n",
      "Reading entity database\n",
      "2021-03-11 17:40:06,511 Created emmental model Bootleg that contains task set().\n",
      "2021-03-11 17:40:24,198 Loading embeddings...\n",
      "2021-03-11 17:40:46,679 Created task: NED\n",
      "2021-03-11 17:40:46,680 Moving bert module to CPU.\n",
      "2021-03-11 17:40:46,685 Moving embedding_payload module to CPU.\n",
      "2021-03-11 17:40:46,686 Moving attn_network module to CPU.\n",
      "2021-03-11 17:40:46,689 Moving pred_layer module to CPU.\n",
      "2021-03-11 17:40:46,690 Moving learned module to CPU.\n",
      "2021-03-11 17:40:46,690 Moving title_static module to CPU.\n",
      "2021-03-11 17:40:46,691 Moving learned_type module to CPU.\n",
      "2021-03-11 17:40:46,692 Moving learned_type_wiki module to CPU.\n",
      "2021-03-11 17:40:46,693 Moving learned_type_relations module to CPU.\n",
      "2021-03-11 17:40:46,694 Moving adj_index module to CPU.\n",
      "2021-03-11 17:40:50,071 Created task: Type\n",
      "2021-03-11 17:40:50,073 Moving bert module to CPU.\n",
      "2021-03-11 17:40:50,079 Moving embedding_payload module to CPU.\n",
      "2021-03-11 17:40:50,080 Moving attn_network module to CPU.\n",
      "2021-03-11 17:40:50,083 Moving pred_layer module to CPU.\n",
      "2021-03-11 17:40:50,084 Moving learned module to CPU.\n",
      "2021-03-11 17:40:50,085 Moving title_static module to CPU.\n",
      "2021-03-11 17:40:50,086 Moving learned_type module to CPU.\n",
      "2021-03-11 17:40:50,087 Moving learned_type_wiki module to CPU.\n",
      "2021-03-11 17:40:50,089 Moving learned_type_relations module to CPU.\n",
      "2021-03-11 17:40:50,090 Moving adj_index module to CPU.\n",
      "2021-03-11 17:40:50,091 Moving type_prediction module to CPU.\n",
      "Loading model\n",
      "2021-03-11 17:41:15,448 [Bootleg] Model loaded from ../tutorial_data_ep/models/bootleg_uncased/bootleg_wiki.pth\n",
      "2021-03-11 17:41:15,448 Moving bert module to CPU.\n",
      "2021-03-11 17:41:15,453 Moving embedding_payload module to CPU.\n",
      "2021-03-11 17:41:15,454 Moving attn_network module to CPU.\n",
      "2021-03-11 17:41:15,456 Moving pred_layer module to CPU.\n",
      "2021-03-11 17:41:15,457 Moving learned module to CPU.\n",
      "2021-03-11 17:41:15,458 Moving title_static module to CPU.\n",
      "2021-03-11 17:41:15,458 Moving learned_type module to CPU.\n",
      "2021-03-11 17:41:15,459 Moving learned_type_wiki module to CPU.\n",
      "2021-03-11 17:41:15,460 Moving learned_type_relations module to CPU.\n",
      "2021-03-11 17:41:15,461 Moving adj_index module to CPU.\n",
      "2021-03-11 17:41:15,461 Moving type_prediction module to CPU.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(config=config_args, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the static, learned entity embedding as a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5832701, 200])\n"
     ]
    }
   ],
   "source": [
    "learned_emb_obj = model.module_pool.learned\n",
    "embedding_as_tensor = torch.Tensor(learned_emb_obj.learned_entity_embedding.weight)\n",
    "print(embedding_as_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Bootleg model was trained on data with 5.8 million entities and each entity embedding is 200-dimensional, as indicated by the shape of the static, learned entity embedding above.\n",
    "\n",
    "The mapping from mentions to rows in the static, learned entity embedding (corresponding to the predicted entity) is also saved in the label file produced by `dump_embs` mode. We check out the label file below and use the `entity_ids` key to find the corresponding embedding row. The `entity_ids` can also be extracted from the returned `qids` by using the `qid2eid.json` mapping in `entity_dir/entity_mappings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: Which of these was not an export of Ancient Greece\n",
      "mentions: ['ancient greece']\n",
      "entity ids: [552973]\n",
      "\n",
      "sentence: Who opened and closed the 1960 Winter Olympics\n",
      "mentions: ['1960 winter olympics']\n",
      "entity ids: [91786]\n",
      "\n",
      "sentence: I see the river Tiber foaming with much blood\n",
      "mentions: ['river tiber']\n",
      "entity ids: [2608573]\n",
      "\n",
      "sentence: What causes a dead zone in the ocean\n",
      "mentions: ['dead zone']\n",
      "entity ids: [2793916]\n",
      "\n",
      "sentence: Who plays Claire Underwood 's mom on House of Cards\n",
      "mentions: ['claire underwood', 'mom', 'house of cards']\n",
      "entity ids: [3443290, 564561, 3993575]\n",
      "\n",
      "sentence: What is the T Rex name in Land Before Time\n",
      "mentions: ['t rex', 'time']\n",
      "entity ids: [3052538, 1284493]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open(bootleg_label_file) as f: \n",
    "    for i, line in enumerate(f): \n",
    "        print('sentence:', line['sentence'])\n",
    "        print('mentions:', line['aliases'])\n",
    "        print('entity ids:', line['entity_ids'])\n",
    "        print()\n",
    "        if i == 5: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the contextual entity embeddings, the static embeddings are not unique across mentions. For instance, if the same entity is predicted across two different mentions, the static entity embedding (and ids in the label file) will be the same for those mentions, whereas the contextual entity embeddings and ids will be different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract the embeddings through the `forward` pass on the embedding class. We will use random entity ids for demonstration.\n",
    "\n",
    "### Important: the `forward` pass will _normalize_ the embedding. Use the weight tensor above to not normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 200])\n"
     ]
    }
   ],
   "source": [
    "learned_emb_obj = model.module_pool.learned\n",
    "batch = 5\n",
    "M = 4\n",
    "K = 3\n",
    "eid_cands = torch.randint(0, 5000, (batch, M, K))\n",
    "# batch_on_the_fly_data is a dictionary used for KG metadata; keep it emtpy for extracting embeddings\n",
    "embs = learned_emb_obj(eid_cands, batch_on_the_fly_data={})\n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat the same process to extract the type embeddings. Our type embeddings are 128 dimensions.\n",
    "\n",
    "### Important: the type module `forward` will also _normalize_ and apply an additive attention mechanism to merge the multiple type embeddings for a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "wd_type_obj = model.module_pool.learned_type_wiki\n",
    "batch = 5\n",
    "M = 4\n",
    "K = 3\n",
    "eid_cands = torch.randint(0, 5000, (batch, M, K))\n",
    "embs = wd_type_obj(eid_cands, {})\n",
    "print(embs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
