{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Embedding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we walk through how to generate Bootleg contextual entity embeddings for use in downstream tasks using a pretrained Bootleg model. We also demonstrate how to extract other Bootleg embeddings for downstream tasks when contextualized embeddings are not needed.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to download the following files for this notebook:\n",
    "- Pretrained Bootleg uncased model and config [here](https://bootleg-data.s3.amazonaws.com/models/lateset/bootleg_uncased.tar.gz). Cased model and config [here](https://bootleg-data.s3.amazonaws.com/models/lateset/bootleg_cased.tar.gz)\n",
    "- Sample of Natural Questions with hand-labelled entities [here](https://bootleg-data.s3.amazonaws.com/data/lateset/nq.tar.gz)\n",
    "- Entity data [here](https://bootleg-data.s3.amazonaws.com/data/lateset/wiki_entity_data.tar.gz)\n",
    "- Embedding data [here](https://bootleg-data.s3.amazonaws.com/data/lateset/emb_data.tar.gz)\n",
    "\n",
    "For convenience, you can run the commands below (from the root directory of the repo) to download all the above files and unpack them to `models` and `data` directories. It will take several minutes to download all the files.\n",
    "\n",
    "```\n",
    "    # use cased for cased model\n",
    "    bash tutorials/download_model.sh uncased\n",
    "    bash tutorials/download_data.sh\n",
    "```\n",
    "\n",
    "You can also run directly in this notebook by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!sh download_model.sh uncased\n",
    "!sh download_data.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Prepare Model Config\n",
    "\n",
    "As with the other tutorials, we set up the config to point to the correct data directories and model checkpoint. We use the sample of [Natural Questions](https://ai.google.com/research/NaturalQuestions) with mentions extracted by Bootleg introduced in the End-to-End tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a GPU with at least 12GB of memory available, set the below to 0 to run inference on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to specify the input directory where files were downloaded below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.utils.parser.parser_utils import parse_boot_and_emm_args\n",
    "from bootleg.utils.utils import load_yaml_file\n",
    "from bootleg.run import run_model\n",
    "\n",
    "# root_dir = FILL IN FULL PATH TO DIRECTORY WHERE DATA IS DOWNLOADED (e.g., root_dir/data and root_dir/models)\n",
    "root_dir = Path(\".\")\n",
    "\n",
    "config_in_path = str(root_dir / 'models/bootleg_uncased/bootleg_config.yaml')\n",
    "data_dir =  str(root_dir / 'data/nq')\n",
    "entity_dir = str(root_dir / 'data/wiki_entity_data')\n",
    "alias_map = \"alias2qids_wiki_filt.json\"\n",
    "test_file = \"test_bootleg_men.jsonl\"\n",
    "\n",
    "config_args = load_yaml_file(config_in_path)\n",
    "\n",
    "# decrease number of data threads as this is a small file\n",
    "config_args[\"run_config\"][\"dataset_threads\"] = 2\n",
    "# set the model checkpoint path \n",
    "config_args[\"emmental\"][\"model_path\"] = str(root_dir / 'models/bootleg_uncased/bootleg_wiki.pth')\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args[\"data_config\"][\"entity_dir\"] = entity_dir\n",
    "config_args[\"data_config\"][\"alias_cand_map\"] = alias_map\n",
    "\n",
    "# set the data path and kore50 test file \n",
    "config_args[\"data_config\"][\"data_dir\"] = data_dir\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args[\"data_config\"][\"test_dataset\"][\"file\"] = test_file\n",
    "\n",
    "# set the embedding paths \n",
    "config_args[\"data_config\"][\"emb_dir\"] =  str(root_dir / 'data/emb_data')\n",
    "config_args[\"data_config\"][\"word_embedding\"][\"cache_dir\"] =  str(root_dir / 'data/embs/pretrained_bert_models')\n",
    "\n",
    "# set the devie if on CPU\n",
    "config_args[\"emmental\"][\"device\"] = device\n",
    "\n",
    "config_args = parse_boot_and_emm_args(config_args) # or you can pass in the config_out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Contextual Entity Embeddings\n",
    "\n",
    "We now show how Bootleg contextualized embeddings can be loaded and used in downstream tasks. First we use the `dump_embs` mode to generate contextual entity embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 14:00:56,371 Logging was already initialized to use bootleg_logs/wiki_full_ft/2021_01_28/12_20_50/bc3d0092.  To configure logging manually, call emmental.init_logging before initialiting Meta.\n",
      "2021-01-28 14:00:56,424 Loading Emmental default config from /dfs/scratch0/lorr1/projects/emmental/src/emmental/emmental-default-config.yaml.\n",
      "2021-01-28 14:00:56,425 Updating Emmental config from user provided config.\n",
      "2021-01-28 14:00:56,549 COMMAND: /dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/ipykernel_launcher.py -f /dfs/scratch0/lorr1/projects/bootleg/notebooks/:/afs/cs.stanford.edu/u/lorr1/.local/apt-cache/share/jupyter/runtime/kernel-4a75c8f6-3129-4873-a5c1-0a51ed79b2fe.json\n",
      "2021-01-28 14:00:56,550 Saving config to bootleg_logs/wiki_full_ft/2021_01_28/12_20_50/bc3d0092/parsed_config.yaml\n",
      "2021-01-28 14:00:57,287 Git Hash: Not able to retrieve git hash\n",
      "2021-01-28 14:00:57,288 Loading entity symbols...\n",
      "2021-01-28 14:03:39,383 Starting to build data for test from /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/test_bootleg_men.jsonl\n",
      "2021-01-28 14:03:39,437 Loading data from /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/test_bootleg_men_bert-base-cased_L100_A10_InC1_Aug1/ned_data.bin and /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/test_bootleg_men_bert-base-cased_L100_A10_InC1_Aug1/ned_label.bin\n",
      "2021-01-28 14:03:39,499 Final data initialization time for test is 0.11511802673339844s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/datasets/dataset.py:912: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  guid_dtype = np.dtype(\n",
      "/dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/numpy/core/memmap.py:230: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  descr = dtypedescr(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 14:03:39,622 Built dataloader for test set with 2540 and 1 threads samples (Shuffle=False, Batch size=32).\n",
      "2021-01-28 14:03:39,630 Building slice dataset for test from /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/test_bootleg_men.jsonl.\n",
      "2021-01-28 14:03:39,686 Loading data from /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/test_bootleg_men_bert-base-cased_L100_A10_InC1_Aug1/ned_slices_1f126b5224.bin and /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/test_bootleg_men_bert-base-cased_L100_A10_InC1_Aug1/ned_slices_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sent idx to row idx mapping: 100%|██████████| 2465/2465 [00:00<00:00, 16114.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 14:03:39,901 Final slice data initialization time from test is 0.2705113887786865s\n",
      "2021-01-28 14:03:39,902 Updating Emmental config from user provided config.\n",
      "2021-01-28 14:03:39,908 Starting Bootleg Model\n",
      "2021-01-28 14:03:39,909 Created emmental model Bootleg that contains task set().\n",
      "2021-01-28 14:03:46,040 Loading embeddings...\n",
      "2021-01-28 14:03:46,041 Embedding \"learned\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedEntityEmb\",\n",
      "    \"key\":\"learned\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 14:04:36,082 Embedding \"title_static\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"StaticEmb\",\n",
      "    \"key\":\"title_static\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 14:06:57,154 Embedding \"learned_type\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedTypeEmb\",\n",
      "    \"key\":\"learned_type\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 14:06:58,268 Embedding \"learned_type_wiki\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedTypeEmb\",\n",
      "    \"key\":\"learned_type_wiki\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 14:06:59,453 Embedding \"learned_type_relations\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedTypeEmb\",\n",
      "    \"key\":\"learned_type_relations\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 14:07:18,174 Embedding \"adj_index\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"KGIndices\",\n",
      "    \"key\":\"adj_index\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":false,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 14:07:26,349 Created task: NED\n",
      "2021-01-28 14:07:26,351 Moving bert module to CPU.\n",
      "2021-01-28 14:07:26,356 Moving embedding_payload module to CPU.\n",
      "2021-01-28 14:07:26,358 Moving attn_network module to CPU.\n",
      "2021-01-28 14:07:26,361 Moving pred_layer module to CPU.\n",
      "2021-01-28 14:07:26,362 Moving learned module to CPU.\n",
      "2021-01-28 14:07:26,363 Moving title_static module to CPU.\n",
      "2021-01-28 14:07:26,364 Moving learned_type module to CPU.\n",
      "2021-01-28 14:07:26,365 Moving learned_type_wiki module to CPU.\n",
      "2021-01-28 14:07:26,366 Moving learned_type_relations module to CPU.\n",
      "2021-01-28 14:07:26,367 Moving adj_index module to CPU.\n",
      "2021-01-28 14:07:31,944 Created task: Type\n",
      "2021-01-28 14:07:31,946 Moving bert module to CPU.\n",
      "2021-01-28 14:07:31,951 Moving embedding_payload module to CPU.\n",
      "2021-01-28 14:07:31,952 Moving attn_network module to CPU.\n",
      "2021-01-28 14:07:31,955 Moving pred_layer module to CPU.\n",
      "2021-01-28 14:07:31,957 Moving learned module to CPU.\n",
      "2021-01-28 14:07:31,958 Moving title_static module to CPU.\n",
      "2021-01-28 14:07:31,958 Moving learned_type module to CPU.\n",
      "2021-01-28 14:07:31,959 Moving learned_type_wiki module to CPU.\n",
      "2021-01-28 14:07:31,960 Moving learned_type_relations module to CPU.\n",
      "2021-01-28 14:07:31,961 Moving adj_index module to CPU.\n",
      "2021-01-28 14:07:31,961 Moving type_prediction module to CPU.\n",
      "2021-01-28 14:09:53,318 [Bootleg] Model loaded from /dfs/scratch0/lorr1/projects/bootleg/tutorial_data/models/bootleg_wiki/bootleg_wiki_1.pth\n",
      "2021-01-28 14:09:53,319 Moving bert module to CPU.\n",
      "2021-01-28 14:09:53,328 Moving embedding_payload module to CPU.\n",
      "2021-01-28 14:09:53,330 Moving attn_network module to CPU.\n",
      "2021-01-28 14:09:53,334 Moving pred_layer module to CPU.\n",
      "2021-01-28 14:09:53,335 Moving learned module to CPU.\n",
      "2021-01-28 14:09:53,336 Moving title_static module to CPU.\n",
      "2021-01-28 14:09:53,338 Moving learned_type module to CPU.\n",
      "2021-01-28 14:09:53,339 Moving learned_type_wiki module to CPU.\n",
      "2021-01-28 14:09:53,340 Moving learned_type_relations module to CPU.\n",
      "2021-01-28 14:09:53,341 Moving adj_index module to CPU.\n",
      "2021-01-28 14:09:53,342 Moving type_prediction module to CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Bootleg (test): 100%|██████████| 80/80 [05:15<00:00,  3.94s/it]\n",
      "100%|██████████| 2540/2540 [00:00<00:00, 4134.38it/s]\n",
      "Reading values for marisa trie: 100%|██████████| 2465/2465 [00:00<00:00, 360368.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 14:15:11,440 Merging sentences together with 2 processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building sent_idx, alias_list_pos mapping: 100%|██████████| 11360/11360 [00:00<00:00, 97337.90it/s] \n",
      "Reading values for marisa trie: 100%|██████████| 11360/11360 [00:00<00:00, 401629.31it/s]\n",
      "Writing data: 100%|██████████| 1233/1233 [00:03<00:00, 400.69it/s]\n",
      "Writing data: 100%|██████████| 1232/1232 [00:00<00:00, 5720.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 14:16:48,177 Merging output files\n",
      "2021-01-28 14:16:50,036 Saving contextual entity embeddings to bootleg_logs/wiki_full_ft/2021_01_28/12_20_50/bc3d0092/test_bootleg_men/bootleg_wiki_1/bootleg_embs.npy\n",
      "2021-01-28 14:16:50,038 Wrote predictions to bootleg_logs/wiki_full_ft/2021_01_28/12_20_50/bc3d0092/test_bootleg_men/bootleg_wiki_1/bootleg_labels.jsonl\n"
     ]
    }
   ],
   "source": [
    "bootleg_label_file, bootleg_emb_file = run_model(mode=\"dump_embs\", config=config_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `dump_embs` mode, Bootleg saves the contextual entity embeddings corresponding to each mention in each sentence to a file. We return this file in the variable `bootleg_emb_file`. We can also see the full file path in the log (ends in `*npy`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11360, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "contextual_entity_embs = np.load(bootleg_emb_file)\n",
    "contextual_entity_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the contextual entity embedding above corresponds to an extracted mention in a sentence. In the above embedding there are 100 extracted mentions total with 512 dimensions for each corresponding contextual entity embedding.\n",
    "\n",
    "The mapping from mentions to rows in the contextual entity embedding is stored in `ctx_emb_ids` in the label file. We now check out the label file, which was also generated and returned from running `dump_embs` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: soccer - japan get lucky win , china in surprise defeat . soccer - japan get lucky win , china in surprise defeat .\n",
      "mentions: ['soccer', 'japan', 'china', 'soccer', 'japan', 'china']\n",
      "contextual emb ids: [0, 1, 2, 3, 4, 5]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . al-ain , United Arab Emirates 1996-12-06\n",
      "mentions: ['soccer', 'japan', 'china', 'alain', 'united arab emirates']\n",
      "contextual emb ids: [6, 7, 8, 9, 10]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .\n",
      "mentions: ['soccer', 'japan', 'china', 'japan', 'asian cup', 'syria', 'group c', 'championship match', 'friday']\n",
      "contextual emb ids: [11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .\n",
      "mentions: ['soccer', 'japan', 'china', 'china', 'luck', 'uzbekistan']\n",
      "contextual emb ids: [20, 21, 22, 23, 24, 25]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .\n",
      "mentions: ['soccer', 'japan', 'china', 'china', 'uzbek', 'striker', 'igor shkvyrin', 'header', 'ball', 'keeper', 'net']\n",
      "contextual emb ids: [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . The former Soviet republic was playing in an Asian Cup finals tie for the first time .\n",
      "mentions: ['soccer', 'japan', 'china', 'soviet republic', 'asian cup', 'time']\n",
      "contextual emb ids: [37, 38, 39, 40, 41, 42]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open(bootleg_label_file) as f: \n",
    "    for i, line in enumerate(f): \n",
    "        print('sentence:', line['sentence'])\n",
    "        print('mentions:', line['aliases'])\n",
    "        print('contextual emb ids:', line['ctx_emb_ids'])\n",
    "        print()\n",
    "        if i == 5: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first sentence, we can find the corresponding contextual entity embedding for \"the voice\", \"the magician\", and \"frosty the snowman\" in rows 0, 1, and 2 of `contextual_entity_embs`, respectively. Similarly, we have unique row ids for the mentions in each of the other sentences. A downstream task can use this process to load the correct contextual entity embeddings for each mention in a simple dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Static Embeddings\n",
    "\n",
    "In addition to contextual entity embeddings, Bootleg learns static entity embeddings as well as type and relation embeddings. These can be useful in downstream tasks when contextual information is not available for the downstream task, or if we want the same entity embedding regardless of the context or position of the mention.\n",
    "\n",
    "We walk through how to extract the static, learned entity embeddings from a pretrained Bootleg model. First, we define a utility function to load a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import emmental\n",
    "from bootleg.task_config import NED_TASK, TYPE_PRED_TASK\n",
    "from bootleg.tasks import ned_task, type_pred_task\n",
    "from bootleg.symbols.entity_symbols import EntitySymbols\n",
    "from emmental.model import EmmentalModel\n",
    "\n",
    "\n",
    "def load_model(config, device=-1):\n",
    "        if \"emmental\" in config:\n",
    "            config = parse_boot_and_emm_args(config)\n",
    "\n",
    "        emmental.init(\n",
    "            log_dir=config[\"meta_config\"][\"log_path\"], config=config\n",
    "        )\n",
    "\n",
    "        print(\"Reading entity database\")\n",
    "        entity_db = EntitySymbols(\n",
    "            os.path.join(\n",
    "                config.data_config.entity_dir,\n",
    "                config.data_config.entity_map_dir,\n",
    "            ),\n",
    "            alias_cand_map_file=config.data_config.alias_cand_map,\n",
    "        )\n",
    "\n",
    "        # Create tasks\n",
    "        tasks = [NED_TASK]\n",
    "        if config.data_config.type_prediction.use_type_pred is True:\n",
    "            tasks.append(TYPE_PRED_TASK)\n",
    "\n",
    "        # Create tasks\n",
    "        model = EmmentalModel(name=\"Bootleg\")\n",
    "        model.add_task(ned_task.create_task(config, entity_db))\n",
    "        if TYPE_PRED_TASK in tasks:\n",
    "            model.add_task(type_pred_task.create_task(config, entity_db))\n",
    "            # Add the mention type embedding to the embedding payload\n",
    "            type_pred_task.update_ned_task(model)\n",
    "\n",
    "        print(\"Loading model\")\n",
    "        # Load the best model from the pretrained model\n",
    "        assert (\n",
    "            config[\"model_config\"][\"model_path\"] is not None\n",
    "        ), f\"Must have a model to load in the model_path for the BootlegAnnotator\"\n",
    "        model.load(config[\"model_config\"][\"model_path\"])\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained Bootleg model. This will take several minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-28 15:01:52,858 Logging was already initialized to use bootleg_logs/wiki_full_ft/2021_01_28/12_20_50/bc3d0092.  To configure logging manually, call emmental.init_logging before initialiting Meta.\n",
      "2021-01-28 15:01:52,917 Loading Emmental default config from /dfs/scratch0/lorr1/projects/emmental/src/emmental/emmental-default-config.yaml.\n",
      "2021-01-28 15:01:52,918 Updating Emmental config from user provided config.\n",
      "Reading entity database\n",
      "Reading word tokenizers\n",
      "2021-01-28 15:04:07,517 Created emmental model Bootleg that contains task set().\n",
      "2021-01-28 15:04:10,958 Loading embeddings...\n",
      "2021-01-28 15:04:10,959 Embedding \"learned\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedEntityEmb\",\n",
      "    \"key\":\"learned\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 15:04:23,757 Embedding \"title_static\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"StaticEmb\",\n",
      "    \"key\":\"title_static\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 15:04:35,536 Embedding \"learned_type\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedTypeEmb\",\n",
      "    \"key\":\"learned_type\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 15:04:35,634 Embedding \"learned_type_wiki\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedTypeEmb\",\n",
      "    \"key\":\"learned_type_wiki\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 15:04:35,775 Embedding \"learned_type_relations\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"LearnedTypeEmb\",\n",
      "    \"key\":\"learned_type_relations\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":true,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 15:04:44,275 Embedding \"adj_index\" has params (these can be changed in the config)\n",
      "{\n",
      "    \"load_class\":\"KGIndices\",\n",
      "    \"key\":\"adj_index\",\n",
      "    \"cpu\":false,\n",
      "    \"freeze\":false,\n",
      "    \"dropout1d\":0.0,\n",
      "    \"dropout2d\":0.0,\n",
      "    \"normalize\":false,\n",
      "    \"send_through_bert\":false\n",
      "}\n",
      "2021-01-28 15:04:50,126 Created task: NED\n",
      "2021-01-28 15:04:50,128 Moving bert module to CPU.\n",
      "2021-01-28 15:04:50,134 Moving embedding_payload module to CPU.\n",
      "2021-01-28 15:04:50,135 Moving attn_network module to CPU.\n",
      "2021-01-28 15:04:50,138 Moving pred_layer module to CPU.\n",
      "2021-01-28 15:04:50,139 Moving learned module to CPU.\n",
      "2021-01-28 15:04:50,140 Moving title_static module to CPU.\n",
      "2021-01-28 15:04:50,141 Moving learned_type module to CPU.\n",
      "2021-01-28 15:04:50,142 Moving learned_type_wiki module to CPU.\n",
      "2021-01-28 15:04:50,143 Moving learned_type_relations module to CPU.\n",
      "2021-01-28 15:04:50,144 Moving adj_index module to CPU.\n",
      "2021-01-28 15:04:53,482 Created task: Type\n",
      "2021-01-28 15:04:53,483 Moving bert module to CPU.\n",
      "2021-01-28 15:04:53,488 Moving embedding_payload module to CPU.\n",
      "2021-01-28 15:04:53,490 Moving attn_network module to CPU.\n",
      "2021-01-28 15:04:53,493 Moving pred_layer module to CPU.\n",
      "2021-01-28 15:04:53,494 Moving learned module to CPU.\n",
      "2021-01-28 15:04:53,495 Moving title_static module to CPU.\n",
      "2021-01-28 15:04:53,495 Moving learned_type module to CPU.\n",
      "2021-01-28 15:04:53,496 Moving learned_type_wiki module to CPU.\n",
      "2021-01-28 15:04:53,497 Moving learned_type_relations module to CPU.\n",
      "2021-01-28 15:04:53,498 Moving adj_index module to CPU.\n",
      "2021-01-28 15:04:53,499 Moving type_prediction module to CPU.\n",
      "Loading model\n",
      "2021-01-28 15:06:28,388 [Bootleg] Model loaded from /dfs/scratch0/lorr1/projects/bootleg/tutorial_data/models/bootleg_wiki/bootleg_wiki_1.pth\n",
      "2021-01-28 15:06:28,390 Moving bert module to CPU.\n",
      "2021-01-28 15:06:28,397 Moving embedding_payload module to CPU.\n",
      "2021-01-28 15:06:28,399 Moving attn_network module to CPU.\n",
      "2021-01-28 15:06:28,402 Moving pred_layer module to CPU.\n",
      "2021-01-28 15:06:28,403 Moving learned module to CPU.\n",
      "2021-01-28 15:06:28,404 Moving title_static module to CPU.\n",
      "2021-01-28 15:06:28,406 Moving learned_type module to CPU.\n",
      "2021-01-28 15:06:28,407 Moving learned_type_wiki module to CPU.\n",
      "2021-01-28 15:06:28,408 Moving learned_type_relations module to CPU.\n",
      "2021-01-28 15:06:28,409 Moving adj_index module to CPU.\n",
      "2021-01-28 15:06:28,410 Moving type_prediction module to CPU.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(config=config_args, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the static, learned entity embedding as a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5832701, 200])\n"
     ]
    }
   ],
   "source": [
    "learned_emb_obj = model.module_pool.learned\n",
    "embedding_as_tensor = torch.Tensor(learned_emb_obj.learned_entity_embedding.weight)\n",
    "print(embedding_as_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Bootleg model was trained on data with 5.8 million entities and each entity embedding is 200-dimensional, as indicated by the shape of the static, learned entity embedding above.\n",
    "\n",
    "The mapping from mentions to rows in the static, learned entity embedding (corresponding to the predicted entity) is also saved in the label file produced by `dump_embs` mode. We check out the label file below and use the `entity_ids` key to find the corresponding embedding row. The `entity_ids` can also be extracted from the returned `qids` by using the `qid2eid.json` mapping in `entity_dir/entity_mappings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: soccer - japan get lucky win , china in surprise defeat . soccer - japan get lucky win , china in surprise defeat .\n",
      "mentions: ['soccer', 'japan', 'china', 'soccer', 'japan', 'china']\n",
      "entity ids: [3157011, 3705410, 106035, 3157011, 3705410, 4486968]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . al-ain , United Arab Emirates 1996-12-06\n",
      "mentions: ['soccer', 'japan', 'china', 'alain', 'united arab emirates']\n",
      "entity ids: [3157011, 4223535, 4486968, 1944367, 2478913]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .\n",
      "mentions: ['soccer', 'japan', 'china', 'japan', 'asian cup', 'syria', 'group c', 'championship match', 'friday']\n",
      "entity ids: [3157011, 1593316, 4486968, 4223535, 2069182, 120274, 1853145, 519968, 1932597]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .\n",
      "mentions: ['soccer', 'japan', 'china', 'china', 'luck', 'uzbekistan']\n",
      "entity ids: [3157011, 4223535, 106035, 4486968, 2437104, 4835404]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .\n",
      "mentions: ['soccer', 'japan', 'china', 'china', 'uzbek', 'striker', 'igor shkvyrin', 'header', 'ball', 'keeper', 'net']\n",
      "entity ids: [3157011, 4223535, 106035, 4486968, 2436091, 2355508, 1127933, 2549912, 3969798, 554382, 1498195]\n",
      "\n",
      "sentence: soccer - japan get lucky win , china in surprise defeat . The former Soviet republic was playing in an Asian Cup finals tie for the first time .\n",
      "mentions: ['soccer', 'japan', 'china', 'soviet republic', 'asian cup', 'time']\n",
      "entity ids: [3157011, 4223535, 4486968, 2873216, 2069182, 2546010]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "with jsonlines.open(bootleg_label_file) as f: \n",
    "    for i, line in enumerate(f): \n",
    "        print('sentence:', line['sentence'])\n",
    "        print('mentions:', line['aliases'])\n",
    "        print('entity ids:', line['entity_ids'])\n",
    "        print()\n",
    "        if i == 5: \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the contextual entity embeddings, the static embeddings are not unique across mentions. For instance, if the same entity is predicted across two different mentions, the static entity embedding (and ids in the label file) will be the same for those mentions, whereas the contextual entity embeddings and ids will be different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also extract the embeddings through the `forward` pass on the embedding class. We will use random entity ids for demonstration.\n",
    "\n",
    "### Important: the `forward` pass will _normalize_ the embedding. Use the weight tensor above to not normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 200])\n"
     ]
    }
   ],
   "source": [
    "learned_emb_obj = model.module_pool.learned\n",
    "batch = 5\n",
    "M = 4\n",
    "K = 3\n",
    "eid_cands = torch.randint(0, 5000, (batch, M, K))\n",
    "# batch_on_the_fly_data is a dictionary used for KG metadata; keep it emtpy for extracting embeddings\n",
    "embs = learned_emb_obj(eid_cands, batch_on_the_fly_data={})\n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat the same process to extract the type embeddings. Our type embeddings are 128 dimensions.\n",
    "\n",
    "### Important: the type module `forward` will also _normalize_ and apply an additive attention mechanism to merge the multiple type embeddings for a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 3, 128])\n"
     ]
    }
   ],
   "source": [
    "wd_type_obj = model.module_pool.learned_type_wiki\n",
    "batch = 5\n",
    "M = 4\n",
    "K = 3\n",
    "eid_cands = torch.randint(0, 5000, (batch, M, K))\n",
    "embs = wd_type_obj(eid_cands, {})\n",
    "print(embs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}