{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import ujson\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict, OrderedDict\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tokens):\n",
    "    mapping = {'-LRB-': '(',\n",
    "                '-RRB-': ')',\n",
    "                '-LSB-': '[',\n",
    "                '-RSB-': ']',\n",
    "                '-LCB-': '{',\n",
    "                '-RCB-': '}'}\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] in mapping:\n",
    "            tokens[i] = mapping[tokens[i]]\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subj_obj(tokens, d):\n",
    "    ss, se = d['subj_start'], d['subj_end']\n",
    "    subj = tokens[ss:se+1]\n",
    "    subj = ' '.join(subj)\n",
    "    subj_span = \"{}:{}\".format(ss, se)\n",
    "    \n",
    "    os, oe = d['obj_start'], d['obj_end']\n",
    "    obj = tokens[os:oe+1]\n",
    "    obj = ' '.join(obj) #CHECK!\n",
    "    obj_span = \"{}:{}\".format(os, oe)\n",
    "    return subj, subj_span, obj, obj_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_file(train, dev, test, all_out, subjobj=False):\n",
    "    with open(train) as infile:\n",
    "        data_train = json.load(infile)\n",
    "        print(len(data_train))\n",
    "\n",
    "    with open(dev) as infile:\n",
    "        data_dev = json.load(infile)\n",
    "        print(len(data_dev))\n",
    "\n",
    "    with open(test) as infile:\n",
    "        data_test = json.load(infile)\n",
    "        print(len(data_test))\n",
    "\n",
    "    unq_id = 0\n",
    "    with open(all_out, 'w') as outfile:\n",
    "        for d in data_train:\n",
    "            tokens = d['token']\n",
    "            tokens = clean(tokens)\n",
    "            example = ' '.join(tokens)\n",
    "            subj, subj_span, obj, obj_span = extract_subj_obj(tokens, d)\n",
    "            entry = {\"sentence\": example, \n",
    "                     \"id\": d['id'], \n",
    "                     \"aliases\": [subj, obj], \n",
    "                     \"spans\": [subj_span, obj_span], \n",
    "                     \"sent_idx_unq\": unq_id\n",
    "                    } \n",
    "            json.save(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "            unq_id += 1\n",
    "        print(\"UNQ ID is: \", unq_id)\n",
    "        \n",
    "        for d in data_dev:\n",
    "            tokens = d['token']\n",
    "            tokens = clean(tokens)\n",
    "            example = ' '.join(tokens)\n",
    "            subj, subj_span, obj, obj_span = extract_subj_obj(tokens, d)\n",
    "            entry = {\"sentence\": example, \n",
    "                     \"id\": d['id'], \n",
    "                     \"aliases\": [subj, obj], \n",
    "                     \"spans\": [subj_span, obj_span], \n",
    "                     \"sent_idx_unq\": unq_id\n",
    "                    }            \n",
    "            json.save(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "            unq_id += 1\n",
    "        print(\"UNQ ID is: \", unq_id)\n",
    "\n",
    "        for d in data_test:\n",
    "            tokens = d['token']\n",
    "            tokens = clean(tokens)\n",
    "            example = ' '.join(tokens)\n",
    "            subj, subj_span, obj, obj_span = extract_subj_obj(tokens, d)\n",
    "            entry = {\"sentence\": example, \n",
    "                     \"id\": d['id'], \n",
    "                     \"aliases\": [subj, obj], \n",
    "                     \"spans\": [subj_span, obj_span], \n",
    "                     \"sent_idx_unq\": unq_id\n",
    "                    }\n",
    "            json.save(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "            unq_id += 1\n",
    "\n",
    "        print(\"UNQ ID is: \", unq_id)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68124\n",
      "22631\n",
      "15509\n",
      "UNQ ID is:  68124\n",
      "UNQ ID is:  90755\n",
      "UNQ ID is:  106264\n"
     ]
    }
   ],
   "source": [
    "expt_dir = '/dfs/scratch1/simran/tacred/tacred-relation-bootleg/dataset_bootleg_cidr_model/bootleg_09132020/subjobj_candjen_only_2/'\n",
    "source_path = '/dfs/scratch1/simran/tacred/tacred-relation-bootleg/dataset_bootleg_cidr_model/tacred/base_data/'\n",
    "inname_train = \"{}train.json\".format(source_path)\n",
    "inname_dev = \"{}dev_rev.json\".format(source_path)\n",
    "inname_test = \"{}test_rev.json\".format(source_path)\n",
    "outname_all = '{}candgen_prepped_for_bootinput.jsonl'.format(expt_dir)\n",
    "create_one_file(inname_train, inname_dev, inname_test, outname_all, subjobj = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL file example:\n",
    "\n",
    "\n",
    "# {\"sentence\": <sentence>,\n",
    "# \"aliases\": [<list of mentions you want to extract>],\n",
    "# \"spans\": [<list of word offsets for each alias>],\n",
    "# \"sent_idx_unq\": <unique sentence index>}\n",
    "\n",
    "# For examples\n",
    "# {\"sentence\": \"Barak enjoys walks on the CA beach with Michelle\",\n",
    "# \"aliases\": [\"barak\", \"ca\"],\n",
    "# \"spans\": [\"0:1\", \"5:6\"],\n",
    "# \"sent_idx_unq\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
