{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch1/simran/tutorial/contextual-embeddings/newenv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/dfs/scratch1/simran/tutorial/contextual-embeddings/newenv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /lfs/1/simran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import nltk \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import ast\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "### BOOTLEG ###\n",
    "# import import_ipynb\n",
    "# import LoadEntityProfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootleg utility functions:\n",
    "# BY ALIAS: \n",
    "def get_candidates(alias):\n",
    "    try:\n",
    "        # To get qid candidates of an alias\n",
    "        cands = LoadEntityProfiles.esp.get_qid_cands(alias)\n",
    "        return cands\n",
    "#         print(f\"Cands {cands}\")\n",
    "#         print([es.get_title(qid) for qid in cands])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual User Inputs\n",
    "model_base = '/dfs/scratch1/simran/tacred/tacred-relation-bootleg/saved_results/09082020-bootleg_BASE_dev_rev_ent.csv'\n",
    "model_alias = '/dfs/scratch1/simran/tacred/tacred-relation-bootleg/saved_results/09102020-095400_adding_manual_alias_dev_rev_ent.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TO_ID = {'no_relation': 0, 'per:title': 1, 'org:top_members/employees': 2, 'per:employee_of': 3, \n",
    "               'org:alternate_names': 4, 'org:country_of_headquarters': 5, 'per:countries_of_residence': 6, \n",
    "               'org:city_of_headquarters': 7, 'per:cities_of_residence': 8, 'per:age': 9, \n",
    "               'per:stateorprovinces_of_residence': 10, 'per:origin': 11, 'org:subsidiaries': 12, \n",
    "               'org:parents': 13, 'per:spouse': 14, 'org:stateorprovince_of_headquarters': 15, 'per:children': 16, \n",
    "               'per:other_family': 17, 'per:alternate_names': 18, 'org:members': 19, 'per:siblings': 20, \n",
    "               'per:schools_attended': 21, 'per:parents': 22, 'per:date_of_death': 23, 'org:member_of': 24, \n",
    "               'org:founded_by': 25, 'org:website': 26, 'per:cause_of_death': 27, \n",
    "               'org:political/religious_affiliation': 28, 'org:founded': 29, 'per:city_of_death': 30, \n",
    "               'org:shareholders': 31, 'org:number_of_employees/members': 32, 'per:date_of_birth': 33, \n",
    "               'per:city_of_birth': 34, 'per:charges': 35, 'per:stateorprovince_of_death': 36, 'per:religion': 37, \n",
    "               'per:stateorprovince_of_birth': 38, 'per:country_of_birth': 39, 'org:dissolved': 40, \n",
    "               'per:country_of_death': 41}\n",
    "\n",
    "LABEL_LST = list(LABEL_TO_ID.keys())\n",
    "\n",
    "STANFORD_NER_TYPES = ['DATE', 'LOCATION', 'MONEY', 'ORGANIZATION', 'PERCENT', 'PERSON', 'TIME']\n",
    "punctuation = [',', ':', '.', ';', \"'\", ')', '(', \"'s\", '--', '-', '``', \"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL_model size:  (22631, 27)\n",
      "ERRS_model size:  (2596, 27)\n",
      "FULL_model_var size:  (22631, 26)\n",
      "ERRS_model_var size:  (2595, 27)\n"
     ]
    }
   ],
   "source": [
    "# Load the model data \n",
    "df_results_base = pd.read_csv(model_base)\n",
    "df_results_var = pd.read_csv(model_alias)\n",
    "\n",
    "# add var predictions to df_results_base\n",
    "df_results_base.rename(columns={'prediction':'prediction_base'}, inplace=True)\n",
    "map_id_pred = {}\n",
    "for ind, row in df_results_var.iterrows():\n",
    "    map_id_pred[row['id']] = row['prediction']\n",
    "df_results_base['prediction_var'] = df_results_base['id'].map(map_id_pred)\n",
    "\n",
    "# error rates\n",
    "df_errors_base = df_results_base[df_results_base['relation'] != df_results_base['prediction_base']]\n",
    "df_errors_var = df_results_base[df_results_base['relation'] != df_results_base['prediction_var']]\n",
    "\n",
    "print(\"FULL_model size: \", df_results_base.shape)\n",
    "print(\"ERRS_model size: \", df_errors_base.shape)\n",
    "\n",
    "print(\"FULL_model_var size: \", df_results_var.shape)\n",
    "print(\"ERRS_model_var size: \", df_errors_var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions: accepts a df with corrected slices, and outputs the predicted result\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "def score_corrections(df):\n",
    "    import scorer\n",
    "    scorer.score(df['relation_model'].tolist(), df['prediction_model'].tolist(), verbose=True)\n",
    "    df_errs = df[df['relation_model'] != df['prediction_model']]\n",
    "    print(\"Number of Errors: \", df_errs.shape[0])\n",
    "# score_corrections(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic error rate by relation \n",
    "def error_rate_by_relation(df_r, key, df_e):\n",
    "    relation_df = pd.DataFrame(columns=['relation','error_rate','error_count','total_count'])\n",
    "    index = 0\n",
    "    for k, v in LABEL_TO_ID.items():\n",
    "        df_relation_tot = df_r[df_r[key] == k] # all rows with k = true trelation\n",
    "        tot = df_relation_tot.shape[0] # number of examples with this true relation\n",
    "    \n",
    "        df_relation_err = df_e[df_e[key] == k] # error rows with k = true trelation\n",
    "        err = df_relation_err.shape[0] # number of errors with this true relation\n",
    "        error_rate = err/tot\n",
    "    \n",
    "        relation_df.loc[index] = pd.Series({'relation':k, 'error_rate':error_rate, 'error_count':err, 'total_count':tot})\n",
    "        index += 1\n",
    "    \n",
    "    print(relation_df.sort_values('relation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['obj', 'obj_mentions', 'subj_qids', 'prop_mentions', 'real_mentions',\n",
       "       'subj_type', 'obj_type', 'id', 'subj_pos', 'subj_leng', 'obj_qids',\n",
       "       'subj_mentions', 'qids', 'obj_leng', 'obj_ner', 'stanford_ner',\n",
       "       'subj_ner', 'prediction_base', 'subj', 'example', 'mentions',\n",
       "       'separation_dist', 'prop_ner', 'obj_pos', 'relation', 'num_ner',\n",
       "       'prediction_var'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errors_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 27)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 27)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = df_results_base[df_results_base['example'].str.contains('organisation')]\n",
    "print(sub_df.shape)\n",
    "sub_df = df_errors_base[df_errors_base['example'].str.contains('organisation')]\n",
    "sub_df.shape\n",
    "# print(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5, 27)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df = df_results_var[df_results_var['example'].str.contains('organisation')]\n",
    "print(sub_df.shape)\n",
    "sub_df = df_errors_var[df_errors_var['example'].str.contains('organisation')]\n",
    "sub_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate_names1 = {}\n",
    "\n",
    "# for index, row in df_errors_var.iterrows():\n",
    "#     if row['relation'] in 'org:alternate_names':\n",
    "#         add = 0\n",
    "#         tokens = row['example'].split(' ')\n",
    "    \n",
    "#         ss, se = row['subj_start'], row['subj_end']\n",
    "#         subj = tokens[ss:se+1]\n",
    "#         subj_qid = row['ent_id'][ss:se+1]\n",
    "#         subj_ner = row['stanford_ner'][ss:se+1]\n",
    "        \n",
    "        \n",
    "#         os, oe = row['obj_start'], row['obj_end']\n",
    "#         obj = tokens[os:oe+1]\n",
    "#         obj_qid = row['ent_id'][os: oe+1]\n",
    "#         obj_ner = row['stanford_ner'][os: oe+1]\n",
    "        \n",
    "        \n",
    "#         if len(subj) == 1 and subj_ner[0] == 'ORGANIZATION':\n",
    "#             if subj_qid[0] != obj_qid[0]:\n",
    "#                 add = 1\n",
    "#         elif len(obj) == 1 and obj_ner[0] == 'ORGANIZATION':\n",
    "#             if subj_qid[0] != obj_qid[0]:\n",
    "#                 add = 1\n",
    "        \n",
    "#         if add == 1:\n",
    "#             alternate_names1[row['id']] = {'subj':subj, 'subj_qid':subj_qid, 'obj':obj, 'obj_qid':obj_qid, 'example':tokens}\n",
    "#             print(subj, subj_qid, obj, obj_qid)\n",
    "#             print(row['example'])\n",
    "#             print()\n",
    "                    \n",
    "# print(len(alternate_names1.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_rate_by_relation(df_results, 'relation_model', df_errors)\n",
    "relation_df = pd.DataFrame(columns=['relation','errrate_var','errcount_var', 'errrate_base','errcount_base','total_count'])\n",
    "index = 0\n",
    "for k, v in LABEL_TO_ID.items():\n",
    "        df_relation_tot_var = df_results_var[df_results_var['relation'] == k] # all rows with k = true trelation\n",
    "        tot_var = df_relation_tot_var.shape[0] # number of examples with this true relation\n",
    "        if tot_var > 0:\n",
    "    \n",
    "            df_relation_err_var = df_errors_var[df_errors_var['relation'] == k] # error rows with k = true trelation\n",
    "            err_var = df_relation_err_var.shape[0] # number of errors with this true relation\n",
    "            error_rate_var = err_var/tot_var\n",
    "\n",
    "            df_relation_tot_base = df_results_base[df_results_base['relation'] == k] # all rows with k = true trelation\n",
    "            tot_base = df_relation_tot_base.shape[0] # number of examples with this true relation\n",
    "\n",
    "            df_relation_err_base = df_errors_base[df_errors_base['relation'] == k] # error rows with k = true trelation\n",
    "            err_base = df_relation_err_base.shape[0] # number of errors with this true relation\n",
    "            error_rate_base = err_base/tot_base\n",
    "\n",
    "            relation_df.loc[index] = pd.Series({'relation':k, 'errrate_var':error_rate_var, 'errcount_var':err_var, 'errrate_base':error_rate_base,'errcount_base': err_base, 'total_count':tot_var})\n",
    "            index += 1\n",
    "    \n",
    "print(relation_df.sort_values('relation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the examples our model gets, but the baseline (vanilla) model misses, why does bootleg help?\n",
    "\n",
    "We want to confirm that the improvement ties back to insights in bootleg - i.e., the mentions/types/relations in bootleg's database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The #examples df_missed_by_variation only are:  689\n",
      "The #examples df_missed_by_base only are:  814\n"
     ]
    }
   ],
   "source": [
    "# get the set difference of errors\n",
    "df_missed_by_variation = df_errors_var[~df_errors_var['id'].isin(df_errors_base['id'])]\n",
    "df_missed_by_base = df_errors_base[~df_errors_base['id'].isin(df_errors_var['id'])]\n",
    "print(\"The #examples df_missed_by_variation only are: \", df_missed_by_variation.shape[0])\n",
    "print(\"The #examples df_missed_by_base only are: \", df_missed_by_base.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE DISTRIBUTION of RELATIONS WHEN BASE IMPROVES OVER VARIATION\n",
      "Counter({'no_relation': 268, 'per:date_of_death': 57, 'per:title': 50, 'per:employee_of': 40, 'per:age': 21, 'per:cities_of_residence': 20, 'per:countries_of_residence': 18, 'org:top_members/employees': 18, 'org:country_of_headquarters': 18, 'org:members': 15, 'org:subsidiaries': 15, 'org:parents': 14, 'per:origin': 12, 'per:city_of_death': 12, 'per:charges': 11, 'per:spouse': 10, 'per:children': 9, 'org:city_of_headquarters': 9, 'org:number_of_employees/members': 9, 'per:cause_of_death': 9, 'per:stateorprovinces_of_residence': 8, 'per:religion': 7, 'per:stateorprovince_of_death': 6, 'org:stateorprovince_of_headquarters': 5, 'per:schools_attended': 4, 'org:shareholders': 4, 'per:parents': 4, 'org:alternate_names': 3, 'per:other_family': 3, 'per:stateorprovince_of_birth': 2, 'org:political/religious_affiliation': 2, 'per:city_of_birth': 2, 'per:date_of_birth': 1, 'per:alternate_names': 1, 'per:siblings': 1, 'per:country_of_birth': 1})\n",
      "\n",
      " THE DISTRIBUTION OF RELATIONS WHEN VARIATION IMPROVES OVER BASE\n",
      "Counter({'no_relation': 592, 'per:employee_of': 26, 'org:top_members/employees': 23, 'org:alternate_names': 20, 'per:title': 14, 'org:city_of_headquarters': 13, 'org:country_of_headquarters': 9, 'per:origin': 9, 'per:countries_of_residence': 8, 'per:cities_of_residence': 8, 'per:stateorprovinces_of_residence': 7, 'per:cause_of_death': 6, 'per:city_of_death': 6, 'org:stateorprovince_of_headquarters': 6, 'org:shareholders': 5, 'org:subsidiaries': 5, 'per:parents': 5, 'per:stateorprovince_of_death': 5, 'org:parents': 5, 'per:charges': 5, 'org:founded_by': 4, 'per:spouse': 4, 'per:city_of_birth': 4, 'per:children': 3, 'per:religion': 3, 'per:date_of_birth': 3, 'per:age': 2, 'per:schools_attended': 2, 'org:founded': 2, 'per:siblings': 2, 'per:date_of_death': 2, 'org:website': 1, 'per:alternate_names': 1, 'per:stateorprovince_of_birth': 1, 'org:political/religious_affiliation': 1, 'per:other_family': 1, 'org:members': 1})\n"
     ]
    }
   ],
   "source": [
    "# THE DISTRIBUTION of RELATIONS WHEN MODEL IMPOVES OVER SPANBERT\n",
    "print(\"THE DISTRIBUTION of RELATIONS WHEN BASE IMPROVES OVER VARIATION\")\n",
    "relations_errs_matchqids = []\n",
    "for index, row in df_missed_by_variation.iterrows():\n",
    "    relations_errs_matchqids.append(row['relation'])\n",
    "print(Counter(relations_errs_matchqids))\n",
    "print()\n",
    "\n",
    "# THE DISTRIBUTION OF RELATIONS WHEN SPANBERT IMPROVES OVER MODEL\n",
    "print(\" THE DISTRIBUTION OF RELATIONS WHEN VARIATION IMPROVES OVER BASE\")\n",
    "relations_errs_matchqids = []\n",
    "for index, row in df_missed_by_base.iterrows():\n",
    "    relations_errs_matchqids.append(row['relation'])\n",
    "print(Counter(relations_errs_matchqids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subj obj missed on variation\n",
      "[(\"['his']\", 206), (\"['he']\", 198), (\"['her']\", 51), (\"['maria', 'kaczynska']\", 47), (\"['julius', 'baer']\", 39), (\"['access', 'industries']\", 39), (\"['burlington', 'northern', 'santa', 'fe', 'corp.']\", 36), (\"['she']\", 36), (\"['eco']\", 34), (\"['us']\", 33), (\"['global', 'infrastructure', 'partners']\", 32), (\"['arcandor']\", 31), (\"['mohammed', 'sayed', 'tantawi']\", 31), (\"['galleon', 'group']\", 31), (\"['him']\", 30), (\"['eta']\", 30), (\"['girija', 'prasad', 'koirala']\", 29), (\"['france']\", 29), (\"['millipore']\", 28), (\"['sasac']\", 27), (\"['thomas', 'more', 'law', 'center']\", 27), (\"['russia']\", 27), (\"['stuart', 'rose']\", 26), (\"['bipartisan', 'policy', 'center']\", 26), (\"['paul', 'gray']\", 22), (\"['rosoboronexport']\", 21), (\"['kissel']\", 21), (\"['election', 'complaints', 'commission']\", 20), (\"['public', 'library', 'of', 'science']\", 20), (\"['united', 'states']\", 19), (\"['abdul', 'aziz', 'al-hakim']\", 19), (\"['anwar', 'chowdhry']\", 19), (\"['nuclear', 'energy', 'institute']\", 19), (\"['u.s.']\", 18), (\"['hwang']\", 18), (\"['len', 'blavatnik']\", 18), (\"['independent']\", 17), (\"['orlando', 'zapata']\", 17), (\"['clifton']\", 17), (\"['douglas', 'flint']\", 17), (\"['escada']\", 17), (\"['dcns']\", 17), (\"['adam', 'gadahn']\", 17), (\"['american', 'association', 'for', 'the', 'advancement', 'of', 'science']\", 17), (\"['gwathmey']\", 16), (\"['stockholm', 'international', 'water', 'institute']\", 16), (\"['new', 'york']\", 16), (\"['patricia', 'neal']\", 16), (\"['ecc']\", 16), (\"['china']\", 16), (\"['bashardost']\", 16), (\"['pakistan']\", 16), (\"['wednesday']\", 16), (\"['washington', 'national', 'opera']\", 16), (\"['al-hakim']\", 16), (\"['adam', 'goldstein']\", 16), (\"['mikel', 'karrera', 'sarobe']\", 16), (\"['freedom', 'communications']\", 15), (\"['united', 'steelworkers']\", 15), (\"['hewitt']\", 14)] \n",
      "\n",
      "\n",
      "Subj obj missed on base\n",
      "[(\"['he']\", 219), (\"['his']\", 213), (\"['maria', 'kaczynska']\", 55), (\"['julius', 'baer']\", 49), (\"['access', 'industries']\", 49), (\"['her']\", 46), (\"['she']\", 36), (\"['galleon', 'group']\", 34), (\"['mohammed', 'sayed', 'tantawi']\", 33), (\"['him']\", 33), (\"['thomas', 'more', 'law', 'center']\", 33), (\"['u.s.']\", 32), (\"['eco']\", 32), (\"['russia']\", 32), (\"['girija', 'prasad', 'koirala']\", 32), (\"['us']\", 30), (\"['stuart', 'rose']\", 29), (\"['bipartisan', 'policy', 'center']\", 29), (\"['arcandor']\", 28), (\"['burlington', 'northern', 'santa', 'fe', 'corp.']\", 27), (\"['rosoboronexport']\", 26), (\"['france']\", 26), (\"['eta']\", 26), (\"['american', 'association', 'for', 'the', 'advancement', 'of', 'science']\", 25), (\"['millipore']\", 24), (\"['kissel']\", 23), (\"['sasac']\", 23), (\"['global', 'infrastructure', 'partners']\", 23), (\"['anwar', 'chowdhry']\", 22), (\"['douglas', 'flint']\", 22), (\"['mikel', 'karrera', 'sarobe']\", 21), (\"['tahawwur', 'hussain', 'rana']\", 20), (\"['gwathmey']\", 20), (\"['nuclear', 'energy', 'institute']\", 20), (\"['new', 'york']\", 19), (\"['election', 'complaints', 'commission']\", 19), (\"['len', 'blavatnik']\", 19), (\"['washington', 'national', 'opera']\", 19), (\"['united', 'states']\", 18), (\"['president']\", 18), (\"['pakistan']\", 18), (\"['iarc']\", 18), (\"['orlando', 'zapata']\", 18), (\"['adam', 'goldstein']\", 18), (\"['zimbabwe']\", 18), (\"['china']\", 17), (\"['escada']\", 17), (\"['freedom', 'communications']\", 17), (\"['al-hakim']\", 17), (\"['hwang']\", 17), (\"['aaas']\", 16), (\"['china', 'banking', 'regulatory', 'commission']\", 16), (\"['public', 'library', 'of', 'science']\", 16), (\"['clifton']\", 16), (\"['hewitt']\", 16), (\"['pekar']\", 16), (\"['american']\", 16), (\"['mcgregor']\", 16), (\"['stockholm', 'international', 'peace', 'research', 'institute']\", 15), (\"['iran']\", 15)]\n"
     ]
    }
   ],
   "source": [
    "# Just for subj and obj\n",
    "missed_words_lst = []\n",
    "print(\"Subj obj missed on variation\")\n",
    "for index, row in df_errors_var.iterrows():\n",
    "    subj = row['subj']\n",
    "    missed_words_lst.append(subj)\n",
    "    obj = row['obj']\n",
    "    missed_words_lst.append(obj)\n",
    "        \n",
    "missed_words = Counter(missed_words_lst).most_common(60)\n",
    "print(missed_words, '\\n\\n')\n",
    "\n",
    "print(\"Subj obj missed on base\")\n",
    "missed_words_lst = []\n",
    "for index, row in df_errors_base.iterrows():\n",
    "    subj = row['subj']\n",
    "    missed_words_lst.append(subj)\n",
    "    obj = row['obj']\n",
    "    missed_words_lst.append(obj)\n",
    "        \n",
    "missed_words = Counter(missed_words_lst).most_common(60)\n",
    "print(missed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed on var\n",
      "[('merck', 55), ('saturday', 49), ('fe', 48), ('partners', 47), ('part', 46), ('afghanistan', 45), ('killed', 45), ('infrastructure', 43), ('announced', 43), ('music', 43), ('assets', 42), ('eco', 42), ('ali', 42), ('private', 41), ('buffett', 41), ('career', 41), ('business', 41), ('san', 41), ('companies', 40), ('age', 40), ('gadahn', 40), ('like', 40), ('brother', 39), ('&', 39)]\n",
      "missed on base\n",
      "[('several', 55), ('10', 48), ('writer', 46), ('worked', 46), ('afp', 45), ('air', 45), ('zapata', 44), ('poland', 44), ('indian', 44), ('management', 43), ('haig', 43), ('television', 43), ('later', 42), ('heart', 42), ('manager', 42), ('rana', 41), ('financial', 41), ('day', 41), ('general', 41), ('prasad', 41), ('saudi', 40), ('based', 40), ('official', 40), ('de', 40)]\n"
     ]
    }
   ],
   "source": [
    "# for any word, not just subj and obj\n",
    "missed_words_lst = []\n",
    "for index, row in df_errors_var.iterrows():\n",
    "    tokens = row['example'].split(' ')\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token not in punctuation:\n",
    "            missed_words_lst.append(token)       \n",
    "missed_words_var = Counter(missed_words_lst).most_common(200)\n",
    "lst_missed_var = [tup[0] for tup in missed_words_var]\n",
    "\n",
    "missed_words_lst = []\n",
    "for index, row in df_errors_base.iterrows():\n",
    "    tokens = row['example'].split(' ')\n",
    "    for token in tokens:\n",
    "        if token not in stop_words and token not in punctuation:\n",
    "            missed_words_lst.append(token)\n",
    "missed_words_base = Counter(missed_words_lst).most_common(200)\n",
    "lst_missed_base = [tup[0] for tup in missed_words_base]\n",
    "\n",
    "print(\"missed on var\")\n",
    "#print(missed_words_var, '\\n\\n')\n",
    "print([tup for tup in missed_words_var if tup[0] not in lst_missed_base])\n",
    "print(\"missed on base\")\n",
    "#print(missed_words_base)\n",
    "print([tup for tup in missed_words_base if tup[0] not in lst_missed_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed on var\n",
      "PRED:  [('no_relation', 18), ('org:parents', 9), ('org:top_members/employees', 5), ('org:subsidiaries', 3), ('org:shareholders', 2), ('org:city_of_headquarters', 1)] \n",
      "\n",
      "TRUE:  [('no_relation', 19), ('org:parents', 12), ('org:country_of_headquarters', 3), ('org:city_of_headquarters', 2), ('org:shareholders', 1), ('org:subsidiaries', 1)]\n",
      "\n",
      "\n",
      "\n",
      "missed on base\n",
      "PRED:  [('no_relation', 16), ('org:parents', 7), ('org:city_of_headquarters', 2), ('org:top_members/employees', 2), ('org:subsidiaries', 1), ('org:shareholders', 1)] \n",
      "\n",
      "TRUE:  [('no_relation', 14), ('org:parents', 9), ('org:city_of_headquarters', 3), ('org:shareholders', 1), ('org:subsidiaries', 1), ('org:country_of_headquarters', 1)]\n"
     ]
    }
   ],
   "source": [
    "# inspect errors for examples with a particular word\n",
    "inspect_word = 'merck'\n",
    "mispreds = []\n",
    "truerels = []\n",
    "for index, row in df_errors_var.iterrows():\n",
    "    tokens = row['example'].split(' ')\n",
    "    if inspect_word in tokens:\n",
    "        mispreds.append(row['prediction_var']) \n",
    "        truerels.append(row['relation'])\n",
    "mispreds_var = Counter(mispreds).most_common(20)\n",
    "truerels_var = Counter(truerels).most_common(20)\n",
    "print(\"missed on var\")\n",
    "print(\"PRED: \", mispreds_var, '\\n')\n",
    "print(\"TRUE: \", truerels_var)\n",
    "\n",
    "mispreds = []\n",
    "truerels = []\n",
    "for index, row in df_errors_base.iterrows():\n",
    "    tokens = row['example'].split(' ')\n",
    "    if inspect_word in tokens:\n",
    "        mispreds.append(row['prediction_var']) \n",
    "        truerels.append(row['relation'])\n",
    "mispreds_base = Counter(mispreds).most_common(20)\n",
    "truerels_base = Counter(truerels).most_common(20)\n",
    "print(\"\\n\\n\\nmissed on base\")\n",
    "print(\"PRED: \", mispreds_base, '\\n')\n",
    "print(\"TRUE: \", truerels_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the error examples with pronouns,  249  have just one person ner tag in them.\n",
      "Of the error examples with pronouns,  142  have subj or obj as a pronoun\n"
     ]
    }
   ],
   "source": [
    "# pronoun cooccurrence \n",
    "num_one_person_var = 0\n",
    "subj_obj_pronoun = 0\n",
    "pronouns = ['he', 'she', 'her', 'his', 'him']\n",
    "for index, row in df_errors_base.iterrows():\n",
    "    tokens = row['example'].split(' ')\n",
    "    if any(pronoun for pronoun in pronouns if pronoun in tokens):\n",
    "        ner_tags = row['stanford_ner']\n",
    "        ner_tags = ast.literal_eval(ner_tags)\n",
    "        if len([tag for tag in ner_tags if tag==\"PERSON\"]) == 1:\n",
    "#             print(row[['example', 'subj', 'obj']])\n",
    "#             print()\n",
    "            num_one_person_var += 1\n",
    "            if any(pronoun for pronoun in pronouns if pronoun in row['subj'] or pronoun in row['obj']):\n",
    "                subj_obj_pronoun += 1\n",
    "print(\"Of the error examples with pronouns, \", num_one_person_var, \" have just one person ner tag in them.\")\n",
    "print(\"Of the error examples with pronouns, \", subj_obj_pronoun, \" have subj or obj as a pronoun\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the subj and obj qid match this tends to indicate alternate names --- bootleg doesn't do well on alternate names --- how does variation do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_errors = 0\n",
    "relations_errs_matchqids = []\n",
    "for index, row in df_errors_var.iterrows():\n",
    "    # subj qid\n",
    "    subj_qids_str = row['subj_qids']\n",
    "    subj_qids = ast.literal_eval(subj_qids_str)\n",
    "    subj_qid = subj_qids[0]\n",
    "    \n",
    "    # obj qid\n",
    "    obj_qids_str = row['obj_qids']\n",
    "    obj_qids = ast.literal_eval(obj_qids_str)\n",
    "    obj_qid = obj_qids[0]\n",
    "    \n",
    "    if subj_qid == obj_qid and subj_qid != 'UNK':\n",
    "        count_errors += 1\n",
    "        relations_errs_matchqids.append(row['relation'])\n",
    "count_errors\n",
    "print(Counter(relations_errs_matchqids))\n",
    "\n",
    "count_results = 0\n",
    "relations_matchqids = []\n",
    "for index, row in df_results_base.iterrows():\n",
    "    # subj qid\n",
    "    subj_qids_str = row['subj_qids']\n",
    "    subj_qids = ast.literal_eval(subj_qids_str)\n",
    "    subj_qid = subj_qids[0]\n",
    "    \n",
    "    # obj qid\n",
    "    obj_qids_str = row['obj_qids']\n",
    "    obj_qids = ast.literal_eval(obj_qids_str)\n",
    "    obj_qid = obj_qids[0]\n",
    "    \n",
    "    if subj_qid == obj_qid and subj_qid != 'UNK':\n",
    "        count_results += 1\n",
    "        relations_matchqids.append(row['relation'])\n",
    "        \n",
    "print(Counter(relations_matchqids))\n",
    "count_results\n",
    "\n",
    "error_rate_orig = df_errors_var.shape[0]/df_results_base.shape[0]\n",
    "error_rate_new = (df_errors_var.shape[0]-count_errors)/df_results_base.shape[0]\n",
    "print(\"original number of errors: \", df_errors_var.shape[0])\n",
    "print(\"total instances of two matching qids subj and obj: \", count_results)\n",
    "print(\"corrected errors: \", count_errors)\n",
    "print(error_rate_orig)\n",
    "print(error_rate_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look errors based on bootleg relations between the sub/obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constants\n",
    "proper_noun = ['NNP', 'NNPS']\n",
    "nonproper_noun = ['NN', 'NNS']\n",
    "number_pos = ['CD']\n",
    "nomention = [\"['UNK']\", \"['UNK', 'UNK']\", \"['UNK', 'UNK', 'UNK']\", \"['UNK', 'UNK', 'UNK', 'UNK']\", \n",
    "             \"['UNK', 'UNK', 'UNK', 'UNK', 'UNK']\"]\n",
    "cols = ['example', 'relation','prediction_base', 'prediction_var', 'mentions', 'subj_pos', 'obj_pos', 'subj', 'obj', 'subj_mentions', 'subj_qids', 'obj_mentions', 'obj_qids', 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations(subj_qids_str, obj_qids_str):\n",
    "    subj_qids = ast.literal_eval(subj_qids_str)\n",
    "    obj_qids = ast.literal_eval(obj_qids_str)\n",
    "    subj_qid = subj_qids[0]\n",
    "    obj_qid = obj_qids[0]\n",
    "    \n",
    "    rels = [LoadEntityProfiles.esp.get_relation_name(r) for r in LoadEntityProfiles.esp.get_all_relations(subj_qid, obj_qid)]\n",
    "    rels = [LoadEntityProfiles.rel_to_name.get(rel, rel) for rel in rels]\n",
    "    return rels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_entities(qid):\n",
    "    # Get all connected qids for a given qid\n",
    "    related_qids = LoadEntityProfiles.du.get_related_qids(qid, LoadEntityProfiles.esp)\n",
    "    print(f\"Related QIDs {related_qids}\")\n",
    "    print([LoadEntityProfiles.es.get_title(qid) for qid in related_qids])\n",
    "\n",
    "    lst = [LoadEntityProfiles.es.get_title(qid) for qid in related_qids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst_pos_subj_var = []\n",
    "lst_subj_nomention_var = {}\n",
    "lst_pos_obj_var = []\n",
    "lst_obj_nomention_var = {}\n",
    "\n",
    "sub_df = df_missed_by_variation[cols]\n",
    "for index, row in sub_df.iterrows():\n",
    "    if any(pos in row['subj_pos'] for pos in proper_noun): # is the subj a proper noun?\n",
    "        lst_pos_subj_var.append(row['subj'])\n",
    "        if any(null in row['subj_mentions'] for null in nomention): # is the bootleg mention empty for this proper noun?\n",
    "            lst_subj_nomention_var[row['id']] = row['subj']\n",
    "            \n",
    "    if any(pos in row['obj_pos'] for pos in proper_noun):\n",
    "        lst_pos_obj_var.append(row['obj'])\n",
    "        if any(null in row['obj_mentions'] for null in nomention):\n",
    "            lst_obj_nomention_var[row['id']] = row['obj']\n",
    "        \n",
    "print(\"The number of proper noun subj in var errors are:\", len(lst_pos_subj_var), \"and obj are:\", len(lst_pos_obj_var))\n",
    "print(\"The number of proper noun subj in var errors that don't get a bootleg mention are:\", len(lst_subj_nomention_var.keys()), \"and obj are:\", len(lst_obj_nomention_var.keys()))\n",
    "print()\n",
    "\n",
    "lst_pos_subj_var.extend(lst_pos_obj_var)\n",
    "print(\"var DOES WORSE THAN BOOTLEG base ON THESE SUBJ/OBJ PROPER NOUNS, THIS MANY TIMES:\")#print(len(lst_pos_subj_var))\n",
    "worst_for_var = Counter(lst_pos_subj_var).most_common(50)\n",
    "print(worst_for_var)\n",
    "worst_for_var = [tup[0] for tup in worst_for_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting examples where bootleg had any relation for the subj, obj pair\n",
    "\n",
    "count = 0\n",
    "for index, row in df_missed_by_variation.iterrows():\n",
    "    if get_relations(row['subj_qids'], row['obj_qids']):\n",
    "        count += 1\n",
    "print(\"For \", count, \" var errors, bootleg has *some* relation between the subj and obj\")\n",
    "\n",
    "count = 0\n",
    "for index, row in df_missed_by_variation.iterrows():\n",
    "    rels = get_relations(row['subj_qids'], row['obj_qids'])\n",
    "    if rels and not any(rel for rel in rels if rel in row['relation']):\n",
    "        print(row[['example', 'subj', 'obj', 'subj_qids', 'obj_qids', 'relation', 'prediction_base', 'prediction_var', 'mentions']])\n",
    "        print(f\"Rels {rels}\")\n",
    "        print()\n",
    "        count += 1\n",
    "\n",
    "print(\"For \", count, \" var errors, the existing bootleg relation is NOT a subset of the gold relation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting examples where bootleg had a relation for the subj, obj pair\n",
    "# booterrors_with_relation_df = df_errors[df_errors['id'].isin(ids_missed_propernoun_with_bootrels)]\n",
    "count = 0\n",
    "for index, row in df_missed_by_variation.iterrows():\n",
    "    rels = get_relations(row['subj_qids'], row['obj_qids'])\n",
    "    if rels and any(rel for rel in rels if rel in row['relation']):\n",
    "        print(row[['example', 'subj', 'obj', 'subj_qids', 'obj_qids', 'relation', 'prediction_base', 'prediction_var', 'mentions']])\n",
    "        print(f\"Rels {rels}\")\n",
    "        print()\n",
    "        count += 1\n",
    "        \n",
    "print(\"For \", count, \" spanbert errors, the bootleg relation is a subset (or the same) as the gold relation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting examples where bootleg had a relation for the subj, obj pair\n",
    "# booterrors_with_relation_df = df_errors[df_errors['id'].isin(ids_missed_propernoun_with_bootrels)]\n",
    "count = 0\n",
    "for index, row in df_missed_by_variation.iterrows():\n",
    "    rels = get_relations(row['subj_qids'], row['obj_qids'])\n",
    "    if rels and any(rel for rel in rels if rel in row['prediction_var']):\n",
    "        print(row[['example', 'subj', 'obj', 'subj_qids', 'obj_qids', 'relation', 'prediction_base', 'prediction_var', 'mentions']])\n",
    "        print(f\"Rels {rels}\")\n",
    "        print()\n",
    "        count += 1\n",
    "        \n",
    "print(\"For \", count, \" var errors, the bootleg relation is a subset (or the same) as the var prediction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR EXAMPLES WHERE BASE IS BETTER THAN VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_pos_subj_model = []\n",
    "lst_subj_nomention_model = {}\n",
    "lst_pos_obj_model = []\n",
    "lst_obj_nomention_model= {}\n",
    "\n",
    "sub_df = df_missed_by_base[cols]\n",
    "for index, row in sub_df.iterrows():\n",
    "    if any(pos in row['subj_pos'] for pos in proper_noun): # is the subj a proper noun?\n",
    "        lst_pos_subj_model.append(row['subj'])\n",
    "        if any(null in row['subj_mentions'] for null in nomention): # is the bootleg mention empty for this proper noun?\n",
    "            lst_subj_nomention_model[row['id']] = row['subj']\n",
    "            \n",
    "    if any(pos in row['obj_pos'] for pos in proper_noun):\n",
    "        lst_pos_obj_model.append(row['obj'])\n",
    "        if any(null in row['obj_mentions'] for null in nomention):\n",
    "            lst_obj_nomention_model[row['id']] = row['obj']\n",
    "        \n",
    "print(\"The number of proper noun subj in base (&not var) errors are:\", len(lst_pos_subj_var), \"and obj are:\", len(lst_pos_obj_var))\n",
    "print(\"The number of proper noun subj in base (&not var) errors that don't get a bootleg mention are:\", len(lst_subj_nomention_var.keys()), \"and obj are:\", len(lst_obj_nomention_var.keys()))\n",
    "print()\n",
    "\n",
    "lst_pos_subj_model.extend(lst_pos_obj_model)\n",
    "print(\"var DOES BETTER THAN base MODEL ON THESE SUBJ/OBJ PROPER NOUNS\")#print(len(lst_pos_subj_spanbert))\n",
    "worst_for_model = Counter(lst_pos_subj_model).most_common(50)\n",
    "print(worst_for_model)\n",
    "worst_for_model = [tup[0] for tup in worst_for_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_df = df_model_not_spanbert_errors[cols] #BOOTLEG ERRORS, and NOT BASELINE ERRORS\n",
    "count_missed_propernoun = 0\n",
    "count_missed_propernoun_with_bootrels = 0\n",
    "ids_missed_propernoun_with_bootrels = []\n",
    "for index, row in sub_df.iterrows():\n",
    "    if any(pos in row['subj_pos'] for pos in proper_noun) or any(pos in row['obj_pos'] for pos in proper_noun):\n",
    "#        print(row[['example', 'subj', 'obj', 'subj_qids', 'obj_qids', 'relation', 'prediction_model', 'prediction_spanbert', 'mentions']])\n",
    "        rels = get_relations(row['subj_qids'], row['obj_qids'])\n",
    "        if rels:\n",
    "            count_missed_propernoun_with_bootrels += 1\n",
    "            ids_missed_propernoun_with_bootrels.append(row['id'])\n",
    "#         print(f\"Rels {rels}\")\n",
    "#         print()\n",
    "        count_missed_propernoun += 1\n",
    "        \n",
    "print(\"Number of examples where bootleg had a relation for the subj, obj pair: \", count_missed_propernoun_with_bootrels)\n",
    "print(\"Number of examples where subj and/or obj is a proper noun: \", count_missed_propernoun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspecting examples where bootleg had any relation for the subj, obj pair\n",
    "# booterrors_with_relation_df = df_errors[df_errors['id'].isin(ids_missed_propernoun_with_bootrels)]\n",
    "count = 0\n",
    "for index, row in df_results.iterrows():\n",
    "    if get_relations(row['subj_qids'], row['obj_qids']):\n",
    "        count += 1\n",
    "print(\"For \", count, \" bootleg errors, bootleg has *some* relation between the subj and obj\\n\")\n",
    "\n",
    "\n",
    "print(\"For these bootleg errors, the existing bootleg relation is NOT a string-subset of the gold relation\")\n",
    "count = 0\n",
    "for index, row in df_results.iterrows():\n",
    "    rels = get_relations(row['subj_qids'], row['obj_qids'])\n",
    "    if rels and not any(rel for rel in rels if rel in row['relation']):\n",
    "#         print(row[['example', 'subj', 'obj', 'subj_qids', 'obj_qids', 'relation', 'prediction_model', 'prediction_spanbert', 'mentions']])\n",
    "#         print(f\"Rels {rels}\")\n",
    "#         print()\n",
    "        count += 1\n",
    "\n",
    "print(\"For \", count, \" bootleg errors, the existing bootleg relation is NOT a subset of the gold relation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting examples where bootleg had a relation for the subj, obj pair\n",
    "# booterrors_with_relation_df = df_errors[df_errors['id'].isin(ids_missed_propernoun_with_bootrels)]\n",
    "count = 0\n",
    "for index, row in df_errors.iterrows():\n",
    "    rels = get_relations(row['subj_qids'], row['obj_qids'])\n",
    "    if rels and any(rel for rel in rels if rel in row['relation']):\n",
    "        print(row[['example', 'subj', 'obj', 'subj_qids', 'obj_qids', 'relation', 'prediction_model', 'prediction_spanbert', 'mentions']])\n",
    "        print(f\"Rels {rels}\")\n",
    "        print()\n",
    "        count += 1\n",
    "        \n",
    "print(\"For \", count, \" bootleg errors, the bootleg relation is a subset (or the same) as the gold relation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting examples where bootleg had a relation for the subj, obj pair\n",
    "# booterrors_with_relation_df = df_errors[df_errors['id'].isin(ids_missed_propernoun_with_bootrels)]\n",
    "count = 0\n",
    "for index, row in df_errors.iterrows():\n",
    "    rels = get_relations(row['subj_qids'], row['obj_qids'])\n",
    "    if rels and any(rel for rel in rels if rel in row['prediction_model']):\n",
    "        print(row[['example', 'subj', 'obj', 'subj_qids', 'obj_qids', 'relation', 'prediction_model', 'prediction_spanbert', 'mentions']])\n",
    "        print(f\"Rels {rels}\")\n",
    "        print()\n",
    "        count += 1\n",
    "        \n",
    "print(\"For \", count, \" bootleg errors, the bootleg relation is a subset (or the same) as the bootleg prediction\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
