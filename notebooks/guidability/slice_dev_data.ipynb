{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "import jsonlines\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "from bootleg.symbols.entity_symbols import EntitySymbols\n",
    "from bootleg.symbols.type_symbols import TypeSymbols\n",
    "from bootleg.symbols.kg_symbols import KGSymbols\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "tqdm.pandas()\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "pd.options.display.max_colwidth = 500\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_dir = Path(\"/dfs/scratch0/lorr1/projects/bootleg-data/data/korealiases_title_0122\")\n",
    "output_dir = input_dir / \"resliced\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "a2q = json.load(open(input_dir / \"entity_db/entity_mappings/alias2qids.json\"))\n",
    "entity_dump = EntitySymbols(load_dir=input_dir / \"entity_db/entity_mappings\")\n",
    "emb_dir = Path('/dfs/scratch0/lorr1/projects/bootleg-data/embs')\n",
    "types_hy = TypeSymbols(entity_dump, emb_dir, max_types=3, type_vocab_file=\"hyena_vocab.json\", type_file=\"hyena_types_1229.json\")\n",
    "types_wd = TypeSymbols(entity_dump, emb_dir, max_types=3, type_vocab_file=\"wikidatatitle_to_typeid_1229.json\", type_file=\"wikidata_types_1229.json\")\n",
    "types_rel = TypeSymbols(entity_dump, emb_dir, max_types=50, type_vocab_file=\"relation_to_typeid_1229.json\", type_file=\"kg_relation_types_1229.json\")\n",
    "kg_syms = KGSymbols(entity_dump, emb_dir, \"kg_adj_1229.txt\")\n",
    "q2title = json.load(open(input_dir / \"entity_db/entity_mappings/qid2title.json\"))\n",
    "title2q = {v:k for k,v in q2title.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def any_word_contained_in_type(regexes, type_name):\n",
    "    return any([re.search(w, type_name) is not None for w in regexes])\n",
    "\n",
    "def any_word_in_any_type_set(regexes, type_names):\n",
    "    return any([any_word_contained_in_type(regexes, type_name) for type_name in type_names])\n",
    "\n",
    "def cand_idx_has_types_with_regexes(regexes, cand_types):\n",
    "    return_cand_idx = []\n",
    "    for cand_idx, type_names in enumerate(cand_types):\n",
    "        if any_word_in_any_type_set(regexes, type_names):\n",
    "            return_cand_idx.append(cand_idx)\n",
    "    return return_cand_idx\n",
    "\n",
    "def num_words_in_sentence(words, sentence):\n",
    "    cnt = 0\n",
    "    for w in sentence.lower().split():\n",
    "        if ps.stem(w) in words:\n",
    "            cnt += 1\n",
    "    return cnt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#SLICE FUNCTIONS\n",
    "team_wd_type_res = [r\"club(?! season)\", r\"team(?! season)\"]\n",
    "location_wd_type_res = [r\"^country$\", r\"city\", r\"town\"]\n",
    "country_wd_type_res = [r\"^country$\"]\n",
    "person_wd_type_res = [r\"^human$\"]\n",
    "sport_words = [ps.stem(w) for w in [\"played\", \"match\", \"team\", \"club\", \"matches\", \"cricket\", \"soccer\", \"league\",\n",
    "                                    \"cup\", \"football\", \"play\", \"teams\", \"champoinship\", \"series\", \"goal\", \"scored\",\n",
    "                                    \"score\", \"win\", \"winner\", \"defense\", \"offense\", \"coach\", \"penalty\", \"tournament\",\n",
    "                                    \"fifa\", \"forward\", \"defender\", \"faced\", \"faces\"]]\n",
    "months = [m.lower() for m in ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']]\n",
    "\n",
    "def is_in_airport(al_idx, sent_idx, sentence, qid, alias, span, title, hy_types, wd_types, es):\n",
    "    return len(set(['international airport', 'airport']).intersection(wd_types)) > 0\n",
    "\n",
    "def is_in_historical(al_idx, sent_idx, sentence, qid, alias, span, title, hy_types, wd_types, es):\n",
    "    return any([\"historical\" in ty for ty in wd_types])\n",
    "\n",
    "def is_in_ethnic(al_idx, sent_idx, sentence, qid, alias, span, title, hy_types, wd_types, es):\n",
    "    return any([\"ethnic\" in ty for ty in wd_types])\n",
    "\n",
    "def is_in_tournament(al_idx, sent_idx, sentence, qid, alias, span, title, hy_types, wd_types, es):\n",
    "    return any([\"tournament\" in ty for ty in wd_types])\n",
    "\n",
    "def is_in_team(al_idx, sent_idx, sentence, qid, alias, span, title, hy_types, wd_types, es):\n",
    "    return any_word_in_any_type_set(team_wd_type_res, wd_types)\n",
    "\n",
    "def is_in_natsoccersports(al_idx, sent_idx, sentence, qid, alias, span, title, hy_types, wd_types, es):\n",
    "    return len(set(['national association football team']).intersection(wd_types)) > 0\n",
    "\n",
    "def is_in_football_type(al_idx, sent_idx, sentence, qid, alias, span, title, hy_types, wd_types, es):\n",
    "    return any([\"American football team\" in ty for ty in wd_types])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_slices(line, slice_names, es, types_hy, types_wd, types_rel):\n",
    "    sentence = line['sentence']\n",
    "    all_spans = line[\"spans\"]\n",
    "    all_titles = [es.get_title(q) for q in line['qids']]\n",
    "    all_types_hy = [types_hy.get_types(q) for q in line['qids']]\n",
    "    all_types_wd = [types_wd.get_types(q) for q in line['qids']]\n",
    "\n",
    "    new_slices = {}\n",
    "    for s in slice_names:\n",
    "        new_slices[s] = {}\n",
    "        func_name = f\"is_in_{s}\"\n",
    "        func = globals()[func_name]\n",
    "        for al_idx in range(len(line['qids'])):\n",
    "            if func(al_idx, line[\"sent_idx_unq\"], sentence, line[\"qids\"][al_idx], line[\"aliases\"][al_idx], all_spans[al_idx], all_titles[al_idx], all_types_hy[al_idx], all_types_wd[al_idx], es):\n",
    "                new_slices[s][str(al_idx)] = 1.0\n",
    "            else:\n",
    "                new_slices[s][str(al_idx)] = 0.0\n",
    "    return new_slices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_files = [input_dir / \"dev.jsonl\", input_dir / \"test.jsonl\"]\n",
    "slice_names = [\n",
    "    \"airport\",\n",
    "    \"historical\",\n",
    "    \"ethnic\",\n",
    "    \"tournament\",\n",
    "    \"team\",\n",
    "    \"natsoccersports\",\n",
    "    \"football_type\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_num_lines(in_file):\n",
    "    count = 0\n",
    "    with open(in_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "for in_f_name in in_files:\n",
    "    out_f_name = f\"{os.path.splitext(in_f_name)[0]}_sliced.jsonl\"\n",
    "    num_lines = get_num_lines(os.path.join(input_dir, in_f_name))\n",
    "    print(f\"Reading in {in_f_name}\")\n",
    "    total_mens = 0\n",
    "    with jsonlines.open(os.path.join(output_dir, out_f_name), \"w\") as out_f, jsonlines.open(os.path.join(input_dir, in_f_name), \"r\") as in_f:\n",
    "        slice_totals = defaultdict(int)\n",
    "        slice_overlaps = defaultdict(lambda: defaultdict(int))\n",
    "        for line in tqdm(in_f, total=num_lines):\n",
    "            old_slices = line[\"slices\"]\n",
    "            new_slices = get_slices(line, slice_names, entity_dump, types_hy, types_wd, types_rel)\n",
    "            for s in new_slices:\n",
    "                old_slices[s] = new_slices[s]\n",
    "            line[\"slices\"] = old_slices\n",
    "            total_mens += len(line[\"aliases\"])\n",
    "            for s in new_slices:\n",
    "                slice_totals[s] += sum(new_slices[s].values())\n",
    "                for s2 in new_slices:\n",
    "                    if s2 == s:\n",
    "                        continue\n",
    "                    for al_idx_str in new_slices[s2]:\n",
    "                        if new_slices[s2][al_idx_str] > 0.5 and new_slices[s][al_idx_str] > 0.5:\n",
    "                            slice_overlaps[s][s2] += 1\n",
    "#             if sum(new_slices[\"airport\"].values()) > 0:\n",
    "#                 print(line[\"aliases\"], line[\"qids\"], line[\"sentence\"])\n",
    "            out_f.write(line)\n",
    "    print(f\"Wrote out to {out_f_name} with {total_mens} mentions and slice totals {json.dumps(slice_totals, indent=4)} and overlaps of {json.dumps(slice_overlaps, indent=4)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}