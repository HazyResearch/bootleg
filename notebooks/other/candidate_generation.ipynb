{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bootleg_data_prep.benchmarks.filter_and_compute_recall as fcr\n",
    "import bootleg_data_prep.benchmarks.candidate_generators as gen\n",
    "import bootleg_data_prep.utils.data_prep_utils as prep_utils\n",
    "import marisa_trie\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from tqdm import tqdm\n",
    "\n",
    "ps = PorterStemmer()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "import bootleg_data_prep.utils.data_prep_utils as prep_utils\n",
    "from bootleg.utils import utils\n",
    "from bootleg.symbols.entity_symbols import EntitySymbols\n",
    "import glob, os, jsonlines, json, time\n",
    "from tqdm import tqdm\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from utils_not_for_public import load_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing cache _cache_wiki0905...\n",
      "Error trying to load _cache_wiki0905/entity_symbols_plus.pkl. The file may not exist. Regenerating...\n",
      "Entity save directory where all information is saved/loaded from is /dfs/scratch0/lorr1/data_prep/data/wiki0905/entity_db/entity_mappings\n",
      "Found existing qid_count at /dfs/scratch0/lorr1/data_prep/data/wiki0905/entity_db/entity_mappings/entity_qid_count\n",
      "Reading types from /dfs/scratch0/lorr1/data_prep/embs with type file hyena_types_0905.json...\n",
      "Loading types from /dfs/scratch0/lorr1/data_prep/embs/hyena_types_0905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading /dfs/scratch0/lorr1/data_prep/embs/hyena_types_0905.json: 100%|██████████| 5310039/5310039 [00:33<00:00, 159136.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading types from /dfs/scratch0/lorr1/data_prep/embs with type file wikidata_types_0905.json...\n",
      "Loading types from /dfs/scratch0/lorr1/data_prep/embs/wikidata_types_0905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading /dfs/scratch0/lorr1/data_prep/embs/wikidata_types_0905.json: 100%|██████████| 5310039/5310039 [00:31<00:00, 170013.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5310039 hyena qids have a type 5310039 wd qids have a type\n",
      "Reading kg adj from /dfs/scratch0/lorr1/data_prep/embs with kg file kg_adj_0905.txt...\n",
      "Reading kg_adj from /dfs/scratch0/lorr1/data_prep/embs/kg_adj_0905.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25730507/25730507 [03:15<00:00, 131540.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of connections 119.0677853333616, 90th percentile 142.0, trimming to 150\n",
      "Len 4764541 for rel_mapping\n",
      "Reading kg triples from /dfs/scratch0/lorr1/data_prep/embs with kg file kg_triples_0905.txt...\n",
      "Reading contextual kg triples from kg_triples_0905.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35179217/35179217 [00:31<00:00, 1103068.63it/s]\n",
      "100%|██████████| 35179217/35179217 [00:55<00:00, 635349.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in existing affordance bags from /dfs/scratch0/lorr1/data_prep/data/wiki0905/entity_db/entity_mappings/entity_type_words\n",
      "0 for affordance_types\n",
      "Reading in existing entity word bags from /dfs/scratch0/lorr1/data_prep/data/wiki0905/entity_db/entity_mappings/entity_bag_of_words\n",
      "0 for entity_words\n",
      "Reading in vocab from /dfs/scratch0/lorr1/data_prep/data/wiki0905/entity_db/entity_mappings/entity_all_words/all_words_vocab.marisa\n",
      "Error reading vocab marisa-trie/lib/marisa/grimoire/io/mapper.cc:141: MARISA_IO_ERROR: ::stat(filename, &st) != 0\n",
      "TYPE QID LENGTHS WIKIDATA 5310039 HYENA 5310039\n",
      "Max Values {'alias2qid': 30, 'rel_mapping': 150, 'qid2typeid_hy': 3, 'qid2typeid_wd': 3}\n",
      "FOUND TRI NAME alias2qid\n",
      "FOUND TRI NAME rel_mapping\n",
      "FOUND TRI NAME qid2typeid_hy\n",
      "FOUND TRI NAME qid2typeid_wd\n",
      "Finished building entity symbols\n",
      "Reading in vocab from /dfs/scratch0/lorr1/data_prep/data/wiki0905/entity_db/entity_mappings/entity_all_words/all_words_vocab.marisa\n",
      "Error reading vocab marisa-trie/lib/marisa/grimoire/io/mapper.cc:141: MARISA_IO_ERROR: ::stat(filename, &st) != 0\n",
      "Loaded entity symbols.\n",
      "Error trying to load _cache_wiki0905/type_symbols_hy.pkl and _cache_wiki0905/type_symbols_wd.pkl. The file may not exist. Regenerating...\n",
      "Loading types from /dfs/scratch0/lorr1/data_prep/embs/hyena_types_0905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading /dfs/scratch0/lorr1/data_prep/embs/hyena_types_0905.json: 100%|██████████| 5310039/5310039 [00:33<00:00, 158578.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading types from /dfs/scratch0/lorr1/data_prep/embs/wikidata_types_0905.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading /dfs/scratch0/lorr1/data_prep/embs/wikidata_types_0905.json: 100%|██████████| 5310039/5310039 [00:32<00:00, 164824.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading alias qid counts from /dfs/scratch0/lorr1/data_prep/data/wiki0905/stats/alias_qid_traindata_withaugment.json\n",
      "Found 506 hyena types\n",
      "Found 26933 wikidata types\n",
      "FINISHED LOADING IN 2598.595745563507\n"
     ]
    }
   ],
   "source": [
    "es, esp, type_vocab, typeid2typename, type_vocab_wd, typeid2typename_wd, alias_qid_count = load_esp(\"wiki_0905\", overwrite=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    data_dir = '/dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest',\n",
    "    entity_dump = '/dfs/scratch0/lorr1/projects/bootleg/data/wiki_0906/entity_db/entity_mappings',\n",
    "    sub_dir = 'unfiltered',\n",
    "    file = \"\",\n",
    "    out_dir = 'filtered',\n",
    "    errors_dir = 'cand_gen_filtering_errors',\n",
    "    verbose = True,\n",
    "    method = \"contextual\",\n",
    "    expand_aliases = True,\n",
    "    gold_given = True,\n",
    "    max_candidates = 30,\n",
    "    large_alias_map = '/dfs/scratch0/lorr1/data_prep/unfiltered_data_0906/entity_db/entity_mappings/alias2qids.json',\n",
    "    large_title_map = '/dfs/scratch0/lorr1/data_prep/unfiltered_data_0906/entity_db/entity_mappings/qid2title.json',\n",
    "    wiki_pages = '/dfs/scratch0/lorr1/data_prep/unfiltered_data_0914/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_wikipedia_pages(args):\n",
    "    files = sorted(glob.glob(os.path.join(args.wiki_pages, '*.jsonl')), reverse=True)\n",
    "    print(f\"Loading {len(files)} wikipedia pages...\")\n",
    "    st = time.time()\n",
    "    qid2page = {}\n",
    "    for file in tqdm(files):\n",
    "        with jsonlines.open(file) as in_file:\n",
    "            for obj in in_file:\n",
    "                qid2page[obj['qid']] = obj\n",
    "    print(f\"Took {time.time()-st}s to load {len(files)} files\")\n",
    "    return qid2page\n",
    "\n",
    "def load_large_alias_map(args, entity_dump):\n",
    "    temp_dir = \"_temp\"\n",
    "    utils.ensure_dir(temp_dir)\n",
    "    temp_f = os.path.join(temp_dir, \"alias_map.pkl\")\n",
    "    if os.path.exists(temp_f):\n",
    "        print(f\"Reading alias map from {temp_f}\")\n",
    "        filtered_alias_map = utils.load_pickle_file(temp_f)\n",
    "        return filtered_alias_map\n",
    "    large_alias_to_qid_map = json.load(open(args.large_alias_map))\n",
    "\n",
    "    # filter down and remove qids which aren't contained in the entity save\n",
    "    filtered_alias_map = {}\n",
    "    for alias, qid_list in large_alias_to_qid_map.items():\n",
    "        filtered_qid_list = []\n",
    "        for qid, count in qid_list:\n",
    "            if entity_dump.qid_exists(qid):\n",
    "                filtered_qid_list.append([qid, count])\n",
    "        filtered_alias_map[alias] = filtered_qid_list\n",
    "    utils.dump_pickle_file(temp_f, filtered_alias_map)\n",
    "    return filtered_alias_map\n",
    "\n",
    "def load_large_title_map(args):\n",
    "    return json.load(open(args.large_title_map))\n",
    "\n",
    "def write_output_file(sentences, fpath, args):\n",
    "    print(f\"Output to {fpath}\")\n",
    "    out_file = open(fpath, 'w')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        out_file.write(json.dumps(sentence) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/241 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 241 wikipedia pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [31:05<00:00,  7.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1865.771460533142s to load 241 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qid2page = load_wikipedia_pages(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading alias map from _temp/alias_map.pkl\n"
     ]
    }
   ],
   "source": [
    "entity_dump = EntitySymbols(load_dir=args.entity_dump)\n",
    "alias_map = load_large_alias_map(args, entity_dump)\n",
    "title_map = load_large_title_map(args)\n",
    "alias_tri = marisa_trie.Trie(alias_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contextual:\n",
    "\n",
    "    def __init__(self, args, entity_dump, qid2page, alias_map, alias_tri, title_map):\n",
    "\n",
    "        self.args = args\n",
    "        self.entity_dump = entity_dump\n",
    "        self.filtered_alias_map = alias_map\n",
    "        self.title_map = title_map\n",
    "        self.alias_tri = alias_tri\n",
    "        self.qid2page = qid2page\n",
    "        self.new_alias2qid = {}\n",
    "        self.saved_metrics = defaultdict(int)\n",
    "        self.detailed_metrics = defaultdict(list)\n",
    "\n",
    "    def convert_char_to_word_boundaries(self, phrase, start_char, end_char):\n",
    "        start_word = max(0, len(phrase[:start_char].split()))\n",
    "        end_word = max(0, len(phrase[:end_char].split()))\n",
    "        return start_word, end_word\n",
    "\n",
    "    def get_lnrm(self, s):\n",
    "        \"\"\"Convert a string to its lnrm form\n",
    "        We form the lower-cased normalized version l(s) of a string s by canonicalizing\n",
    "        its UTF-8 characters, eliminating diacritics, lower-casing the UTF-8 and\n",
    "        throwing out all ASCII-range characters that are not alpha-numeric.\n",
    "        from http://nlp.stanford.edu/pubs/subctackbp.pdf Section 2.3\n",
    "        Args:\n",
    "            input string\n",
    "        Returns:\n",
    "            the lnrm form of the string\n",
    "        \"\"\"\n",
    "        lnrm = unicodedata.normalize('NFD', str(s))\n",
    "        lnrm = lnrm.lower()\n",
    "        lnrm = ''.join([x for x in lnrm if (not unicodedata.combining(x)\n",
    "                                            and x.isalnum() or x == ' ')]).strip()\n",
    "        # will remove if there are any duplicate white spaces e.g. \"the  alias    is here\"\n",
    "        lnrm = \" \".join(lnrm.split())\n",
    "        return lnrm\n",
    "\n",
    "    def get_proper_nouns(self, text):\n",
    "        words = text.split()\n",
    "        tagged_text = pos_tag(words)\n",
    "        # all_nouns = [word for word,pos in tagged_text if pos[:2] in ['NN'] and (word.lower() not in stopwords) and (word.lower().rstrip(\"s\") not in stopwords)]\n",
    "        all_nouns = []\n",
    "        #\n",
    "        # doc = nlp(text)\n",
    "        # ent_idx = 0\n",
    "        # while ent_idx < len(doc.ents):\n",
    "        #     ent = doc.ents[ent_idx]\n",
    "        #     ent_text = ent.text\n",
    "        #     ent_idx += 1\n",
    "        #     print(\"ENT TX\", ent_text)\n",
    "        i = 0\n",
    "        nouns = []\n",
    "        while i < len(words):\n",
    "            if words[i][0].isupper() and words[i].lower() not in stopwords:\n",
    "                start_i = i\n",
    "                i += 1\n",
    "                while True and i < len(words):\n",
    "                    if words[i][0].isupper():\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "                nouns.append(' '.join(words[start_i: i]))\n",
    "                continue\n",
    "            i += 1\n",
    "        full_nouns = set(all_nouns)\n",
    "        for noun in nouns:\n",
    "            full_nouns.add(noun)\n",
    "            for part in noun.split():\n",
    "                full_nouns.add(part)\n",
    "        return list(full_nouns)\n",
    "\n",
    "    def get_proper_nouns_old(self, text):\n",
    "        nouns = []\n",
    "        words = text.split()\n",
    "        start = 0\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            if words[i][0].isupper():\n",
    "                start_i = i\n",
    "                i += 1\n",
    "                while True and i < len(words):\n",
    "                    if words[i][0].isupper():\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        break\n",
    "                nouns.append(' '.join(words[start_i: i]))\n",
    "                continue\n",
    "            i += 1\n",
    "        full_nouns = []\n",
    "        for noun in nouns:\n",
    "            full_nouns.append(noun)\n",
    "            for part in noun.split():\n",
    "                full_nouns.append(part)\n",
    "        return list(set(full_nouns))\n",
    "\n",
    "    def get_sub_alias_match(self, alias, alias2qids):\n",
    "        assert len(alias.strip()) > 0, f\"The alias is empty. This shouldn't happen.\"\n",
    "        words = alias.split()\n",
    "        subseqs = set()\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i, len(words)+1):\n",
    "                phrase = ' '.join(words[i:j])\n",
    "                if len(phrase) > 0:\n",
    "                    subseqs.add(phrase)\n",
    "        subseqs = list(subseqs)\n",
    "        subseqs.sort(key=lambda s: len(s), reverse=True)\n",
    "        qid2count = defaultdict(int)\n",
    "        curr_len = len(subseqs[0].split())\n",
    "        for alias in subseqs:\n",
    "            if len(alias.split()) != curr_len:\n",
    "                if len(qid2count) > 0:\n",
    "                    break\n",
    "                curr_len = len(alias.split())\n",
    "            if alias in alias2qids:\n",
    "                for pair in alias2qids[alias]:\n",
    "                    qid2count[pair[0]] = pair[1] + qid2count[pair[0]]\n",
    "        candidates = [[qid, value] for qid, value in qid2count.items() if value > 0]\n",
    "        return candidates\n",
    "\n",
    "    def get_sub_alias_match2(self, alias, alias2qids):\n",
    "        assert len(alias.strip()) > 0, f\"The alias is empty. This shouldn't happen.\"\n",
    "        words = alias.split()\n",
    "        subseqs = set()\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i, len(words)+1):\n",
    "                phrase = ' '.join(words[i:j])\n",
    "                if len(phrase) > 0 and len(phrase.split()) > 0:\n",
    "                    subseqs.add(phrase)\n",
    "        if len(subseqs) <= 0:\n",
    "            return []\n",
    "        subseqs = list(subseqs)\n",
    "        subseqs.sort(key=lambda s: len(s), reverse=True)\n",
    "        qid2count = defaultdict(int)\n",
    "        curr_len = len(subseqs[0].split())\n",
    "        for alias in subseqs:\n",
    "            if len(alias.split()) != curr_len:\n",
    "                if len(qid2count) > 0:\n",
    "                    break\n",
    "                curr_len = len(alias.split())\n",
    "            if alias in alias2qids:\n",
    "                for pair in alias2qids[alias]:\n",
    "                    qid2count[pair[0]] = pair[1] + qid2count[pair[0]]\n",
    "        candidates = [[qid, value] for qid, value in qid2count.items() if value > 0]\n",
    "        return candidates\n",
    "\n",
    "    def get_qids(self, target_alias, alias2qids):       \n",
    "        if target_alias in alias2qids:\n",
    "            return alias2qids[target_alias]\n",
    "\n",
    "        if target_alias.replace(\" \", \"\") in alias2qids:\n",
    "            return alias2qids[target_alias.replace(\" \", \"\")]\n",
    "\n",
    "        return self.get_sub_alias_match2(target_alias, alias2qids)\n",
    "\n",
    "\n",
    "    def recompute_qid_rank(self, sample, alias, true_qid, qid2page, alias2qid, entity_dump):\n",
    "        qid_pairs = self.get_qids(alias, alias2qid)\n",
    "        if len(qid_pairs) == 0:\n",
    "            self.saved_metrics[\"no_alias\"] += 1\n",
    "            self.detailed_metrics[\"no_alias\"].append(\n",
    "                {\n",
    "                    \"true_qid\": true_qid,\n",
    "                    \"alias\": alias\n",
    "                }\n",
    "            )\n",
    "        if len(qid_pairs) <= 1:\n",
    "            self.saved_metrics[\"len_1_alias\"] += 1\n",
    "            return qid_pairs\n",
    "        # Each element in qid_pairs is [qid, qid_score]\n",
    "        qid_pairs.sort(key=lambda x:x[1], reverse=True)\n",
    "        context = sample['sentence']\n",
    "        # Extract proper (capitalized) nouns from sentence\n",
    "        prop_nouns = [word.lower() for word in self.get_proper_nouns(context) if word.lower() != alias]\n",
    "        qids = [x[0] for x in qid_pairs]\n",
    "        saved_counts = {}\n",
    "        # Count number of times proper noun appears on Wikipedia page\n",
    "        for i, qid in enumerate(qids):\n",
    "            count = {p: 0 for p in prop_nouns}\n",
    "            if qid in qid2page:\n",
    "                for sentence in qid2page[qid]['sentences']:\n",
    "                    text = \" \" + sentence['sentence'].lower() + \" \"\n",
    "                    for term in prop_nouns:\n",
    "                        if f\" {term} \" in text:\n",
    "                            count[term] += 1\n",
    "            saved_counts[qid] = count\n",
    "        counts = [x[1] for x in qid_pairs]\n",
    "        # Compute score for qid based on counts of proper nouns\n",
    "        for i, qid in enumerate(qids):\n",
    "            score = 0\n",
    "            num_hits = 0\n",
    "            for w in list(saved_counts[qid].keys()):\n",
    "                x = saved_counts[qid][w]\n",
    "                if x > 0:\n",
    "                    num_hits += 1\n",
    "                    # Heuristic threshold function\n",
    "                    score += min(x, 5)*len(w.split())\n",
    "            counts[i] += 10000*score\n",
    "        # TODO: fix if acronym\n",
    "        # SCORE BASED ON ALIAS TO ENTITY TITLE (partial)\n",
    "        for i, qid in enumerate(qids):\n",
    "            title = self.get_lnrm(entity_dump.get_title(qid).lower())\n",
    "            title_score = fuzz.partial_ratio(title, alias)/100\n",
    "            counts[i] += 100000*title_score\n",
    "        # ADD TYPE SCORE\n",
    "\n",
    "        scored_qids = [list(x) for x in zip(qids, counts)]\n",
    "        scored_qids = sorted(scored_qids, key= lambda x: x[1], reverse = True)\n",
    "        # Take top 5 most popular qids and remaining 25 from scored qids\n",
    "        final_scored_qids = qid_pairs[:5]\n",
    "        only_qids = [x[0] for x in final_scored_qids]\n",
    "        for qid_pair in scored_qids:\n",
    "            if len(final_scored_qids) == self.args.max_candidates:\n",
    "                break\n",
    "            if qid_pair[0] not in only_qids:\n",
    "                only_qids.append(qid_pair[0])\n",
    "                final_scored_qids.append(qid_pair)\n",
    "\n",
    "        if self.args.gold_given:\n",
    "            if true_qid not in only_qids[:self.args.max_candidates]:\n",
    "                if true_qid in [x[0] for x in scored_qids]:\n",
    "                    self.saved_metrics[\"in_long_cands_not_short\"] += 1\n",
    "                    self.detailed_metrics[\"in_long_cands_not_sort\"].append(\n",
    "                        {\n",
    "                            \"true_qid\": true_qid,\n",
    "                            \"alias\": alias,\n",
    "                            \"context\": context,\n",
    "                            \"nouns\": prop_nouns,\n",
    "                            \"saved_counts\": saved_counts,\n",
    "                            \"scored_qids\": scored_qids,\n",
    "                            \"titles\": [entity_dump.get_title(q) for q in only_qids]\n",
    "                        }\n",
    "                    )\n",
    "#                     print(\"TRUE QID\", true_qid, entity_dump.get_title(true_qid))\n",
    "#                     print(\"CONTEXT\", context)\n",
    "#                     print(\"NOUNS\", prop_nouns)\n",
    "#                     print(\"COUNTS\", saved_counts[true_qid])\n",
    "#                     for tqid in only_qids[:self.args.max_candidates]:\n",
    "#                         print(\"OTHER COUNTS\", tqid, entity_dump.get_title(tqid),  saved_counts[tqid])\n",
    "#                     print(\"SCORES\", scored_qids[:50])\n",
    "#                     print(\"TITLES\", [entity_dump.get_title(q) for q in only_qids[:50]])\n",
    "                    # print(\"ALL SENTENCES\", qid2page[true_qid]['sentences'])\n",
    "                else:\n",
    "                    self.saved_metrics[\"not_long_cands_not_short\"] += 1\n",
    "                    self.detailed_metrics[\"not_long_cands_not_sort\"].append(\n",
    "                        {\n",
    "                            \"true_qid\": true_qid,\n",
    "                            \"alias\": alias,\n",
    "                            \"context\": context,\n",
    "                            \"nouns\": prop_nouns,\n",
    "                            \"saved_counts\": saved_counts,\n",
    "                            \"scored_qids\": scored_qids,\n",
    "                            \"titles\": [entity_dump.get_title(q) for q in only_qids]\n",
    "                        }\n",
    "                    )\n",
    "            else:\n",
    "                self.saved_metrics[\"success\"] += 1\n",
    "        return final_scored_qids[:self.args.max_candidates]\n",
    "\n",
    "    def filter_sentence(self, sentence):\n",
    "        success = 0\n",
    "        no_alias_exists = 0\n",
    "        qid_not_in_cands = 0\n",
    "        no_qid_in_dump = 0\n",
    "        errors = []\n",
    "        new_sentence = {\n",
    "            'sentence': sentence['sentence'],\n",
    "            'sent_idx_unq': sentence['sent_idx_unq'],\n",
    "            'aliases': [],\n",
    "            'spans': [],\n",
    "            'qids': [],\n",
    "            'gold': [],\n",
    "            'cands': []\n",
    "        }\n",
    "        phrase = sentence['sentence']\n",
    "        aliases = sentence['aliases']\n",
    "        spans = sentence['spans']\n",
    "        # Uses NER tagger to expand aliases\n",
    "        if self.args.expand_aliases:\n",
    "            doc = nlp(phrase)\n",
    "            al_idx = 0\n",
    "            ent_idx = 0\n",
    "            while ent_idx < len(doc.ents) and al_idx < len(aliases):\n",
    "                ent = doc.ents[ent_idx]\n",
    "                ent_text = ent.text\n",
    "                ent_start_char = ent.start_char\n",
    "                ent_end_char = ent.end_char\n",
    "                # remove starting \"the\" and trailing \"'s\"\n",
    "                if ent_text.startswith(\"the \"):\n",
    "                    ent_text = ent_text[4:]\n",
    "                    ent_start_char += 4\n",
    "                if ent_text.endswith(\" 's\"):\n",
    "                    ent_text = ent_text[:-3]\n",
    "                    ent_end_char -= 3\n",
    "                start_span, end_span = self.convert_char_to_word_boundaries(phrase, ent_start_char, ent_end_char)\n",
    "                cur_alias = aliases[al_idx]\n",
    "                cur_spans = spans[al_idx]\n",
    "                is_bleeding_into_next = False\n",
    "                if al_idx+1 < len(spans):\n",
    "                    next_cur_start_span = spans[al_idx+1][0]\n",
    "                    is_bleeding_into_next = end_span >= next_cur_start_span\n",
    "                if start_span >= cur_spans[1]:\n",
    "                    al_idx += 1\n",
    "                elif start_span <= cur_spans[0] and end_span >= cur_spans[1] and not is_bleeding_into_next and aliases[al_idx] != ent_text.lower():\n",
    "#                     print(\"Swapping\", start_span, end_span, cur_spans, cur_alias, \"**\", ent_text, \"**\", phrase)\n",
    "                    spans[al_idx] = [start_span, end_span]\n",
    "                    aliases[al_idx] = ent_text\n",
    "                    al_idx += 1\n",
    "                    ent_idx += 1\n",
    "                else:\n",
    "                    ent_idx += 1\n",
    "        if self.args.gold_given:\n",
    "            qids = sentence['qids']\n",
    "        else:\n",
    "            qids = [\"Q-1\" for _ in range(len(sentence['aliases']))]\n",
    "        gold = sentence.get('gold', [True for _ in range(len(sentence['aliases']))])\n",
    "        sent_idx_unq = sentence['sent_idx_unq']\n",
    "        for i in range(len(aliases)):\n",
    "            alias = prep_utils.get_lnrm(aliases[i])\n",
    "            candidates = self.recompute_qid_rank(sentence, alias, qids[i], self.qid2page, self.filtered_alias_map, self.entity_dump)\n",
    "            #######\n",
    "            new_sentence[\"cands\"].append(candidates)\n",
    "            # With no golds and not nil model, need to set QIDs to be something in the save; nil models can have Q-1\n",
    "            qid_to_add = qids[i]\n",
    "#             if not self.args.gold_given and len(candidates) > 0:\n",
    "#                 qid_to_add = candidates[0][0]\n",
    "            #print(f\"Alias: {alias}, Candidates {candidates}\")\n",
    "            if len(candidates) == 0:\n",
    "                # print(f\"{alias} not in alias map. True QID: {qids[i]}\")\n",
    "                errors.append({\n",
    "                    'alias': alias,\n",
    "                    'qid': qid_to_add,\n",
    "                    'sentence': sentence['sentence'],\n",
    "                    'label': 'alias not found',\n",
    "                })\n",
    "                no_alias_exists += 1\n",
    "                continue\n",
    "            if self.args.gold_given and not self.entity_dump.qid_exists(qid_to_add):\n",
    "                print(f\"QID {qid_to_add} doesn't exist in entity save for {alias}.\")\n",
    "                no_qid_in_dump += 1\n",
    "                continue\n",
    "            if qid_to_add in [x[0] for x in candidates]:\n",
    "                assert int(spans[i][1]) <= len(sentence['sentence'].split()), sentence\n",
    "                alias = f\"{alias}_{sent_idx_unq}\"\n",
    "                self.new_alias2qid[alias] = candidates\n",
    "                new_sentence['aliases'].append(alias)\n",
    "                new_sentence['qids'].append(qid_to_add)\n",
    "                new_sentence['spans'].append(spans[i])\n",
    "                new_sentence['gold'].append(gold[i])\n",
    "                success += 1\n",
    "            else:\n",
    "                # print(f\"Alias: {alias}, True QID: {qids[i]}, Candidates in map: {candidates}\")\n",
    "                errors.append({\n",
    "                    'alias': alias,\n",
    "                    'candidates': list(candidates),\n",
    "                    'qid': qid_to_add,\n",
    "                    'sentence': sentence['sentence'],\n",
    "                    'label': 'candidate not found',\n",
    "                })\n",
    "                assert int(spans[i][1]) <= len(sentence['sentence'].split()), sentence\n",
    "                alias = f\"{alias}_{sent_idx_unq}\"\n",
    "                self.new_alias2qid[alias] = candidates\n",
    "                new_sentence['aliases'].append(alias)\n",
    "                new_sentence['qids'].append(qid_to_add)\n",
    "                new_sentence['spans'].append(spans[i])\n",
    "                new_sentence['gold'].append(gold[i])\n",
    "                qid_not_in_cands += 1\n",
    "\n",
    "        # return number of aliases kept, and dropped\n",
    "        return new_sentence, success, no_alias_exists, qid_not_in_cands, no_qid_in_dump, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making /dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest/filtered\n",
      "Making /dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest/cand_gen_filtering_errors\n",
      "Candidate generator: contextual\n"
     ]
    }
   ],
   "source": [
    "out_dir = prep_utils.get_outdir(args.data_dir, args.out_dir)\n",
    "errors_dir = prep_utils.get_outdir(args.data_dir, args.errors_dir)\n",
    "\n",
    "print(f\"Candidate generator: {args.method}\")\n",
    "if args.method == 'standard':\n",
    "    candidate_generator = gen.Standard(args, entity_dump)\n",
    "elif args.method == 'contextual':\n",
    "    candidate_generator = Contextual(args, entity_dump, qid2page, alias_map, alias_tri, title_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:   1%|          | 2/357 [00:00<00:18, 19.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering file /dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest/unfiltered/test_rss500.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:  17%|█▋        | 61/357 [00:06<00:39,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID -1 doesn't exist in entity save for mohr davidow ventures.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:  40%|████      | 143/357 [00:13<00:16, 12.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID -1 doesn't exist in entity save for st joseph s regional medical center.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:  69%|██████▊   | 245/357 [00:26<00:14,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID Q3453515 doesn't exist in entity save for russia in global affairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:  98%|█████████▊| 351/357 [00:35<00:00, 20.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID Q6979295 doesn't exist in entity save for national venture capital association.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████| 357/357 [00:36<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest/unfiltered/test_rss500.jsonl. Total mentions 524. Succesfully mapped: 473. Alias not found for 1 mentions. Correct QID not in candidate list for 46 mentions. Correct QID not in save for 4 mentions.\n",
      "Output to /dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest/filtered/test_rss500.jsonl\n",
      "Filtering file /dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest/unfiltered/test_rss500_clean.jsonl\n",
      "Skipping test_rss500_clean.jsonl\n",
      "Saved new alias mapping to /dfs/scratch0/lorr1/projects/bootleg/data/benchmarks/rss500wikinewdatatest/filtered/entity_db/entity_mappings/alias2qid_new.json\n",
      "Recall {'test_rss500.jsonl': {'total_mentions': 524, 'found_mentions': 473, 'alias_in_list': 523}}\n",
      "Metrics {\n",
      "    \"len_1_alias\": 193,\n",
      "    \"success\": 288,\n",
      "    \"not_long_cands_not_short\": 33,\n",
      "    \"in_long_cands_not_short\": 10,\n",
      "    \"no_alias\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "candidate_generator.saved_metrics = defaultdict(int)\n",
    "candidate_generator.detailed_metrics = defaultdict(list)\n",
    "if args.file:\n",
    "    files = [args.file]\n",
    "else:\n",
    "    files = prep_utils.glob_files(os.path.join(args.data_dir, args.sub_dir, '*.jsonl'))\n",
    "recall_results = {}\n",
    "for f in files:\n",
    "    print(f\"Filtering file {f}\")\n",
    "    fname = os.path.basename(f)\n",
    "    if fname == \"test_rss500_clean.jsonl\":\n",
    "        print(f\"Skipping {fname}\")\n",
    "        continue\n",
    "    filtered_sentences, errors, recall = fcr.filter_and_clean(f, candidate_generator, args)\n",
    "    write_output_file(filtered_sentences, os.path.join(out_dir, fname), args)\n",
    "    fcr.write_errors(errors, os.path.join(errors_dir, fname.replace('jsonl', 'txt')))\n",
    "    recall_results[fname] = recall\n",
    "\n",
    "entity_dump_dir = os.path.join(out_dir, 'entity_db/entity_mappings')\n",
    "utils.ensure_dir(entity_dump_dir)\n",
    "new_alias_f = os.path.join(entity_dump_dir, \"alias2qid_new.json\")\n",
    "# save new alias mapping\n",
    "with open(new_alias_f, \"w\") as out_f:\n",
    "    json.save(candidate_generator.new_alias2qid, out_f)\n",
    "print(f\"Saved new alias mapping to {new_alias_f}\")\n",
    "\n",
    "result_fpath = os.path.join(errors_dir, 'recall_results.json')\n",
    "json.save(recall_results, open(result_fpath, 'w'))\n",
    "json.save(candidate_generator.detailed_metrics, open(os.path.join(errors_dir, \"detailed.json\"), \"w\"))\n",
    "print(f\"Recall {recall_results}\")\n",
    "print(f\"Metrics {json.dumps(candidate_generator.saved_metrics, indent=4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"Q938005\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load alias2qids,\n",
    "\n",
    "a2q_modified = json.load(open(\"/dfs/scratch0/lorr1/data_prep/unfiltered_data_0906/entity_db/entity_mappings/alias2qids.json\"))\n",
    "q2title = json.load(open(\"/dfs/scratch0/lorr1/projects/bootleg/data/wiki_0906/entity_db/entity_mappings/qid2title.json\"))\n",
    "q2score = {}\n",
    "\n",
    "for al in a2q_modified:\n",
    "    for pair in a2q_modified[al]:\n",
    "        qid, score = pair\n",
    "        if qid in q2score:\n",
    "            assert score == q2score[qid]\n",
    "            continue\n",
    "        else:\n",
    "            q2score[qid] = score\n",
    "\n",
    "\n",
    "person_set, gender_map = np.load('/dfs/scratch0/lorr1/projects/bootleg/data/wikidata_mappings/person.npy', allow_pickle=True)\n",
    "\n",
    "# for each person: modify the alias\n",
    "for qid in q2title:\n",
    "    if qid is person:\n",
    "        name = q2title[q]\n",
    "        new_alias = get_lnrm(strip_middle(name))\n",
    "        if new_alias in a2q_modified:\n",
    "            a2q_modified[new_alias].append([qid, q2score[qid]])\n",
    "        else:\n",
    "            a2q_modified[new_alias] = [[qid, q2score[qid]]]\n",
    "\n",
    "json.save(a2q_modified, open(\"/dfs/scratch0/lorr1/data_prep/unfiltered_data_0906/entity_db/entity_mappings/alias2qids_modified.json\"))\n",
    "# In the args above, use /dfs/scratch0/lorr1/data_prep/unfiltered_data_0906/entity_db/entity_mappings/alias2qids_modified.json for large alias map parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
