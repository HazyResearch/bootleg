{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import ujson\n",
    "from bootleg.run import run_model\n",
    "sys.path.append(\"/dfs/scratch0/lorr1/projects/bootleg/tutorials\")\n",
    "from utils import load_mentions\n",
    "\n",
    "# set up logging\n",
    "import sys\n",
    "import logging\n",
    "from importlib import reload\n",
    "reload(logging)\n",
    "# Changing logging level will impact mention extractor\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "root_dir = Path(\"/dfs/scratch0/lorr1/projects/bootleg/tutorial_data/\")\n",
    "cand_map = root_dir / 'data/wiki_entity_data/entity_mappings/alias2qids_wiki_filt.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid2title = ujson.load(open(root_dir / 'data/wiki_entity_data/entity_mappings/qid2title.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_sample_orig = Path('/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/test.jsonl')\n",
    "nq_sample_bootleg = Path('/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/test_bootleg_men.jsonl')\n",
    "\n",
    "# nq_sample_orig = Path('/dfs/scratch0/lorr1/projects/bootleg-emmental/tutorial_data/data/nq/test_natural_questions_50.jsonl')\n",
    "# nq_sample_bootleg = Path('/dfs/scratch0/lorr1/projects/bootleg-emmental/tutorial_data/data/nq/test_natural_questions_50_bootleg.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15202497/15202497 [00:27<00:00, 558424.61it/s]\n",
      "100%|██████████| 309/309 [00:11<00:00, 27.82it/s]\n",
      "100%|██████████| 309/309 [00:12<00:00, 25.03it/s]\n",
      "100%|██████████| 309/309 [00:12<00:00, 24.29it/s]\n",
      "100%|██████████| 302/302 [00:13<00:00, 22.52it/s]\n",
      "100%|██████████| 309/309 [00:15<00:00, 19.50it/s]\n",
      "100%|██████████| 309/309 [00:18<00:00, 16.45it/s]\n",
      "100%|██████████| 309/309 [00:18<00:00, 16.36it/s]\n",
      "100%|██████████| 309/309 [00:20<00:00, 15.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from bootleg.end2end.extract_mentions import extract_mentions\n",
    "extract_mentions(in_filepath=nq_sample_orig, out_filepath=nq_sample_bootleg, cand_map_file=cand_map, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>aliases_hand</th>\n",
       "      <th>spans_hand</th>\n",
       "      <th>aliases_bootleg</th>\n",
       "      <th>spans_bootleg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>vail , Colorado 1996-12-07 . vail , Colorado 1996-12-07</td>\n",
       "      <td>[alias_1344_0, alias_1344_1, alias_1344_0, alias_1344_1]</td>\n",
       "      <td>[[0, 1], [2, 3], [5, 6], [7, 8]]</td>\n",
       "      <td>[vail colorado, vail colorado]</td>\n",
       "      <td>[[0, 3], [5, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>cricket - lara endures another miserable day . All-rounder Greg Blewett steered his side to a comfortable victory with an unbeaten 57 in 90 balls to the delight of the 42,442 crowd .</td>\n",
       "      <td>[alias_1177_0, alias_1177_9]</td>\n",
       "      <td>[[2, 3], [9, 11]]</td>\n",
       "      <td>[lara, greg blewett, 90 balls]</td>\n",
       "      <td>[[2, 3], [9, 11], [23, 25]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>nba basketball - standings after friday 's games . charlotte at seattle</td>\n",
       "      <td>[alias_1383_0, alias_1383_51, alias_1383_52]</td>\n",
       "      <td>[[0, 1], [9, 10], [11, 12]]</td>\n",
       "      <td>[nba basketball, friday, charlotte, seattle]</td>\n",
       "      <td>[[0, 2], [5, 6], [9, 10], [11, 12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>rtrs - Australian mp John Langmore formally resigns . canberra 1996-12-06</td>\n",
       "      <td>[alias_1331_0, alias_1331_1, alias_1331_2, alias_1331_3]</td>\n",
       "      <td>[[0, 1], [2, 3], [4, 6], [9, 10]]</td>\n",
       "      <td>[rtrs, john langmore, canberra]</td>\n",
       "      <td>[[0, 1], [4, 6], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>tokyo 1996-12-06 . Japan 's Economic Planning Agency has not changed its view that the economy is gradually recovering , despite relatively weak gross domestic product figures released on Tuesday , epa Vice Minister Shimpei Nukaya told reporters on Friday .</td>\n",
       "      <td>[alias_1337_0, alias_1337_1]</td>\n",
       "      <td>[[0, 1], [3, 4]]</td>\n",
       "      <td>[tokyo, japan, economic planning, gross domestic product, vice minister, friday]</td>\n",
       "      <td>[[0, 1], [3, 4], [5, 7], [23, 26], [32, 34], [39, 40]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>Multinational commander going back to east Zaire . If he decided it was necessary and safe for the aircrew , he would not hesitate to order airdrops of food for the refugees , even against the wishes of the government in Kinshasa and the Zairean rebels who control much of eastern Zaire , he said .</td>\n",
       "      <td>[alias_1241_0, alias_1241_9, alias_1241_10, alias_1241_11]</td>\n",
       "      <td>[[6, 7], [41, 42], [44, 45], [51, 52]]</td>\n",
       "      <td>[airdrops, kinshasa, zaire]</td>\n",
       "      <td>[[26, 27], [41, 42], [51, 52]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>chonju , South Korea 1996-12-07 . Horii Manabu ( Japan ) 37.23 seconds ; 2 .</td>\n",
       "      <td>[alias_1360_0, alias_1360_2]</td>\n",
       "      <td>[[2, 4], [9, 10]]</td>\n",
       "      <td>[chonju, south korea, japan]</td>\n",
       "      <td>[[0, 1], [2, 4], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>Ex-minister , son killed in Central Africa unrest . bangui 1996-12-06</td>\n",
       "      <td>[alias_1245_0, alias_1245_1]</td>\n",
       "      <td>[[5, 7], [9, 10]]</td>\n",
       "      <td>[central africa, bangui]</td>\n",
       "      <td>[[5, 7], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>Plastic surgery gets boost in Brazil . Plastic surgery scares like the case in which Brazilian model Claudia Liz fell into a coma after being anaesthetised for a liposuction in October are not much of a deterrent .</td>\n",
       "      <td>[alias_1259_0, alias_1259_12]</td>\n",
       "      <td>[[5, 6], [15, 16]]</td>\n",
       "      <td>[plastic surgery, brazil, plastic surgery, scares, coma, liposuction, deterrent]</td>\n",
       "      <td>[[0, 2], [5, 6], [7, 9], [9, 10], [22, 23], [28, 29], [36, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>soccer - scottish league and cup results . soccer - scottish league and cup results .</td>\n",
       "      <td>[alias_1372_0, alias_1372_0]</td>\n",
       "      <td>[[2, 3], [10, 11]]</td>\n",
       "      <td>[soccer, scottish league, soccer, scottish league]</td>\n",
       "      <td>[[0, 1], [2, 4], [8, 9], [10, 12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>soccer - english league and cup results . Results of English league and cup</td>\n",
       "      <td>[alias_1373_0, alias_1373_2]</td>\n",
       "      <td>[[2, 3], [10, 11]]</td>\n",
       "      <td>[soccer, english league, english league]</td>\n",
       "      <td>[[0, 1], [2, 4], [10, 12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>tempe , Ariz . tempe , Ariz .</td>\n",
       "      <td>[alias_1284_0, alias_1284_0]</td>\n",
       "      <td>[[0, 1], [4, 5]]</td>\n",
       "      <td>[tempe ariz, tempe ariz]</td>\n",
       "      <td>[[0, 3], [4, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>nhl ice hockey - standings after thursday 's games . central division</td>\n",
       "      <td>[alias_1186_0, alias_1186_17]</td>\n",
       "      <td>[[0, 1], [10, 12]]</td>\n",
       "      <td>[central division]</td>\n",
       "      <td>[[10, 12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>Canadian grain statistics weekly . In addition , Statistics Canada indicated the following exports to the U.S. between August and September 1996 , in tonnes :</td>\n",
       "      <td>[alias_1279_0, alias_1279_3, alias_1279_4]</td>\n",
       "      <td>[[0, 1], [8, 10], [16, 17]]</td>\n",
       "      <td>[grain, statistics canada, us, september 1996]</td>\n",
       "      <td>[[1, 2], [8, 10], [16, 17], [20, 22]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>nhl ice hockey - standings after friday 's games . new jersey 14 10 1 61 61 29</td>\n",
       "      <td>[alias_1385_0, alias_1385_12]</td>\n",
       "      <td>[[0, 1], [10, 12]]</td>\n",
       "      <td>[friday, new jersey]</td>\n",
       "      <td>[[6, 7], [10, 12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>soccer - vieira saves arsenal with last-minute equaliser . Liverpool suffered an upset first home league defeat of the season , beaten 1-0 by a Guy Whittingham goal for Sheffield Wednesday .</td>\n",
       "      <td>[alias_1371_0, alias_1371_1, alias_1371_13, alias_1371_14, alias_1371_15]</td>\n",
       "      <td>[[2, 3], [4, 5], [9, 10], [25, 27], [29, 31]]</td>\n",
       "      <td>[soccer, vieira, saves, arsenal, liverpool, guy whittingham, sheffield wednesday]</td>\n",
       "      <td>[[0, 1], [2, 3], [3, 4], [4, 5], [9, 10], [25, 27], [29, 31]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>nhl ice hockey - standings after friday 's games . chicago at montreal</td>\n",
       "      <td>[alias_1385_0, alias_1385_39, alias_1385_40]</td>\n",
       "      <td>[[0, 1], [10, 11], [12, 13]]</td>\n",
       "      <td>[friday, chicago, montreal]</td>\n",
       "      <td>[[6, 7], [10, 11], [12, 13]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>vail , Colorado 1996-12-07 . 9. Ingeborg Helen Markein ( Norway ) 58</td>\n",
       "      <td>[alias_1349_0, alias_1349_1, alias_1349_18]</td>\n",
       "      <td>[[0, 1], [2, 3], [10, 11]]</td>\n",
       "      <td>[vail colorado, norway]</td>\n",
       "      <td>[[0, 3], [10, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>nba basketball - standings after thursday 's games . cleveland 11 5 .688 5</td>\n",
       "      <td>[alias_1189_0, alias_1189_13]</td>\n",
       "      <td>[[0, 1], [9, 10]]</td>\n",
       "      <td>[nba basketball, cleveland]</td>\n",
       "      <td>[[0, 2], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>Union leaders outraged by wto snub to ilo head . Jordan said the wto 's credibility was at stake over the issue of trade and labour .</td>\n",
       "      <td>[alias_1268_0, alias_1268_1, alias_1268_18]</td>\n",
       "      <td>[[4, 5], [7, 8], [13, 14]]</td>\n",
       "      <td>[ilo, jordan, wto, stake, labour]</td>\n",
       "      <td>[[7, 8], [10, 11], [13, 14], [18, 19], [25, 26]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>Iowa-S Minn fed cattle market quiet , no sales-USDA . Slaughter steers and heifers not tested , compared with Thursday 's close , usda said .</td>\n",
       "      <td>[alias_1277_0, alias_1277_2]</td>\n",
       "      <td>[[1, 2], [23, 24]]</td>\n",
       "      <td>[fed cattle, slaughter, steers, heifers, usda]</td>\n",
       "      <td>[[2, 4], [10, 11], [11, 12], [13, 14], [23, 24]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>soccer - english league standings . Hartlepool 21 6 4 11 23 28 22</td>\n",
       "      <td>[alias_1370_0, alias_1370_94]</td>\n",
       "      <td>[[2, 3], [6, 7]]</td>\n",
       "      <td>[soccer, english league, hartlepool]</td>\n",
       "      <td>[[0, 1], [2, 4], [6, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>vail , Colorado 1996-12-07 . 2. Pernilla Wiberg ( Sweden ) 353</td>\n",
       "      <td>[alias_1349_0, alias_1349_1, alias_1349_41, alias_1349_42]</td>\n",
       "      <td>[[0, 1], [2, 3], [6, 8], [9, 10]]</td>\n",
       "      <td>[vail colorado, pernilla wiberg, sweden]</td>\n",
       "      <td>[[0, 3], [6, 8], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>soccer - english premier league summaries . soccer - english premier league summaries .</td>\n",
       "      <td>[alias_1368_0, alias_1368_0]</td>\n",
       "      <td>[[2, 3], [9, 10]]</td>\n",
       "      <td>[soccer, english premier league, soccer, english premier league, summaries]</td>\n",
       "      <td>[[0, 1], [2, 5], [7, 8], [9, 12], [12, 13]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>oestersund , Sweden 1996-12-07 . oestersund , Sweden 1996-12-07</td>\n",
       "      <td>[alias_1351_0, alias_1351_1, alias_1351_0, alias_1351_1]</td>\n",
       "      <td>[[0, 1], [2, 3], [5, 6], [7, 8]]</td>\n",
       "      <td>[sweden, sweden]</td>\n",
       "      <td>[[2, 3], [7, 8]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>vail , Colorado 1996-12-07 . 25. Hilary Lindh ( U.S. ) 1:19.41</td>\n",
       "      <td>[alias_1345_0, alias_1345_1, alias_1345_37, alias_1345_38]</td>\n",
       "      <td>[[0, 1], [2, 3], [6, 8], [9, 10]]</td>\n",
       "      <td>[vail colorado, hilary lindh, us]</td>\n",
       "      <td>[[0, 3], [6, 8], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>Hindu party forces India parliament to adjourn . new delhi 1996-12-06</td>\n",
       "      <td>[alias_1294_0, alias_1294_1, alias_1294_2]</td>\n",
       "      <td>[[0, 1], [3, 4], [8, 10]]</td>\n",
       "      <td>[india parliament, adjourn, new delhi]</td>\n",
       "      <td>[[3, 5], [6, 7], [8, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>soccer - german first division results / standings . Results of German first division</td>\n",
       "      <td>[alias_1194_0, alias_1194_2]</td>\n",
       "      <td>[[2, 3], [11, 12]]</td>\n",
       "      <td>[soccer, german first division, german first division]</td>\n",
       "      <td>[[0, 1], [2, 5], [11, 14]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>nba basketball - standings after friday 's games . indiana 8 8 .500 8</td>\n",
       "      <td>[alias_1383_0, alias_1383_17]</td>\n",
       "      <td>[[0, 1], [9, 10]]</td>\n",
       "      <td>[nba basketball, friday, indiana 8]</td>\n",
       "      <td>[[0, 2], [5, 6], [9, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>soccer - english league standings . Standings in English league soccer</td>\n",
       "      <td>[alias_1370_0, alias_1370_2]</td>\n",
       "      <td>[[2, 3], [8, 9]]</td>\n",
       "      <td>[soccer, english league, english league, soccer]</td>\n",
       "      <td>[[0, 1], [2, 4], [8, 10], [10, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>nhl ice hockey - friday 's results . ny rangers 6 Toronto 5</td>\n",
       "      <td>[alias_1386_0, alias_1386_3, alias_1386_4]</td>\n",
       "      <td>[[0, 1], [8, 10], [11, 12]]</td>\n",
       "      <td>[friday, ny rangers, toronto]</td>\n",
       "      <td>[[4, 5], [8, 10], [11, 12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>rugby union - little to miss campese farewell . Owen Finegan has recovered from the knocks he took in last weekend 's test against Wales and retains his place in the back-row ahead of Daniel Manu .</td>\n",
       "      <td>[alias_1173_0, alias_1173_1, alias_1173_9, alias_1173_10]</td>\n",
       "      <td>[[0, 2], [6, 7], [9, 11], [24, 25]]</td>\n",
       "      <td>[rugby union, owen finegan, test, wales, daniel manu]</td>\n",
       "      <td>[[0, 2], [9, 11], [22, 23], [24, 25], [34, 36]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>vail , Colorado 1996-12-07 . 19. Stefanie Schuster ( Austria ) 89</td>\n",
       "      <td>[alias_1349_0, alias_1349_1, alias_1349_69]</td>\n",
       "      <td>[[0, 1], [2, 3], [9, 10]]</td>\n",
       "      <td>[vail colorado, stefanie schuster, austria]</td>\n",
       "      <td>[[0, 3], [6, 8], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>Turkey hindered by own landmines on Syrian border . Damascus denies aiding the rebels .</td>\n",
       "      <td>[alias_1304_0, alias_1304_10]</td>\n",
       "      <td>[[6, 7], [9, 10]]</td>\n",
       "      <td>[turkey, landmines, damascus]</td>\n",
       "      <td>[[0, 1], [4, 5], [9, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>nba basketball - standings after friday 's games . golden state 6 13 .316 8 1/2</td>\n",
       "      <td>[alias_1383_0, alias_1383_32]</td>\n",
       "      <td>[[0, 1], [9, 11]]</td>\n",
       "      <td>[nba basketball, friday, golden state]</td>\n",
       "      <td>[[0, 2], [5, 6], [9, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>chonju , South Korea 1996-12-07 . Jaegal Sung-Yeol ( South Korea ) 37.46 ; 3 .</td>\n",
       "      <td>[alias_1360_0, alias_1360_3]</td>\n",
       "      <td>[[2, 4], [9, 11]]</td>\n",
       "      <td>[chonju, south korea, jaegal sungyeol, south korea]</td>\n",
       "      <td>[[0, 1], [2, 4], [6, 8], [9, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179</th>\n",
       "      <td>soccer - english league standings . Scarborough 21 7 9 5 30 27 30</td>\n",
       "      <td>[alias_1370_0, alias_1370_82]</td>\n",
       "      <td>[[2, 3], [6, 7]]</td>\n",
       "      <td>[soccer, english league]</td>\n",
       "      <td>[[0, 1], [2, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>Clean tankers fixtures and enquiries - 2321 gmt . Clean tankers fixtures and enquiries - 2321 gmt .</td>\n",
       "      <td>[alias_1234_0, alias_1234_0]</td>\n",
       "      <td>[[7, 8], [16, 17]]</td>\n",
       "      <td>[tankers, gmt, tankers, gmt]</td>\n",
       "      <td>[[1, 2], [7, 8], [10, 11], [16, 17]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>igls , Austria 1996-12-07 . First run leaders Guenther Huber and breakman Antonio Tartaglia in the Italy I sleigh finished second two-hundredths of a second behind the Americans .</td>\n",
       "      <td>[alias_1353_0, alias_1353_5, alias_1353_6, alias_1353_7, alias_1353_8]</td>\n",
       "      <td>[[2, 3], [8, 10], [12, 14], [16, 17], [27, 28]]</td>\n",
       "      <td>[igls, austria, guenther huber, breakman, antonio tartaglia, italy]</td>\n",
       "      <td>[[0, 1], [2, 3], [8, 10], [11, 12], [12, 14], [16, 17]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>Zimbabwe executes convicted murderer . Zimbabwe hanged a convicted murderer on Friday , bringing to eight the number of executions carried out in the past year .</td>\n",
       "      <td>[alias_1240_0, alias_1240_2]</td>\n",
       "      <td>[[0, 1], [5, 6]]</td>\n",
       "      <td>[zimbabwe, murderer, zimbabwe, murderer, friday]</td>\n",
       "      <td>[[0, 1], [3, 4], [5, 6], [9, 10], [11, 12]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>nfl american football-standings after thursday 's game . buffalo at seattle</td>\n",
       "      <td>[alias_1190_0, alias_1190_49, alias_1190_50]</td>\n",
       "      <td>[[0, 1], [8, 9], [10, 11]]</td>\n",
       "      <td>[thursdays game, buffalo, seattle]</td>\n",
       "      <td>[[4, 7], [8, 9], [10, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2310</th>\n",
       "      <td>cricket - jones hits century as victoria fight back . Jones became the fourth century-maker of the match , equalling the feats of Tasmanian trio David Boon , Shaun Young and Michael DiVenuto .</td>\n",
       "      <td>[alias_1378_0, alias_1378_11, alias_1378_12, alias_1378_13, alias_1378_14]</td>\n",
       "      <td>[[6, 7], [23, 24], [25, 27], [28, 30], [31, 33]]</td>\n",
       "      <td>[jones, victoria, jones, david boon, shaun young]</td>\n",
       "      <td>[[2, 3], [6, 7], [10, 11], [25, 27], [28, 30]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>Chinese girl nearly dies from cigarette smoke . shanghai 1996-12-06</td>\n",
       "      <td>[alias_1334_0, alias_1334_1]</td>\n",
       "      <td>[[0, 1], [8, 9]]</td>\n",
       "      <td>[chinese girl, cigarette smoke, shanghai]</td>\n",
       "      <td>[[0, 2], [5, 7], [8, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>cbot grain / oilseed receipts and shipments . St. Louis 21,346 0</td>\n",
       "      <td>[alias_1282_0, alias_1282_4]</td>\n",
       "      <td>[[0, 1], [8, 10]]</td>\n",
       "      <td>[oilseed, receipts, st louis]</td>\n",
       "      <td>[[3, 4], [4, 5], [8, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>Italian President urges separatists to turn back . Let 's march together , \" Scalfaro , a northerner himself , said .</td>\n",
       "      <td>[alias_1315_0, alias_1315_8]</td>\n",
       "      <td>[[0, 1], [14, 15]]</td>\n",
       "      <td>[italian president, scalfaro, northerner]</td>\n",
       "      <td>[[0, 2], [14, 15], [17, 18]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>golf - zimbabwe open third round scores . 207 Clinton Whitelaw 70 70 67 , Mark Cayeux ( Zimbabwe )</td>\n",
       "      <td>[alias_1376_0, alias_1376_9, alias_1376_10]</td>\n",
       "      <td>[[2, 4], [9, 11], [18, 19]]</td>\n",
       "      <td>[zimbabwe open, clinton whitelaw, zimbabwe]</td>\n",
       "      <td>[[2, 4], [9, 11], [18, 19]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>soccer - blinker ban lifted . london 1996-12-06</td>\n",
       "      <td>[alias_1170_0, alias_1170_1]</td>\n",
       "      <td>[[2, 3], [6, 7]]</td>\n",
       "      <td>[soccer, blinker, ban, london]</td>\n",
       "      <td>[[0, 1], [2, 3], [3, 4], [6, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>poland got money from post-war swiss accounts . Rosati confirmed that the 1949 agreement had provided for granting Switzerland about 53 million francs and most of this sum was repaid with coal exports .</td>\n",
       "      <td>[alias_1216_0, alias_1216_1, alias_1216_31, alias_1216_32]</td>\n",
       "      <td>[[0, 1], [5, 6], [8, 9], [18, 19]]</td>\n",
       "      <td>[poland, got money, swiss accounts, rosati, switzerland, francs, sum]</td>\n",
       "      <td>[[0, 1], [1, 3], [5, 7], [8, 9], [18, 19], [22, 23], [27, 28]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>nhl ice hockey - standings after thursday 's games . toronto 11 15 0 76 89 22</td>\n",
       "      <td>[alias_1186_0, alias_1186_22]</td>\n",
       "      <td>[[0, 1], [10, 11]]</td>\n",
       "      <td>[toronto]</td>\n",
       "      <td>[[10, 11]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>soccer - spanish first division result / standings . Tenerife 15 5 4 6 23 17 19</td>\n",
       "      <td>[alias_1392_0, alias_1392_16]</td>\n",
       "      <td>[[2, 3], [9, 10]]</td>\n",
       "      <td>[soccer, spanish first division, tenerife]</td>\n",
       "      <td>[[0, 1], [2, 5], [9, 10]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                        sentence  \\\n",
       "1613                                                                                                                                                                                                                                                     vail , Colorado 1996-12-07 . vail , Colorado 1996-12-07   \n",
       "203                                                                                                                       cricket - lara endures another miserable day . All-rounder Greg Blewett steered his side to a comfortable victory with an unbeaten 57 in 90 balls to the delight of the 42,442 crowd .   \n",
       "2408                                                                                                                                                                                                                                     nba basketball - standings after friday 's games . charlotte at seattle   \n",
       "1507                                                                                                                                                                                                                                   rtrs - Australian mp John Langmore formally resigns . canberra 1996-12-06   \n",
       "1559                                           tokyo 1996-12-06 . Japan 's Economic Planning Agency has not changed its view that the economy is gradually recovering , despite relatively weak gross domestic product figures released on Tuesday , epa Vice Minister Shimpei Nukaya told reporters on Friday .   \n",
       "883   Multinational commander going back to east Zaire . If he decided it was necessary and safe for the aircrew , he would not hesitate to order airdrops of food for the refugees , even against the wishes of the government in Kinshasa and the Zairean rebels who control much of eastern Zaire , he said .   \n",
       "1928                                                                                                                                                                                                                                chonju , South Korea 1996-12-07 . Horii Manabu ( Japan ) 37.23 seconds ; 2 .   \n",
       "918                                                                                                                                                                                                                                        Ex-minister , son killed in Central Africa unrest . bangui 1996-12-06   \n",
       "1027                                                                                      Plastic surgery gets boost in Brazil . Plastic surgery scares like the case in which Brazilian model Claudia Liz fell into a coma after being anaesthetised for a liposuction in October are not much of a deterrent .   \n",
       "2200                                                                                                                                                                                                                       soccer - scottish league and cup results . soccer - scottish league and cup results .   \n",
       "2221                                                                                                                                                                                                                                 soccer - english league and cup results . Results of English league and cup   \n",
       "1224                                                                                                                                                                                                                                                                               tempe , Ariz . tempe , Ariz .   \n",
       "302                                                                                                                                                                                                                                        nhl ice hockey - standings after thursday 's games . central division   \n",
       "1182                                                                                                                                              Canadian grain statistics weekly . In addition , Statistics Canada indicated the following exports to the U.S. between August and September 1996 , in tonnes :   \n",
       "2433                                                                                                                                                                                                                              nhl ice hockey - standings after friday 's games . new jersey 14 10 1 61 61 29   \n",
       "2197                                                                                                              soccer - vieira saves arsenal with last-minute equaliser . Liverpool suffered an upset first home league defeat of the season , beaten 1-0 by a Guy Whittingham goal for Sheffield Wednesday .   \n",
       "2456                                                                                                                                                                                                                                      nhl ice hockey - standings after friday 's games . chicago at montreal   \n",
       "1715                                                                                                                                                                                                                                        vail , Colorado 1996-12-07 . 9. Ingeborg Helen Markein ( Norway ) 58   \n",
       "363                                                                                                                                                                                                                                   nba basketball - standings after thursday 's games . cleveland 11 5 .688 5   \n",
       "1103                                                                                                                                                                       Union leaders outraged by wto snub to ilo head . Jordan said the wto 's credibility was at stake over the issue of trade and labour .   \n",
       "1176                                                                                                                                                               Iowa-S Minn fed cattle market quiet , no sales-USDA . Slaughter steers and heifers not tested , compared with Thursday 's close , usda said .   \n",
       "2191                                                                                                                                                                                                                                           soccer - english league standings . Hartlepool 21 6 4 11 23 28 22   \n",
       "1730                                                                                                                                                                                                                                              vail , Colorado 1996-12-07 . 2. Pernilla Wiberg ( Sweden ) 353   \n",
       "2044                                                                                                                                                                                                                     soccer - english premier league summaries . soccer - english premier league summaries .   \n",
       "1808                                                                                                                                                                                                                                             oestersund , Sweden 1996-12-07 . oestersund , Sweden 1996-12-07   \n",
       "1687                                                                                                                                                                                                                                              vail , Colorado 1996-12-07 . 25. Hilary Lindh ( U.S. ) 1:19.41   \n",
       "1280                                                                                                                                                                                                                                       Hindu party forces India parliament to adjourn . new delhi 1996-12-06   \n",
       "471                                                                                                                                                                                                                        soccer - german first division results / standings . Results of German first division   \n",
       "2382                                                                                                                                                                                                                                       nba basketball - standings after friday 's games . indiana 8 8 .500 8   \n",
       "2099                                                                                                                                                                                                                                      soccer - english league standings . Standings in English league soccer   \n",
       "2465                                                                                                                                                                                                                                                 nhl ice hockey - friday 's results . ny rangers 6 Toronto 5   \n",
       "167                                                                                                        rugby union - little to miss campese farewell . Owen Finegan has recovered from the knocks he took in last weekend 's test against Wales and retains his place in the back-row ahead of Daniel Manu .   \n",
       "1747                                                                                                                                                                                                                                           vail , Colorado 1996-12-07 . 19. Stefanie Schuster ( Austria ) 89   \n",
       "1360                                                                                                                                                                                                                     Turkey hindered by own landmines on Syrian border . Damascus denies aiding the rebels .   \n",
       "2397                                                                                                                                                                                                                             nba basketball - standings after friday 's games . golden state 6 13 .316 8 1/2   \n",
       "1929                                                                                                                                                                                                                              chonju , South Korea 1996-12-07 . Jaegal Sung-Yeol ( South Korea ) 37.46 ; 3 .   \n",
       "2179                                                                                                                                                                                                                                           soccer - english league standings . Scarborough 21 7 9 5 30 27 30   \n",
       "837                                                                                                                                                                                                          Clean tankers fixtures and enquiries - 2321 gmt . Clean tankers fixtures and enquiries - 2321 gmt .   \n",
       "1838                                                                                                                         igls , Austria 1996-12-07 . First run leaders Guenther Huber and breakman Antonio Tartaglia in the Italy I sleigh finished second two-hundredths of a second behind the Americans .   \n",
       "877                                                                                                                                            Zimbabwe executes convicted murderer . Zimbabwe hanged a convicted murderer on Friday , bringing to eight the number of executions carried out in the past year .   \n",
       "433                                                                                                                                                                                                                                  nfl american football-standings after thursday 's game . buffalo at seattle   \n",
       "2310                                                                                                            cricket - jones hits century as victoria fight back . Jones became the fourth century-maker of the match , equalling the feats of Tasmanian trio David Boon , Shaun Young and Michael DiVenuto .   \n",
       "1539                                                                                                                                                                                                                                         Chinese girl nearly dies from cigarette smoke . shanghai 1996-12-06   \n",
       "1211                                                                                                                                                                                                                                            cbot grain / oilseed receipts and shipments . St. Louis 21,346 0   \n",
       "1423                                                                                                                                                                                       Italian President urges separatists to turn back . Let 's march together , \" Scalfaro , a northerner himself , said .   \n",
       "2283                                                                                                                                                                                                          golf - zimbabwe open third round scores . 207 Clinton Whitelaw 70 70 67 , Mark Cayeux ( Zimbabwe )   \n",
       "124                                                                                                                                                                                                                                                              soccer - blinker ban lifted . london 1996-12-06   \n",
       "729                                                                                                   poland got money from post-war swiss accounts . Rosati confirmed that the 1949 agreement had provided for granting Switzerland about 53 million francs and most of this sum was repaid with coal exports .   \n",
       "307                                                                                                                                                                                                                                nhl ice hockey - standings after thursday 's games . toronto 11 15 0 76 89 22   \n",
       "2516                                                                                                                                                                                                                             soccer - spanish first division result / standings . Tenerife 15 5 4 6 23 17 19   \n",
       "\n",
       "                                                                    aliases_hand  \\\n",
       "1613                    [alias_1344_0, alias_1344_1, alias_1344_0, alias_1344_1]   \n",
       "203                                                 [alias_1177_0, alias_1177_9]   \n",
       "2408                                [alias_1383_0, alias_1383_51, alias_1383_52]   \n",
       "1507                    [alias_1331_0, alias_1331_1, alias_1331_2, alias_1331_3]   \n",
       "1559                                                [alias_1337_0, alias_1337_1]   \n",
       "883                   [alias_1241_0, alias_1241_9, alias_1241_10, alias_1241_11]   \n",
       "1928                                                [alias_1360_0, alias_1360_2]   \n",
       "918                                                 [alias_1245_0, alias_1245_1]   \n",
       "1027                                               [alias_1259_0, alias_1259_12]   \n",
       "2200                                                [alias_1372_0, alias_1372_0]   \n",
       "2221                                                [alias_1373_0, alias_1373_2]   \n",
       "1224                                                [alias_1284_0, alias_1284_0]   \n",
       "302                                                [alias_1186_0, alias_1186_17]   \n",
       "1182                                  [alias_1279_0, alias_1279_3, alias_1279_4]   \n",
       "2433                                               [alias_1385_0, alias_1385_12]   \n",
       "2197   [alias_1371_0, alias_1371_1, alias_1371_13, alias_1371_14, alias_1371_15]   \n",
       "2456                                [alias_1385_0, alias_1385_39, alias_1385_40]   \n",
       "1715                                 [alias_1349_0, alias_1349_1, alias_1349_18]   \n",
       "363                                                [alias_1189_0, alias_1189_13]   \n",
       "1103                                 [alias_1268_0, alias_1268_1, alias_1268_18]   \n",
       "1176                                                [alias_1277_0, alias_1277_2]   \n",
       "2191                                               [alias_1370_0, alias_1370_94]   \n",
       "1730                  [alias_1349_0, alias_1349_1, alias_1349_41, alias_1349_42]   \n",
       "2044                                                [alias_1368_0, alias_1368_0]   \n",
       "1808                    [alias_1351_0, alias_1351_1, alias_1351_0, alias_1351_1]   \n",
       "1687                  [alias_1345_0, alias_1345_1, alias_1345_37, alias_1345_38]   \n",
       "1280                                  [alias_1294_0, alias_1294_1, alias_1294_2]   \n",
       "471                                                 [alias_1194_0, alias_1194_2]   \n",
       "2382                                               [alias_1383_0, alias_1383_17]   \n",
       "2099                                                [alias_1370_0, alias_1370_2]   \n",
       "2465                                  [alias_1386_0, alias_1386_3, alias_1386_4]   \n",
       "167                    [alias_1173_0, alias_1173_1, alias_1173_9, alias_1173_10]   \n",
       "1747                                 [alias_1349_0, alias_1349_1, alias_1349_69]   \n",
       "1360                                               [alias_1304_0, alias_1304_10]   \n",
       "2397                                               [alias_1383_0, alias_1383_32]   \n",
       "1929                                                [alias_1360_0, alias_1360_3]   \n",
       "2179                                               [alias_1370_0, alias_1370_82]   \n",
       "837                                                 [alias_1234_0, alias_1234_0]   \n",
       "1838      [alias_1353_0, alias_1353_5, alias_1353_6, alias_1353_7, alias_1353_8]   \n",
       "877                                                 [alias_1240_0, alias_1240_2]   \n",
       "433                                 [alias_1190_0, alias_1190_49, alias_1190_50]   \n",
       "2310  [alias_1378_0, alias_1378_11, alias_1378_12, alias_1378_13, alias_1378_14]   \n",
       "1539                                                [alias_1334_0, alias_1334_1]   \n",
       "1211                                                [alias_1282_0, alias_1282_4]   \n",
       "1423                                                [alias_1315_0, alias_1315_8]   \n",
       "2283                                 [alias_1376_0, alias_1376_9, alias_1376_10]   \n",
       "124                                                 [alias_1170_0, alias_1170_1]   \n",
       "729                   [alias_1216_0, alias_1216_1, alias_1216_31, alias_1216_32]   \n",
       "307                                                [alias_1186_0, alias_1186_22]   \n",
       "2516                                               [alias_1392_0, alias_1392_16]   \n",
       "\n",
       "                                            spans_hand  \\\n",
       "1613                  [[0, 1], [2, 3], [5, 6], [7, 8]]   \n",
       "203                                  [[2, 3], [9, 11]]   \n",
       "2408                       [[0, 1], [9, 10], [11, 12]]   \n",
       "1507                 [[0, 1], [2, 3], [4, 6], [9, 10]]   \n",
       "1559                                  [[0, 1], [3, 4]]   \n",
       "883             [[6, 7], [41, 42], [44, 45], [51, 52]]   \n",
       "1928                                 [[2, 4], [9, 10]]   \n",
       "918                                  [[5, 7], [9, 10]]   \n",
       "1027                                [[5, 6], [15, 16]]   \n",
       "2200                                [[2, 3], [10, 11]]   \n",
       "2221                                [[2, 3], [10, 11]]   \n",
       "1224                                  [[0, 1], [4, 5]]   \n",
       "302                                 [[0, 1], [10, 12]]   \n",
       "1182                       [[0, 1], [8, 10], [16, 17]]   \n",
       "2433                                [[0, 1], [10, 12]]   \n",
       "2197     [[2, 3], [4, 5], [9, 10], [25, 27], [29, 31]]   \n",
       "2456                      [[0, 1], [10, 11], [12, 13]]   \n",
       "1715                        [[0, 1], [2, 3], [10, 11]]   \n",
       "363                                  [[0, 1], [9, 10]]   \n",
       "1103                        [[4, 5], [7, 8], [13, 14]]   \n",
       "1176                                [[1, 2], [23, 24]]   \n",
       "2191                                  [[2, 3], [6, 7]]   \n",
       "1730                 [[0, 1], [2, 3], [6, 8], [9, 10]]   \n",
       "2044                                 [[2, 3], [9, 10]]   \n",
       "1808                  [[0, 1], [2, 3], [5, 6], [7, 8]]   \n",
       "1687                 [[0, 1], [2, 3], [6, 8], [9, 10]]   \n",
       "1280                         [[0, 1], [3, 4], [8, 10]]   \n",
       "471                                 [[2, 3], [11, 12]]   \n",
       "2382                                 [[0, 1], [9, 10]]   \n",
       "2099                                  [[2, 3], [8, 9]]   \n",
       "2465                       [[0, 1], [8, 10], [11, 12]]   \n",
       "167                [[0, 2], [6, 7], [9, 11], [24, 25]]   \n",
       "1747                         [[0, 1], [2, 3], [9, 10]]   \n",
       "1360                                 [[6, 7], [9, 10]]   \n",
       "2397                                 [[0, 1], [9, 11]]   \n",
       "1929                                 [[2, 4], [9, 11]]   \n",
       "2179                                  [[2, 3], [6, 7]]   \n",
       "837                                 [[7, 8], [16, 17]]   \n",
       "1838   [[2, 3], [8, 10], [12, 14], [16, 17], [27, 28]]   \n",
       "877                                   [[0, 1], [5, 6]]   \n",
       "433                         [[0, 1], [8, 9], [10, 11]]   \n",
       "2310  [[6, 7], [23, 24], [25, 27], [28, 30], [31, 33]]   \n",
       "1539                                  [[0, 1], [8, 9]]   \n",
       "1211                                 [[0, 1], [8, 10]]   \n",
       "1423                                [[0, 1], [14, 15]]   \n",
       "2283                       [[2, 4], [9, 11], [18, 19]]   \n",
       "124                                   [[2, 3], [6, 7]]   \n",
       "729                 [[0, 1], [5, 6], [8, 9], [18, 19]]   \n",
       "307                                 [[0, 1], [10, 11]]   \n",
       "2516                                 [[2, 3], [9, 10]]   \n",
       "\n",
       "                                                                        aliases_bootleg  \\\n",
       "1613                                                     [vail colorado, vail colorado]   \n",
       "203                                                      [lara, greg blewett, 90 balls]   \n",
       "2408                                       [nba basketball, friday, charlotte, seattle]   \n",
       "1507                                                    [rtrs, john langmore, canberra]   \n",
       "1559   [tokyo, japan, economic planning, gross domestic product, vice minister, friday]   \n",
       "883                                                         [airdrops, kinshasa, zaire]   \n",
       "1928                                                       [chonju, south korea, japan]   \n",
       "918                                                            [central africa, bangui]   \n",
       "1027   [plastic surgery, brazil, plastic surgery, scares, coma, liposuction, deterrent]   \n",
       "2200                                 [soccer, scottish league, soccer, scottish league]   \n",
       "2221                                           [soccer, english league, english league]   \n",
       "1224                                                           [tempe ariz, tempe ariz]   \n",
       "302                                                                  [central division]   \n",
       "1182                                     [grain, statistics canada, us, september 1996]   \n",
       "2433                                                               [friday, new jersey]   \n",
       "2197  [soccer, vieira, saves, arsenal, liverpool, guy whittingham, sheffield wednesday]   \n",
       "2456                                                        [friday, chicago, montreal]   \n",
       "1715                                                            [vail colorado, norway]   \n",
       "363                                                         [nba basketball, cleveland]   \n",
       "1103                                                  [ilo, jordan, wto, stake, labour]   \n",
       "1176                                     [fed cattle, slaughter, steers, heifers, usda]   \n",
       "2191                                               [soccer, english league, hartlepool]   \n",
       "1730                                           [vail colorado, pernilla wiberg, sweden]   \n",
       "2044        [soccer, english premier league, soccer, english premier league, summaries]   \n",
       "1808                                                                   [sweden, sweden]   \n",
       "1687                                                  [vail colorado, hilary lindh, us]   \n",
       "1280                                             [india parliament, adjourn, new delhi]   \n",
       "471                              [soccer, german first division, german first division]   \n",
       "2382                                                [nba basketball, friday, indiana 8]   \n",
       "2099                                   [soccer, english league, english league, soccer]   \n",
       "2465                                                      [friday, ny rangers, toronto]   \n",
       "167                               [rugby union, owen finegan, test, wales, daniel manu]   \n",
       "1747                                        [vail colorado, stefanie schuster, austria]   \n",
       "1360                                                      [turkey, landmines, damascus]   \n",
       "2397                                             [nba basketball, friday, golden state]   \n",
       "1929                                [chonju, south korea, jaegal sungyeol, south korea]   \n",
       "2179                                                           [soccer, english league]   \n",
       "837                                                        [tankers, gmt, tankers, gmt]   \n",
       "1838                [igls, austria, guenther huber, breakman, antonio tartaglia, italy]   \n",
       "877                                    [zimbabwe, murderer, zimbabwe, murderer, friday]   \n",
       "433                                                  [thursdays game, buffalo, seattle]   \n",
       "2310                                  [jones, victoria, jones, david boon, shaun young]   \n",
       "1539                                          [chinese girl, cigarette smoke, shanghai]   \n",
       "1211                                                      [oilseed, receipts, st louis]   \n",
       "1423                                          [italian president, scalfaro, northerner]   \n",
       "2283                                        [zimbabwe open, clinton whitelaw, zimbabwe]   \n",
       "124                                                      [soccer, blinker, ban, london]   \n",
       "729               [poland, got money, swiss accounts, rosati, switzerland, francs, sum]   \n",
       "307                                                                           [toronto]   \n",
       "2516                                         [soccer, spanish first division, tenerife]   \n",
       "\n",
       "                                                        spans_bootleg  \n",
       "1613                                                 [[0, 3], [5, 8]]  \n",
       "203                                       [[2, 3], [9, 11], [23, 25]]  \n",
       "2408                              [[0, 2], [5, 6], [9, 10], [11, 12]]  \n",
       "1507                                        [[0, 1], [4, 6], [9, 10]]  \n",
       "1559           [[0, 1], [3, 4], [5, 7], [23, 26], [32, 34], [39, 40]]  \n",
       "883                                    [[26, 27], [41, 42], [51, 52]]  \n",
       "1928                                        [[0, 1], [2, 4], [9, 10]]  \n",
       "918                                                 [[5, 7], [9, 10]]  \n",
       "1027  [[0, 2], [5, 6], [7, 9], [9, 10], [22, 23], [28, 29], [36, 37]]  \n",
       "2200                               [[0, 1], [2, 4], [8, 9], [10, 12]]  \n",
       "2221                                       [[0, 1], [2, 4], [10, 12]]  \n",
       "1224                                                 [[0, 3], [4, 7]]  \n",
       "302                                                        [[10, 12]]  \n",
       "1182                            [[1, 2], [8, 10], [16, 17], [20, 22]]  \n",
       "2433                                               [[6, 7], [10, 12]]  \n",
       "2197    [[0, 1], [2, 3], [3, 4], [4, 5], [9, 10], [25, 27], [29, 31]]  \n",
       "2456                                     [[6, 7], [10, 11], [12, 13]]  \n",
       "1715                                               [[0, 3], [10, 11]]  \n",
       "363                                                 [[0, 2], [9, 10]]  \n",
       "1103                 [[7, 8], [10, 11], [13, 14], [18, 19], [25, 26]]  \n",
       "1176                 [[2, 4], [10, 11], [11, 12], [13, 14], [23, 24]]  \n",
       "2191                                         [[0, 1], [2, 4], [6, 7]]  \n",
       "1730                                        [[0, 3], [6, 8], [9, 10]]  \n",
       "2044                      [[0, 1], [2, 5], [7, 8], [9, 12], [12, 13]]  \n",
       "1808                                                 [[2, 3], [7, 8]]  \n",
       "1687                                        [[0, 3], [6, 8], [9, 10]]  \n",
       "1280                                        [[3, 5], [6, 7], [8, 10]]  \n",
       "471                                        [[0, 1], [2, 5], [11, 14]]  \n",
       "2382                                        [[0, 2], [5, 6], [9, 11]]  \n",
       "2099                              [[0, 1], [2, 4], [8, 10], [10, 11]]  \n",
       "2465                                      [[4, 5], [8, 10], [11, 12]]  \n",
       "167                   [[0, 2], [9, 11], [22, 23], [24, 25], [34, 36]]  \n",
       "1747                                        [[0, 3], [6, 8], [9, 10]]  \n",
       "1360                                        [[0, 1], [4, 5], [9, 10]]  \n",
       "2397                                        [[0, 2], [5, 6], [9, 11]]  \n",
       "1929                                [[0, 1], [2, 4], [6, 8], [9, 11]]  \n",
       "2179                                                 [[0, 1], [2, 4]]  \n",
       "837                              [[1, 2], [7, 8], [10, 11], [16, 17]]  \n",
       "1838          [[0, 1], [2, 3], [8, 10], [11, 12], [12, 14], [16, 17]]  \n",
       "877                       [[0, 1], [3, 4], [5, 6], [9, 10], [11, 12]]  \n",
       "433                                        [[4, 7], [8, 9], [10, 11]]  \n",
       "2310                   [[2, 3], [6, 7], [10, 11], [25, 27], [28, 30]]  \n",
       "1539                                         [[0, 2], [5, 7], [8, 9]]  \n",
       "1211                                        [[3, 4], [4, 5], [8, 10]]  \n",
       "1423                                     [[0, 2], [14, 15], [17, 18]]  \n",
       "2283                                      [[2, 4], [9, 11], [18, 19]]  \n",
       "124                                  [[0, 1], [2, 3], [3, 4], [6, 7]]  \n",
       "729    [[0, 1], [1, 3], [5, 7], [8, 9], [18, 19], [22, 23], [27, 28]]  \n",
       "307                                                        [[10, 11]]  \n",
       "2516                                        [[0, 1], [2, 5], [9, 10]]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orig_mentions_df = load_mentions(nq_sample_orig)\n",
    "bootleg_mentions_df = load_mentions(nq_sample_bootleg)\n",
    "\n",
    "# join dataframes and sample\n",
    "res = pd.merge(orig_mentions_df, bootleg_mentions_df, on=['sentence'], suffixes=['_hand', '_bootleg'])\n",
    "display(res.sample(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK = \"kore\"\n",
    "if BENCHMARK == \"kore\":\n",
    "    data_dir = \"/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/kore50_0114/filtered\"\n",
    "    entity_dir = \"/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/kore50_0114/filtered/entity_db\"\n",
    "    alias_map = \"alias2qids.json\"\n",
    "    test_file = \"test.jsonl\"\n",
    "    raw_test_file = Path(data_dir) / test_file\n",
    "elif BENCHMARK == \"rss\":\n",
    "    data_dir = \"/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/rss500wiki_0114/filtered\"\n",
    "    entity_dir = \"/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/rss500wiki_0114/filtered/entity_db\"\n",
    "    alias_map = \"alias2qids.json\"\n",
    "    test_file = \"test_rss500.jsonl\"\n",
    "    raw_test_file = Path(data_dir) / test_file\n",
    "elif BENCHMARK == \"aida\":\n",
    "    data_dir = \"/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0114/filtered\"\n",
    "    entity_dir = \"/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0114/filtered/entity_db\"\n",
    "    alias_map = \"alias2qids.json\"\n",
    "    test_file = \"test.jsonl\"\n",
    "    raw_test_file = Path(data_dir) / test_file\n",
    "else:\n",
    "    data_dir = \"/dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered\"\n",
    "    entity_dir = root_dir / 'data/wiki_entity_data'\n",
    "    alias_map = \"alias2qids_wiki_filt.json\"\n",
    "    test_file = \"test_bootleg_men.jsonl\"\n",
    "    raw_test_file = Path(data_dir) / test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.utils.parser.parser_utils import parse_boot_and_emm_args\n",
    "from bootleg.utils.utils import dump_yaml_file, load_yaml_file\n",
    "\n",
    "config_in_path = root_dir / 'models/bootleg_cased_mini/bootleg_config.yaml'\n",
    "\n",
    "config_args = load_yaml_file(config_in_path)\n",
    "\n",
    "# decrease number of data threads as this is a small file\n",
    "config_args[\"run_config\"][\"dataset_threads\"] = 2\n",
    "config_args[\"run_config\"][\"log_level\"] = \"info\"\n",
    "# set the model checkpoint path \n",
    "config_args[\"emmental\"][\"model_path\"] = str(root_dir / 'models/bootleg_cased_mini/bootleg_wiki.pth')\n",
    "\n",
    "# set the path for the entity db and candidate map\n",
    "config_args[\"data_config\"][\"entity_dir\"] = entity_dir\n",
    "config_args[\"data_config\"][\"alias_cand_map\"] = alias_map\n",
    "\n",
    "# set the data path and kore50 test file \n",
    "config_args[\"data_config\"][\"data_dir\"] = data_dir\n",
    "\n",
    "# to speed things up for the tutorial, we have already prepped the data with the mentions detected by Bootleg\n",
    "config_args[\"data_config\"][\"test_dataset\"][\"file\"] = test_file\n",
    "\n",
    "# set the embedding paths \n",
    "config_args[\"data_config\"][\"emb_dir\"] =  str(root_dir / 'data/emb_data')\n",
    "config_args[\"data_config\"][\"word_embedding\"][\"cache_dir\"] =  str(root_dir / 'data/emb_data/pretrained_bert_models')\n",
    "\n",
    "# set the devie if on CPU\n",
    "config_args[\"emmental\"][\"device\"] = device\n",
    "\n",
    "# save the new args (helps if you want to run things via command line)\n",
    "# config_out_path = root_dir / f'models/bootleg_wiki/bootleg_wiki_config_eval_{BENCHMARK}.yaml'\n",
    "# dump_yaml_file(config_out_path, config_args)\n",
    "config_args = parse_boot_and_emm_args(config_args) # or you can pass in the config_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/datasets/dataset.py:913: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  guid_dtype = np.dtype(\n",
      "/dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/numpy/core/memmap.py:230: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  descr = dtypedescr(dtype)\n",
      "Building sent idx to row idx mapping: 100%|██████████| 50/50 [00:00<00:00, 6831.78it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = run_model(mode=\"eval\", config=config_args)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/datasets/dataset.py:913: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  guid_dtype = np.dtype(\n",
      "Iterating over aliases: 100%|██████████| 15202497/15202497 [03:44<00:00, 67597.03it/s]\n",
      "Reading in /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_dataset_files/create_examples_input/out_1.jsonl: 100%|██████████| 1232/1232 [00:03<00:00, 395.33it/s]\n",
      "Reading in /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_dataset_files/create_examples_input/out_0.jsonl: 100%|██████████| 1233/1233 [00:03<00:00, 390.64it/s]\n",
      "/dfs/scratch0/lorr1/env_bootleg_38/lib/python3.8/site-packages/numpy/core/memmap.py:230: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  descr = dtypedescr(dtype)\n",
      "Processing /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_dataset_files/create_examples_output/out_1.jsonl: 100%|██████████| 1238/1238 [00:00<00:00, 2474.58it/s]\n",
      "Processing /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_dataset_files/create_examples_output/out_0.jsonl: 100%|██████████| 1256/1256 [00:00<00:00, 2581.89it/s]\n",
      "Checking sentence uniqueness: 100%|██████████| 2494/2494 [00:00<00:00, 27197.78it/s]\n",
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/datasets/dataset.py:1185: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  X_dict[\"sent_idx\"] = torch.from_numpy(mmap_file[\"sent_idx\"])\n",
      "Reading values for marisa trie: 100%|██████████| 5832699/5832699 [00:05<00:00, 1107773.68it/s]\n",
      "Processing types: 100%|██████████| 1247/1247 [00:00<00:00, 7196.24it/s]\n",
      "Processing types: 100%|██████████| 1247/1247 [00:00<00:00, 7266.24it/s]\n",
      "Building type data: 100%|██████████| 2/2 [00:00<00:00,  9.38it/s]\n",
      "Verifying type labels: 100%|██████████| 2494/2494 [00:00<00:00, 25828.24it/s]\n",
      "Reading in /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_slice_files/create_examples_input/out_0.jsonl: 100%|██████████| 1233/1233 [00:00<00:00, 26032.69it/s]\n",
      "Reading in /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_slice_files/create_examples_input/out_1.jsonl: 100%|██████████| 1232/1232 [00:00<00:00, 26057.10it/s]\n",
      "Processing /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_slice_files/create_examples_output/out_1.jsonl: 100%|██████████| 1232/1232 [00:00<00:00, 4247.38it/s]\n",
      "Processing /dfs/scratch0/lorr1/projects/bootleg-data/data/benchmarks/aida_0928_nosep/filtered/prep/prep_test_slice_files/create_examples_output/out_0.jsonl: 100%|██████████| 1233/1233 [00:00<00:00, 3039.24it/s]\n",
      "Checking sentence uniqueness: 100%|██████████| 2465/2465 [00:00<00:00, 20538.30it/s]\n",
      "Building sent idx to row idx mapping: 100%|██████████| 2465/2465 [00:00<00:00, 17622.25it/s]\n",
      "Evaluating Bootleg (test): 100%|██████████| 78/78 [01:07<00:00,  1.15it/s]\n",
      "100%|██████████| 2494/2494 [00:00<00:00, 18789.69it/s]\n",
      "Reading values for marisa trie: 100%|██████████| 2465/2465 [00:00<00:00, 825268.15it/s]\n",
      "Building sent_idx, alias_list_pos mapping: 100%|██████████| 10290/10290 [00:00<00:00, 105498.64it/s]\n",
      "Reading values for marisa trie: 100%|██████████| 10290/10290 [00:00<00:00, 420624.01it/s]\n",
      "Writing data: 100%|██████████| 1233/1233 [00:02<00:00, 477.02it/s]\n",
      "Writing data: 100%|██████████| 1232/1232 [00:00<00:00, 7633.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# parse the args for running\n",
    "bootleg_label_file, bootleg_emb_file = run_model(mode=\"dump_preds\", config=config_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2465/2465 [00:00<00:00, 5308.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import score_predictions, load_title_map, load_cand_map\n",
    "\n",
    "bootleg_errors = score_predictions(raw_test_file, bootleg_label_file,\n",
    "                                   qid2title,\n",
    "                                   cands_map=load_cand_map(entity_dir, \"alias2qids.json\"),\n",
    "                                   type_symbols=None, kg_symbols=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11360, 18)\n",
      "(11360, 18)\n",
      "(0, 18)\n"
     ]
    }
   ],
   "source": [
    "print(bootleg_errors.shape)\n",
    "print(bootleg_errors[bootleg_errors[\"pred_qid\"] != bootleg_errors[\"gold_qid\"]].shape)\n",
    "print(bootleg_errors[bootleg_errors[\"pred_qid\"] == bootleg_errors[\"gold_qid\"]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prediction file\n",
    "def load_predictions(file):\n",
    "    lines = {}\n",
    "    num_lines = sum(1 for line in open(file))\n",
    "    with open(file) as f:\n",
    "        for line in tqdm(f, total=num_lines):\n",
    "            line = ujson.loads(line)\n",
    "            lines[line[\"sent_idx_unq\"]] = line\n",
    "    return lines\n",
    "    \n",
    "# Reads in the files, joining on the sentence index\n",
    "# If AIDA, we only keep the second sentence predictions as we merge the sentences together\n",
    "def read_label_file(label_file, test_file, is_aida=False, threshold=0.0):\n",
    "    num_lines = sum(1 for line in open(test_file))\n",
    "    preds = load_predictions(label_file)\n",
    "    gold_sentences = {}\n",
    "    pred_sentences = {}\n",
    "    with open(test_file) as f:\n",
    "        for line in tqdm(f, total=num_lines):\n",
    "            line = ujson.loads(line)\n",
    "            offset_span = len(line[\"sentence\"].split(\" . \")[0].strip().split())+1\n",
    "            sent_idx = line[\"sent_idx_unq\"]\n",
    "            gold_qids = line[\"qids\"]\n",
    "            gold_spans = line[\"spans\"]\n",
    "            gold_aliases = line[\"aliases\"]\n",
    "            gold_golds = line[\"gold\"]\n",
    "            if is_aida:\n",
    "                filtered_values = list(filter(lambda x: x[3] is True, zip(gold_qids, gold_spans, gold_aliases, gold_golds)))\n",
    "                if len(filtered_values) > 0:\n",
    "                    gold_qids, gold_spans, gold_aliases, gold_golds = zip(*filtered_values)\n",
    "                else:\n",
    "                    gold_qids, gold_spans, gold_aliases, gold_golds = [], [], [], []\n",
    "                    \n",
    "            pred_aliases = preds.get(sent_idx, {}).get(\"aliases\", [])\n",
    "            pred_qids = preds.get(sent_idx, {}).get(\"qids\", [])\n",
    "            pred_spans = preds.get(sent_idx, {}).get(\"spans\", [])\n",
    "            pred_probs = preds.get(sent_idx, {}).get(\"probs\", [])\n",
    "            pred_cand_probs = preds.get(sent_idx, {}).get(\"cand_probs\", [])\n",
    "            pred_cands = preds.get(sent_idx, {}).get(\"cands\", [])\n",
    "            assert len(pred_aliases) == len(pred_qids) == len(pred_spans) == len(pred_probs) == len(pred_cand_probs) == len(pred_cands)\n",
    "            filt_qids, filt_spans, filt_aliases, filt_probs, filt_cand_probs, filt_cands = [], [], [], [], [], []\n",
    "            for q, s, a, p, pcp, pc in zip(pred_qids, pred_spans, pred_aliases, pred_probs, pred_cand_probs, pred_cands):\n",
    "                if p > threshold:\n",
    "                    # As we stitch the first sentence of the doc with the remaining sentences,\n",
    "                    # we must undo that when scoring\n",
    "                    if is_aida:\n",
    "                        if s[0] >= offset_span:\n",
    "                            filt_qids.append(q)\n",
    "                            filt_spans.append(s)\n",
    "                            filt_aliases.append(a)\n",
    "                            filt_probs.append(p)\n",
    "                            filt_cand_probs.append(pcp)\n",
    "                            filt_cands.append(pc)\n",
    "                    else:\n",
    "                        filt_qids.append(q)\n",
    "                        filt_spans.append(s)\n",
    "                        filt_aliases.append(a)\n",
    "                        filt_probs.append(p)\n",
    "                        filt_cand_probs.append(pcp)\n",
    "                        filt_cands.append(pc)\n",
    "                        \n",
    "            gold_sentences[sent_idx] = {\n",
    "                \"sentence\": line[\"sentence\"],\n",
    "                \"sent_idx\": sent_idx,\n",
    "                \"doc_sent_idx\": line.get(\"doc_sent_idx\", line.get(\"doc_id\", -1)),\n",
    "                \"aliases\": gold_aliases,\n",
    "                \"qids\": gold_qids,\n",
    "                \"spans\": gold_spans\n",
    "            }\n",
    "            \n",
    "            pred_sentences[sent_idx] = {\n",
    "                \"sentence\": line[\"sentence\"],\n",
    "                \"sent_idx\": sent_idx,\n",
    "                \"doc_sent_idx\": line.get(\"doc_sent_idx\", line.get(\"doc_id\", -1)),\n",
    "                \"aliases\": filt_aliases,\n",
    "                \"qids\": filt_qids,\n",
    "                \"spans\": filt_spans,\n",
    "                \"probs\": filt_probs,\n",
    "                \"cands\": filt_cands,\n",
    "                \"cand_probs\": filt_cand_probs\n",
    "            }\n",
    "                \n",
    "    return gold_sentences, pred_sentences\n",
    "\n",
    "def print_precision_and_recall(gold_sentences, pred_sentences):\n",
    "    print(\"SENTENCES\", len(gold_sentences))\n",
    "    micro_crc, micro_gold, micro_pred = 0, 0, 0\n",
    "    crc = defaultdict(int)\n",
    "    for sent_idx in gold_sentences:\n",
    "        gold_qids = gold_sentences[sent_idx][\"qids\"]\n",
    "        pred_qids = pred_sentences[sent_idx][\"qids\"]\n",
    "        num_intersection = len(list((Counter(gold_qids) & Counter(pred_qids)).elements()))\n",
    "        crc[sent_idx] = num_intersection\n",
    "        micro_crc += crc[sent_idx]\n",
    "        micro_gold += len(gold_qids)\n",
    "        micro_pred += len(pred_qids)\n",
    "    macro_recall = np.mean([crc[sent_idx]/len(gold_sentences[sent_idx][\"qids\"]) for sent_idx in gold_sentences])\n",
    "    macro_prec = np.mean([crc[sent_idx]/len(pred_sentences[sent_idx][\"qids\"])\n",
    "                          if len(pred_sentences[sent_idx][\"qids\"]) > 0\n",
    "                          else 1 for sent_idx in gold_sentences])\n",
    "    print(\"MICRO\")\n",
    "    micro_prec = micro_crc/micro_pred\n",
    "    micro_recall = micro_crc/micro_gold\n",
    "    print(\"PREC\", micro_prec, \"RECALL\", micro_recall, \"F1\", (2*micro_prec*micro_recall/(micro_prec + micro_recall)))\n",
    "    print(\"MACRO\")\n",
    "    print(\"PREC\", macro_prec, \"RECALL\", macro_recall, \"F1\", (2*macro_prec*macro_recall/(macro_prec + macro_recall)))\n",
    "    \n",
    "    \n",
    "def span_overlap(a, b):\n",
    "    return max(0, min(a[1], b[1]) - max(a[0], b[0]))\n",
    "\n",
    "def create_error(gold_qid, gold_alias, gold_span, gold_label, pred_qid, pred_alias, pred_span, sent_obj, gold_aliases, gold_qids, gold_spans, found_aliases, found_spans, pred_qids, error_type):\n",
    "    return {\n",
    "        \"error_type\": error_type,\n",
    "        \"doc_idx\": sent_obj[\"doc_sent_idx\"],\n",
    "        \"sentence\": sent_obj[\"sentence\"],\n",
    "        \"gold_qid\": gold_qid,\n",
    "        \"gold_title\": qid2title.get(gold_qid, \"\"),\n",
    "        \"gold_alias\": gold_alias,\n",
    "        \"gold_label\": gold_label,\n",
    "        \"gold_span\": gold_span,\n",
    "        \"pred_qid\": pred_qid,\n",
    "        \"pred_title\": qid2title.get(pred_qid, \"\"),\n",
    "        \"pred_alias\": pred_alias,\n",
    "        \"pred_span\": pred_span,\n",
    "        \"all_gold_aliases\": gold_aliases,\n",
    "        \"all_gold_qids\": gold_qids,\n",
    "        \"all_gold_spans\": gold_spans,\n",
    "        \"all_pred_aliases\": found_aliases,\n",
    "        \"all_pred_spans\": found_spans,\n",
    "        \"all_pred_qids\": pred_qids\n",
    "    }\n",
    "\n",
    "def compute_errors(gold_sentence, pred_sentence):\n",
    "    # read in first file and map by index for fast retrieval\n",
    "    total_mentions = 0\n",
    "    correct_mentions = 0\n",
    "    pred_mentions = 0\n",
    "    errors = []\n",
    "    gold_aliases = gold_sentence['aliases']\n",
    "    gold_spans = gold_sentence['spans']\n",
    "    gold_qids = gold_sentence['qids']\n",
    "    is_gold_label = gold_sentence.get('gold', [True]*len(gold_aliases))\n",
    "\n",
    "    pred_aliases = pred_sentence['aliases']\n",
    "    pred_spans = pred_sentence['spans']\n",
    "    pred_qids = pred_sentence['qids']\n",
    "\n",
    "    total_mentions += len(gold_aliases)\n",
    "    pred_mentions += len(pred_qids)\n",
    "\n",
    "    for gold_alias, gold_qid, gold_span, gold_label in zip(gold_aliases, gold_qids, gold_spans, is_gold_label):\n",
    "        gold_span_start, gold_span_end = gold_span\n",
    "        overlap_scores = np.array([span_overlap(gold_span, ps) for ps in pred_spans])\n",
    "        # print(gold_span, pred_spans, overlap_scores, gold_sentence['sentence'])\n",
    "        if any([o > 0 for o in overlap_scores]):\n",
    "            pred_idx = np.argmax(overlap_scores)\n",
    "            if gold_qid == pred_qids[pred_idx]:\n",
    "                correct_mentions += 1\n",
    "                errors.append(create_error(gold_qid, gold_alias, gold_span, gold_label, pred_qids[pred_idx], pred_aliases[pred_idx], pred_spans[pred_idx], gold_sentence, gold_aliases,\n",
    "                                                        gold_qids, gold_spans, pred_aliases, pred_spans,\n",
    "                                                        pred_qids, error_type='correct'))\n",
    "            else:\n",
    "                errors.append(create_error(gold_qid, gold_alias, gold_span, gold_label, pred_qids[pred_idx], pred_aliases[pred_idx], pred_spans[pred_idx], gold_sentence, gold_aliases,\n",
    "                                                        gold_qids, gold_spans, pred_aliases, pred_spans,\n",
    "                                                        pred_qids, error_type='wrong_entity'))\n",
    "        else:\n",
    "            errors.append(create_error(gold_qid, gold_alias, gold_span, gold_label, \"N/A\", \"N/A\", \"N/A\", gold_sentence, gold_aliases, gold_qids,\n",
    "                                                        gold_spans, pred_aliases, pred_spans,\n",
    "                                                         pred_qids, error_type='missing_mention'))\n",
    "\n",
    "    for pred_alias, pred_span, pred_qid in zip(pred_aliases, pred_spans, pred_qids):\n",
    "        pred_span_start, pred_span_end = pred_span\n",
    "        fuzzy_gold_left = [pred_span_start-1,pred_span_end]\n",
    "        fuzzy_gold_right = [pred_span_start+1,pred_span_end]\n",
    "        if pred_span not in gold_spans and fuzzy_gold_left not in gold_spans and fuzzy_gold_right not in gold_spans and pred_qid != 'NC':\n",
    "            errors.append(create_error(\"N/A\", \"N/A\", \"N/A\", \"N/A\", pred_qid, pred_alias, pred_span, gold_sentence, gold_aliases, gold_qids, gold_spans, pred_aliases, pred_spans,\n",
    "                                                      pred_qids, error_type='extra_mention'))\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2465/2465 [00:00<00:00, 17368.52it/s]\n",
      "100%|██████████| 2465/2465 [00:00<00:00, 45792.39it/s]\n"
     ]
    }
   ],
   "source": [
    "gold_sentences, pred_sentences = read_label_file(bootleg_label_file, nq_sample_orig, is_aida=True, threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCES 2465\n",
      "MICRO\n",
      "PREC 0.5469678953626635 RECALL 0.717948717948718 F1 0.6209024296182029\n",
      "MACRO\n",
      "PREC 0.6303627167145527 RECALL 0.7115612237088426 F1 0.6685053490353806\n"
     ]
    }
   ],
   "source": [
    "# Generate errors\n",
    "print_precision_and_recall(gold_sentences, pred_sentences)\n",
    "all_errors = []\n",
    "for sent_idx, gold_sent in gold_sentences.items():\n",
    "    pred_sent = pred_sentences[sent_idx]\n",
    "    all_errors.extend(compute_errors(gold_sent, pred_sent))\n",
    "    \n",
    "error_df = pd.DataFrame(all_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df[error_df[\"error_type\"] == \"extra_mention\"].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, spacy, nltk, re\n",
    "from bootleg.end2end.extract_mentions import get_new_to_old_dict, get_lnrm\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "ALL_STOPWORDS = nlp.Defaults.stop_words\n",
    "PUNC = string.punctuation\n",
    "KEEP_POS = {\"PROPN\", \"NOUN\"} # ADJ, VERB, ADV, SYM\n",
    "PLURAL = {\"s\", \"'s\"}\n",
    "table = str.maketrans(\n",
    "    dict.fromkeys(PUNC)\n",
    ")  # OR {key: None for key in string.punctuation}\n",
    "\n",
    "\n",
    "def find_aliases_in_sentence_new(sentence, all_aliases, max_alias_len=6):\n",
    "    used_aliases = []\n",
    "    # Remove multiple spaces and replace with single - tokenization eats multiple spaces but ngrams doesn't which can cause parse issues\n",
    "    sentence = re.sub(\" +\", \" \", sentence)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    split_sent = sentence.split()\n",
    "    new_to_old_span = get_new_to_old_dict(split_sent)\n",
    "    # find largest aliases first\n",
    "    for n in range(max_alias_len + 1, 0, -1):\n",
    "        grams = nltk.ngrams(doc, n)\n",
    "        j_st = -1\n",
    "        j_end = n - 1\n",
    "        for gram_words in grams:\n",
    "            j_st += 1\n",
    "            j_end += 1\n",
    "            j_st_adjusted = new_to_old_span[j_st]\n",
    "            j_end_adjusted = new_to_old_span[j_end]\n",
    "            # Check if nlp has split the word and we are looking at a subword mention - which we don't want\n",
    "            is_subword = j_st_adjusted == j_end_adjusted\n",
    "            if j_st > 0:\n",
    "                is_subword = is_subword | (j_st_adjusted == new_to_old_span[j_st - 1])\n",
    "            # j_end is exclusive and should be a new word from the previous j_end-1\n",
    "            is_subword = is_subword | (j_end_adjusted == new_to_old_span[j_end - 1])\n",
    "            if is_subword:\n",
    "                continue\n",
    "            # Assert we are a full word\n",
    "            assert j_st_adjusted != j_end_adjusted, f\"Something went wrong getting mentions for {sentence}\"\n",
    "            # If single word and not in a POS we care about, skip\n",
    "            if len(gram_words) == 1 and gram_words[0].pos_ not in KEEP_POS:\n",
    "                continue\n",
    "            # If multiple word and not any word in a POS we care about, skip\n",
    "            if len(gram_words) > 1 and not any(g.pos_ in KEEP_POS for g in gram_words):\n",
    "                continue\n",
    "            # print(\"@\", gram_words, [g.pos_ for g in gram_words])\n",
    "            # If we are part of a proper noun, make sure there isn't another part of the proper noun to the \n",
    "            # left or right - this means we didn't have the entire name in our alias and we should skip\n",
    "            if len(gram_words) == 1 and gram_words[0].pos_ == \"PROPN\":\n",
    "                if j_st > 0 and doc[j_st - 1].pos_ == \"PROPN\":\n",
    "                    continue\n",
    "                # End spans are exclusive so no +1\n",
    "                if j_end < len(doc) and doc[j_end].pos_ == \"PROPN\":\n",
    "                    continue\n",
    "            # print(\"3\", j_st, gram_words, [g.pos_ for g in gram_words])\n",
    "            # We don't want punctuation words to be used at the beginning/end unless it's capitalized\n",
    "            # or first word of sentence\n",
    "            if (\n",
    "                gram_words[-1].text in PLURAL\n",
    "                or gram_words[0].text in PLURAL\n",
    "                or (\n",
    "                    gram_words[0].text.lower() in ALL_STOPWORDS\n",
    "                    and (not gram_words[0].text[0].isupper() or j_st == 0)\n",
    "                )\n",
    "            ):\n",
    "                continue\n",
    "            # If the word starts with punctuation and there is a space in between, also continue; keep\n",
    "            # if punctuation is part of the word boundary\n",
    "            # print(\"4\", j_st, gram_words, [g.pos_ for g in gram_words])\n",
    "            if (\n",
    "                (\n",
    "                    gram_words[0].text in PUNC\n",
    "                    and (j_st+1 >= len(doc) or new_to_old_span[j_st] != new_to_old_span[j_st+1])\n",
    "                )\n",
    "                or\n",
    "                (\n",
    "                    gram_words[-1].text in PUNC\n",
    "                    and (j_end-2 < 0 or new_to_old_span[j_end-1] != new_to_old_span[j_end-2])\n",
    "                )\n",
    "            ):\n",
    "                continue\n",
    "            joined_gram = \" \".join(split_sent[j_st_adjusted:j_end_adjusted])\n",
    "            # If 's in alias, make sure we remove the space and try that alias, too\n",
    "            joined_gram_merged_plural = joined_gram.replace(\" 's\", \"'s\")\n",
    "            # If PUNC in alias, make sure we remove the space and try that alias, too\n",
    "            joined_gram_merged_nopunc = joined_gram_merged_plural.translate(table)\n",
    "            gram_attempt = get_lnrm(joined_gram, strip=True, lower=True)\n",
    "            gram_attempt_merged_plural = get_lnrm(\n",
    "                joined_gram_merged_plural, strip=True, lower=True\n",
    "            )\n",
    "            gram_attempt_merged_nopunc = get_lnrm(\n",
    "                joined_gram_merged_nopunc, strip=True, lower=True\n",
    "            )\n",
    "            # Remove numbers\n",
    "            if (\n",
    "                gram_attempt.isnumeric()\n",
    "                or joined_gram_merged_plural.isnumeric()\n",
    "                or gram_attempt_merged_nopunc.isnumeric()\n",
    "            ):\n",
    "                continue\n",
    "            final_gram = None\n",
    "            # print(\"4\", gram_attempt, [g.pos_ for g in gram_words])\n",
    "            if gram_attempt in all_aliases:\n",
    "                final_gram = gram_attempt\n",
    "            elif gram_attempt_merged_plural in all_aliases:\n",
    "                final_gram = gram_attempt_merged_plural\n",
    "            elif gram_attempt_merged_nopunc in all_aliases:\n",
    "                final_gram = gram_attempt_merged_nopunc\n",
    "                # print(\"5\", final_gram, [g.pos_ for g in gram_words])\n",
    "            # print(\"FINAL GRAM\", final_gram)\n",
    "            if final_gram is not None:\n",
    "                keep = True\n",
    "                # We start from the largest n-grams and go down in size. This prevents us from adding an alias that is a subset of another.\n",
    "                # For example: \"Tell me about the mother on how I met you mother\" will find \"the mother\" as alias and \"mother\". We want to\n",
    "                # only take \"the mother\" and not \"mother\" as it's likely more descriptive of the real entity.\n",
    "                for u_al in used_aliases:\n",
    "                    u_j_st = u_al[1]\n",
    "                    u_j_end = u_al[2]\n",
    "                    if j_st_adjusted < u_j_end and j_end_adjusted > u_j_st:\n",
    "                        keep = False\n",
    "                        break\n",
    "                if not keep:\n",
    "                    continue\n",
    "                used_aliases.append(tuple([final_gram, j_st_adjusted, j_end_adjusted]))\n",
    "    # sort based on span order\n",
    "    aliases_for_sorting = sorted(used_aliases, key=lambda elem: [elem[1], elem[2]])\n",
    "    used_aliases = [a[0] for a in aliases_for_sorting]\n",
    "    spans = [[a[1], a[2]] for a in aliases_for_sorting]\n",
    "    assert all([sp[1] <= len(doc) for sp in spans]), f\"{spans} {sentence}\"\n",
    "    print(spans)\n",
    "    return used_aliases, spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/lorr1/projects/bootleg/bootleg/layers/alias_to_ent_encoder.py:97: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  alias2entity_table = torch.from_numpy(alias2entity_table)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from bootleg.end2end.bootleg_annotator import BootlegAnnotator\n",
    "\n",
    "ann = BootlegAnnotator(cache_dir='/dfs/scratch0/lorr1/projects/bootleg_cache', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.set_threshold(0.0)\n",
    "ann.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Avro Lincoln']]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.label_mentions([\"i want to buy a avro lincoln\"])[\"titles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 7], [10, 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Robin Friday', 'Philadelphia Flyers']]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"nhl ice hockey - standings after friday 's games . philadelphia 15 12 2 81 78 32\"\n",
    "ann.label_mentions(s, label_func=find_aliases_in_sentence_new)[\"titles\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
