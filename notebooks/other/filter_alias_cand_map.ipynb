{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import ujson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load alias map to filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootleg.symbols.entity_symbols import EntitySymbols\n",
    "root_dir = Path(\"/dfs/scratch0/lorr1/projects/bootleg-data/data/squad_0310\")\n",
    "entity_dump = EntitySymbols.load_from_cache(load_dir=root_dir / \"entity_db/entity_mappings\")\n",
    "wikidata_aliases = ujson.load(open(\"/dfs/scratch0/lorr1/projects/platelet-data/auxiliary/wikidata_aliases_1216.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikidata or filter\n",
    "DUMP_METHOD = \"filter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_aliases = entity_dump.get_alias2qids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load type mappings for adding back in countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load count files for all of wikipedia --- these were computed with `compute_statistics.py` (in utils/preprocessing) over the merged data file of test, dev, and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times alias phrase occurs in the text across ALL of wikipedia\n",
    "alias_text_counts = ujson.load(\n",
    "    open(root_dir / 'stats/alias_text_counts.json'))\n",
    "\n",
    "# number of times alias occurs as an alias across ALL of wikipedia\n",
    "alias_counts = ujson.load(\n",
    "    open(root_dir / 'stats/alias_counts.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple function to find aliases to remove based on the count files above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_value(alias, verbose=False):\n",
    "    if verbose:\n",
    "        print('# times occurs as alias:', alias_counts.get(alias, 0))\n",
    "        print('# times occurs in text:', alias_text_counts.get(alias, 0))\n",
    "    return alias_counts.get(alias, 0) / (alias_text_counts[alias]) if alias in alias_text_counts else -1\n",
    "\n",
    "def get_aliases_by_wikidata(curr_aliases, wikidata_aliases):\n",
    "    \"\"\"\n",
    "    Remove aliases that are not a wikidata also known as alias.\n",
    "    \"\"\"\n",
    "    aliases_to_remove = set()\n",
    "    for alias in tqdm(curr_aliases):\n",
    "        if alias not in wikidata_aliases:\n",
    "            aliases_to_remove.add(alias)\n",
    "    return aliases_to_remove\n",
    "\n",
    "def get_aliases_to_remove(curr_aliases, norm_threshold=0.017, min_seen=500, min_alias_count=10000):\n",
    "    \"\"\"\n",
    "    Remove aliases which are frequent words but infrequent aliases due to rarity \n",
    "    or mislabel (e.g. band \"themselves\").\n",
    "    \"\"\"\n",
    "    aliases_to_remove = set()\n",
    "    cnts = defaultdict(int)\n",
    "    grps = defaultdict(list)\n",
    "    for alias in tqdm(curr_aliases):\n",
    "        # If alias is not seen in Wikipedia\n",
    "        if alias not in alias_counts:\n",
    "            # If alias is seen in text but only a few times, skip as it's too few to make a decision\n",
    "            if (alias in alias_text_counts and alias_text_counts[alias] < min_seen):\n",
    "                continue\n",
    "            # if alias occurs in Wikidata (so it's in our alias map), but not as alias in Wikipedia\n",
    "            # and occurs more than min_seen times, only keep if one candidate (indicating a fairly unique alias)\n",
    "            # and if that one candidate is a type we care about (e.g., people and locations)\n",
    "            elif len(curr_aliases[alias]) == 1:\n",
    "                continue\n",
    "            # else make sure we don't think it's a person or location name - we want to keep those\n",
    "            # even if more general alias\n",
    "            else:\n",
    "                cnts[\"not_in_wikipedia\"] += 1\n",
    "                grps[\"not_in_wikipedia\"].append(alias)\n",
    "                aliases_to_remove.add(alias)\n",
    "                continue \n",
    "        # length greater than max_alias_len and weak labels cause some aliases to occur as aliases \n",
    "        # but not occur in the text\n",
    "        if alias not in alias_text_counts:\n",
    "            continue \n",
    "        # filter out aliases which occur commonly in the text but uncommonly as an alias\n",
    "        # we require that the alias is a common phrase in text \n",
    "        # and that the phrase isn't very commonly an alias \n",
    "        if (get_norm_value(alias) < norm_threshold):\n",
    "            if alias_text_counts[alias] > min_seen:\n",
    "                if alias_counts[alias] < min_alias_count:\n",
    "                    aliases_to_remove.add(alias)\n",
    "                    cnts[\"removed_filter\"] += 1\n",
    "                    grps[\"removed_filter\"].append(alias)\n",
    "                else:\n",
    "                    cnts[\"grt_min_alias_cnt\"] += 1\n",
    "                    grps[\"grt_min_alias_cnt\"].append(alias)\n",
    "            else:\n",
    "                cnts[\"lt_min_seen\"] += 1\n",
    "                grps[\"lt_min_seen\"].append(alias)\n",
    "    \n",
    "    return aliases_to_remove, cnts, grps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22893/4343182 [00:00<00:18, 228926.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using stats to filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4343182/4343182 [00:04<00:00, 934085.91it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124330\n",
      "{\n",
      "    \"removed_filter\": 4326,\n",
      "    \"lt_min_seen\": 6286,\n",
      "    \"not_in_wikipedia\": 120004\n",
      "}\n",
      "Will remove 124330 out of 4343182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if DUMP_METHOD == \"filter\":\n",
    "    print(\"Using stats to filter\")\n",
    "    aliases_to_remove, cnts, grps = get_aliases_to_remove(curr_aliases)\n",
    "    print(len(aliases_to_remove))\n",
    "    print(ujson.dumps(cnts, indent=4))\n",
    "else:\n",
    "    print(\"Using Wikidata to filter\")\n",
    "    aliases_to_remove = get_aliases_by_wikidata(curr_aliases, wikidata_aliases)\n",
    "\n",
    "print(f\"Will remove {len(aliases_to_remove)} out of {len(curr_aliases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks on the filter step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "цезиас мец\n",
      "10 xronia mazi\n",
      "rockfunk\n",
      "ustajikistan relations\n",
      "ak47su\n",
      "body transistor\n",
      "gare dolten\n",
      "henry richardson cricketer born 1846\n",
      "justice william o douglas\n",
      "woollcott alexander\n",
      "mtv swedish tv channel\n",
      "sir michael atiyah\n",
      "west los angeles ca\n",
      "kfar sava\n",
      "hunters island\n",
      "french concession of shanghai\n",
      "roadshow films\n",
      "2015 yale bulldogs football\n",
      "black sea region turkey\n",
      "henry gilroy baseball\n",
      "beckham putra nugraha\n",
      "korbr\n",
      "anne dormer lady hungerford\n",
      "saint torpes of pisa\n",
      "iso 639zir\n",
      "ballets by marius petipa\n",
      "marine cadets\n",
      "pacific northwest bell telephone company\n",
      "avid d weinberger\n",
      "5 x 5 cube\n",
      "draftzayn africa\n",
      "daphne anne caruana galizia\n",
      "national bishop for torres strait people\n",
      "catchment water\n",
      "notre dame fighting irish football 1985\n",
      "lockheed hudson iva\n",
      "englishborn\n",
      "united statesman\n",
      "gadaræ\n",
      "trygve martin bratteli\n",
      "jakobstadt\n",
      "albanian national liberation front\n",
      "sirkesh\n",
      "cuisine of boston\n",
      "still standing tv series\n",
      "norske skogindustrier asa\n",
      "klein charles\n",
      "mpeg2 layer ii\n",
      "poaching of white rhinoceroses\n",
      "the henegar center for the performing arts\n"
     ]
    }
   ],
   "source": [
    "# sample what aliases are getting removed\n",
    "num_to_sample = 50\n",
    "for alias in np.random.choice(list(aliases_to_remove), num_to_sample): \n",
    "    print(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for existence of certain words in aliases_to_remove\n",
    "sanity_checks = [('themselves', True), \n",
    "                 ('dolittle', False),\n",
    "                 ('us', False),\n",
    "                 ('s', True),\n",
    "                 ('is', True),\n",
    "                 ('also', True),\n",
    "                 ('in a world', True), \n",
    "                 ('of', True),\n",
    "                 ('the', True),\n",
    "                 ('by year', True),\n",
    "                 ('apoptosis', False),\n",
    "                 ('england', False)]\n",
    "for s, bool_val in sanity_checks: \n",
    "    assert (s in aliases_to_remove) is bool_val, f'{s} {bool_val} {s in aliases_to_remove}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WILL KEEP\n",
      "[['Q17', 72356], ['Q161652', 5851], ['Q5287', 5438], ['Q188712', 5093], ['Q184963', 4675], ['Q476215', 4653], ['Q219712', 3382], ['Q205662', 2752], ['Q1146127', 2364], ['Q170566', 2231], ['Q848647', 869], ['Q388232', 747], ['Q696251', 578], ['Q731647', 575], ['Q850204', 571], ['Q1122433', 562], ['Q234138', 498], ['Q179103', 457], ['Q831454', 408], ['Q575453', 368], ['Q130436', 359], ['Q962145', 346], ['Q736311', 340], ['Q231425', 287], ['Q603399', 271], ['Q210688', 258], ['Q533312', 249], ['Q3658577', 231], ['Q579842', 197], ['Q841337', 180]]\n",
      "['<yagoGeoEntity>']\n",
      "# times occurs as alias: 76417\n",
      "# times occurs in text: 558866\n",
      "NORM 0.1367358186041019\n"
     ]
    }
   ],
   "source": [
    "# debug the norm values to set different thresholds\n",
    "t = 'japan'\n",
    "if t in aliases_to_remove:\n",
    "    print(\"WILL REMOVE\")\n",
    "else:\n",
    "    print(\"WILL KEEP\")\n",
    "print(curr_aliases.get(t))\n",
    "print(types_coarse.get_types(curr_aliases[t][0][0]))\n",
    "print(f\"NORM\", get_norm_value(t, verbose=True))\n",
    "for k in grps:\n",
    "    if t in grps[k]:\n",
    "        print(k)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove aliases and save new candidate mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4218852 VS 4343182\n"
     ]
    }
   ],
   "source": [
    "new_aliases = {}\n",
    "for alias in list(curr_aliases): \n",
    "    if alias not in aliases_to_remove:\n",
    "        new_aliases[alias] = curr_aliases[alias] \n",
    "new_alias_idx = {al:i for i, al in enumerate(new_aliases.keys())}\n",
    "print(len(new_aliases), \"VS\", len(curr_aliases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved alias mapping at /dfs/scratch0/lorr1/projects/bootleg-data/data/squad_0310/entity_db/entity_mappings/alias2qids_filt.json and id to /dfs/scratch0/lorr1/projects/bootleg-data/data/squad_0310/entity_db/entity_mappings/alias2id_filt.json\n"
     ]
    }
   ],
   "source": [
    "new_dir = root_dir / 'entity_db/entity_mappings'\n",
    "# os.makedirs(new_dir, exist_ok=True)\n",
    "new_alias_file = new_dir / f'alias2qids_filt.json'\n",
    "new_aliasidx_file = new_dir / f'alias2id_filt.json'\n",
    "\n",
    "with open(new_alias_file, 'w') as f: \n",
    "    ujson.dump(new_aliases, f)\n",
    "with open(new_aliasidx_file, 'w') as f: \n",
    "    ujson.dump(new_alias_idx, f)\n",
    "\n",
    "print(f\"Saved alias mapping at {new_alias_file} and id to {new_aliasidx_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
